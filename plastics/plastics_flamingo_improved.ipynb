{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovRLzgZl65LR"
      },
      "source": [
        "### morgan fingerprints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mlK-YsUs-z6f",
        "outputId": "fad9e421-ba0f-489d-b4be-ac7fb0c22b9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: rdkit in /usr/local/lib/python3.11/dist-packages (2024.9.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rdkit) (2.0.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from rdkit) (11.1.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install rdkit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-j9ixx3b9WW9",
        "outputId": "8d8733cb-e464-4494-ef95-d0674f1c2d70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[12:31:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:1389: ConvergenceWarning: Number of distinct clusters (23) found smaller than n_clusters (40). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "<ipython-input-4-f24b7a52aba3>:39: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  return df.groupby('cluster', group_keys=False).apply(lambda x: x.sample(frac=subset_fraction, random_state=42))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set: 3980 samples, 17 clusters\n",
            "Validation set: 536 samples, 6 clusters\n",
            "Cluster overlap between train and val: 0\n",
            "Average train set similarity: 0.4520\n",
            "Average validation set similarity: 0.3789\n",
            "Validation set is more diverse than training set\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import AllChem\n",
        "from sklearn.cluster import KMeans\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import euclidean_distances\n",
        "\n",
        "df_combined = pd.read_csv('sigma_data.csv')\n",
        "df_combined = df_combined.drop(columns=[col for col in ['cluster', 'Cluster'] if col in df_combined.columns], errors='ignore')\n",
        "\n",
        "def smiles_to_morgan_fp(smiles, radius=2, nBits=2048):\n",
        "    mol = Chem.MolFromSmiles(smiles)\n",
        "    if mol is None:\n",
        "        return None\n",
        "    return np.array(AllChem.GetMorganFingerprintAsBitVect(mol, radius, nBits=nBits))\n",
        "\n",
        "fingerprints = []\n",
        "valid_indices = []\n",
        "\n",
        "for i, smiles in enumerate(df_combined['smiles']):\n",
        "    try:\n",
        "        fp = smiles_to_morgan_fp(smiles)\n",
        "        if fp is not None:\n",
        "            fingerprints.append(fp)\n",
        "            valid_indices.append(i)\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing SMILES {smiles}: {e}\")\n",
        "\n",
        "X = np.array(fingerprints)\n",
        "n_clusters = min(40, len(X))\n",
        "\n",
        "kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=15)\n",
        "cluster_labels = kmeans.fit_predict(X)\n",
        "\n",
        "df_valid = df_combined.iloc[valid_indices].copy()\n",
        "df_valid['cluster'] = cluster_labels\n",
        "\n",
        "def subset_clusters(df, subset_fraction=0.5):\n",
        "    return df.groupby('cluster', group_keys=False).apply(lambda x: x.sample(frac=subset_fraction, random_state=42))\n",
        "\n",
        "df_subset = subset_clusters(df_valid, subset_fraction=0.5)\n",
        "cluster_centers = kmeans.cluster_centers_\n",
        "\n",
        "def split_by_cluster_dissimilarity(df, cluster_centers, val_size=0.3):\n",
        "    unique_clusters = df['cluster'].unique()\n",
        "    n_clusters = len(unique_clusters)\n",
        "    n_val = int(n_clusters * val_size)\n",
        "    distances = euclidean_distances(cluster_centers)\n",
        "    np.random.seed(42)\n",
        "    current_cluster = np.random.choice(unique_clusters)\n",
        "    val_clusters = [current_cluster]\n",
        "    remaining_clusters = set(unique_clusters) - {current_cluster}\n",
        "\n",
        "    while len(val_clusters) < n_val:\n",
        "        avg_distances = [(cluster, np.mean([distances[cluster, val_cluster] for val_cluster in val_clusters])) for cluster in remaining_clusters]\n",
        "        next_cluster = max(avg_distances, key=lambda x: x[1])[0]\n",
        "        val_clusters.append(next_cluster)\n",
        "        remaining_clusters.remove(next_cluster)\n",
        "\n",
        "    train_clusters = list(set(unique_clusters) - set(val_clusters))\n",
        "    return df[df['cluster'].isin(train_clusters)], df[df['cluster'].isin(val_clusters)]\n",
        "\n",
        "train_df, val_df = split_by_cluster_dissimilarity(df_subset, cluster_centers)\n",
        "\n",
        "print(f\"Training set: {len(train_df)} samples, {train_df['cluster'].nunique()} clusters\")\n",
        "print(f\"Validation set: {len(val_df)} samples, {val_df['cluster'].nunique()} clusters\")\n",
        "\n",
        "train_clusters = set(train_df['cluster'].unique())\n",
        "val_clusters = set(val_df['cluster'].unique())\n",
        "print(f\"Cluster overlap between train and val: {len(train_clusters.intersection(val_clusters))}\")\n",
        "\n",
        "train_df.to_csv('train_data.csv', index=False)\n",
        "val_df.to_csv('val_data.csv', index=False)\n",
        "\n",
        "train_fps = np.array([fingerprints[valid_indices.index(i)] for i in train_df.index if i in valid_indices])\n",
        "val_fps = np.array([fingerprints[valid_indices.index(i)] for i in val_df.index if i in valid_indices])\n",
        "\n",
        "def calculate_avg_tanimoto(fps):\n",
        "    sum_sim, count = 0, 0\n",
        "    for i in range(len(fps)):\n",
        "        for j in range(i+1, len(fps)):\n",
        "            intersection, union = np.sum(fps[i] & fps[j]), np.sum(fps[i] | fps[j])\n",
        "            if union > 0:\n",
        "                sum_sim += intersection / union\n",
        "                count += 1\n",
        "    return sum_sim / count if count > 0 else 0\n",
        "\n",
        "if len(train_fps) > 1 and len(val_fps) > 1:\n",
        "    train_diversity = calculate_avg_tanimoto(train_fps)\n",
        "    val_diversity = calculate_avg_tanimoto(val_fps)\n",
        "    print(f\"Average train set similarity: {train_diversity:.4f}\")\n",
        "    print(f\"Average validation set similarity: {val_diversity:.4f}\")\n",
        "    print(f\"Validation set is {('more diverse' if val_diversity < train_diversity else 'less diverse')} than training set\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "6_TVOHpq_FVj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "a3576b02-7cc5-41fc-91a1-775fb91ef03f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                           Plastic Type      Enzyme Name  \\\n",
              "2673                              Nylon      Polyamidase   \n",
              "2642                              Nylon  Nylon_hydrolase   \n",
              "2668                              Nylon      Polyamidase   \n",
              "2627                              Nylon  Nylon_hydrolase   \n",
              "2561                              Nylon        Hydrolase   \n",
              "...                                 ...              ...   \n",
              "876   PBAT_PBS_PBSA_PCL_PET_PHB_PLA_PHA         Cutinase   \n",
              "883   PBAT_PBS_PBSA_PCL_PET_PHB_PLA_PHA         Cutinase   \n",
              "893   PBAT_PBS_PBSA_PCL_PET_PHB_PLA_PHA         Cutinase   \n",
              "911   PBAT_PBS_PBSA_PCL_PET_PHB_PLA_PHA         Cutinase   \n",
              "892   PBAT_PBS_PBSA_PCL_PET_PHB_PLA_PHA         Cutinase   \n",
              "\n",
              "                                       protein_sequence  \\\n",
              "2673  MTVAEYAAHDATGLAELVRRGQVSAAEVATAARTALEAVNPELCAV...   \n",
              "2642  MNTTPVHALTLITGGPAVDPAPRPAGEPAAGGPGKAAEDLVPLRSD...   \n",
              "2668  MDVAEYAAHDATGLADLIRAGQVSAAEVATAAKTALAAVEPELAAV...   \n",
              "2627  MNATPAHALTGIDSGIAVTPAPRLGGDEVFGGSGNAAFDLVPVAST...   \n",
              "2561  MNTTPVHALTGIDSGIAVDPAPRLAGPPVPGGPGDDAFDLAPGRST...   \n",
              "...                                                 ...   \n",
              "876   MRRRRQAGTGARAGRARAIGVAVLALAVLVGAVGGVAGAEVSTAQD...   \n",
              "883   MRIRRSAGAGARARGRRAIVVMTTALAVLVGAVGGVAGAEVATAPD...   \n",
              "893   MRIRRQAGTGARRRMARAIGVYTTALAVLTGAVGGVAGAEVATAQD...   \n",
              "911   MRIRRSAETGARASRARRITVVTTAVAVLVGAVGGVAGAEVSDAAD...   \n",
              "892   MRIGRQAGTGARATMARRIGVLTLALAVLVGAVGGPAGAPPSTAQD...   \n",
              "\n",
              "                                                 smiles  protein_length  \\\n",
              "2673                                    NCCCCNCCCC(=O)O             479   \n",
              "2642                                    NCCCCNCCCC(=O)O             355   \n",
              "2668                                    NCCCCNCCCC(=O)O             479   \n",
              "2627                                    NCCCCNCCCC(=O)O             355   \n",
              "2561                                    NCCCCNCCCC(=O)O             355   \n",
              "...                                                 ...             ...   \n",
              "876   [*]OCCCCOC(=O)CC(=O)O[*].[*]OCCCCOC(=O)CC(=O)O...             304   \n",
              "883   [*]OCCCCOC(=O)CC(=O)O[*].[*]OCCCCOC(=O)CC(=O)O...             304   \n",
              "893   [*]OCCCCOC(=O)CC(=O)O[*].[*]OCCCCOC(=O)CC(=O)O...             304   \n",
              "911   [*]OCCCCOC(=O)CC(=O)O[*].[*]OCCCCOC(=O)CC(=O)O...             304   \n",
              "892   [*]OCCCCOC(=O)CC(=O)O[*].[*]OCCCCOC(=O)CC(=O)O...             304   \n",
              "\n",
              "     synthetic  cluster  \n",
              "2673      True        3  \n",
              "2642      True        3  \n",
              "2668      True        3  \n",
              "2627      True        3  \n",
              "2561      True        3  \n",
              "...        ...      ...  \n",
              "876       True       16  \n",
              "883       True       16  \n",
              "893       True       16  \n",
              "911       True       16  \n",
              "892       True       16  \n",
              "\n",
              "[536 rows x 7 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6fc304e9-eb9f-4aad-9743-03e0fd04b331\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Plastic Type</th>\n",
              "      <th>Enzyme Name</th>\n",
              "      <th>protein_sequence</th>\n",
              "      <th>smiles</th>\n",
              "      <th>protein_length</th>\n",
              "      <th>synthetic</th>\n",
              "      <th>cluster</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2673</th>\n",
              "      <td>Nylon</td>\n",
              "      <td>Polyamidase</td>\n",
              "      <td>MTVAEYAAHDATGLAELVRRGQVSAAEVATAARTALEAVNPELCAV...</td>\n",
              "      <td>NCCCCNCCCC(=O)O</td>\n",
              "      <td>479</td>\n",
              "      <td>True</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2642</th>\n",
              "      <td>Nylon</td>\n",
              "      <td>Nylon_hydrolase</td>\n",
              "      <td>MNTTPVHALTLITGGPAVDPAPRPAGEPAAGGPGKAAEDLVPLRSD...</td>\n",
              "      <td>NCCCCNCCCC(=O)O</td>\n",
              "      <td>355</td>\n",
              "      <td>True</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2668</th>\n",
              "      <td>Nylon</td>\n",
              "      <td>Polyamidase</td>\n",
              "      <td>MDVAEYAAHDATGLADLIRAGQVSAAEVATAAKTALAAVEPELAAV...</td>\n",
              "      <td>NCCCCNCCCC(=O)O</td>\n",
              "      <td>479</td>\n",
              "      <td>True</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2627</th>\n",
              "      <td>Nylon</td>\n",
              "      <td>Nylon_hydrolase</td>\n",
              "      <td>MNATPAHALTGIDSGIAVTPAPRLGGDEVFGGSGNAAFDLVPVAST...</td>\n",
              "      <td>NCCCCNCCCC(=O)O</td>\n",
              "      <td>355</td>\n",
              "      <td>True</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2561</th>\n",
              "      <td>Nylon</td>\n",
              "      <td>Hydrolase</td>\n",
              "      <td>MNTTPVHALTGIDSGIAVDPAPRLAGPPVPGGPGDDAFDLAPGRST...</td>\n",
              "      <td>NCCCCNCCCC(=O)O</td>\n",
              "      <td>355</td>\n",
              "      <td>True</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>876</th>\n",
              "      <td>PBAT_PBS_PBSA_PCL_PET_PHB_PLA_PHA</td>\n",
              "      <td>Cutinase</td>\n",
              "      <td>MRRRRQAGTGARAGRARAIGVAVLALAVLVGAVGGVAGAEVSTAQD...</td>\n",
              "      <td>[*]OCCCCOC(=O)CC(=O)O[*].[*]OCCCCOC(=O)CC(=O)O...</td>\n",
              "      <td>304</td>\n",
              "      <td>True</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>883</th>\n",
              "      <td>PBAT_PBS_PBSA_PCL_PET_PHB_PLA_PHA</td>\n",
              "      <td>Cutinase</td>\n",
              "      <td>MRIRRSAGAGARARGRRAIVVMTTALAVLVGAVGGVAGAEVATAPD...</td>\n",
              "      <td>[*]OCCCCOC(=O)CC(=O)O[*].[*]OCCCCOC(=O)CC(=O)O...</td>\n",
              "      <td>304</td>\n",
              "      <td>True</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>893</th>\n",
              "      <td>PBAT_PBS_PBSA_PCL_PET_PHB_PLA_PHA</td>\n",
              "      <td>Cutinase</td>\n",
              "      <td>MRIRRQAGTGARRRMARAIGVYTTALAVLTGAVGGVAGAEVATAQD...</td>\n",
              "      <td>[*]OCCCCOC(=O)CC(=O)O[*].[*]OCCCCOC(=O)CC(=O)O...</td>\n",
              "      <td>304</td>\n",
              "      <td>True</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>911</th>\n",
              "      <td>PBAT_PBS_PBSA_PCL_PET_PHB_PLA_PHA</td>\n",
              "      <td>Cutinase</td>\n",
              "      <td>MRIRRSAETGARASRARRITVVTTAVAVLVGAVGGVAGAEVSDAAD...</td>\n",
              "      <td>[*]OCCCCOC(=O)CC(=O)O[*].[*]OCCCCOC(=O)CC(=O)O...</td>\n",
              "      <td>304</td>\n",
              "      <td>True</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>892</th>\n",
              "      <td>PBAT_PBS_PBSA_PCL_PET_PHB_PLA_PHA</td>\n",
              "      <td>Cutinase</td>\n",
              "      <td>MRIGRQAGTGARATMARRIGVLTLALAVLVGAVGGPAGAPPSTAQD...</td>\n",
              "      <td>[*]OCCCCOC(=O)CC(=O)O[*].[*]OCCCCOC(=O)CC(=O)O...</td>\n",
              "      <td>304</td>\n",
              "      <td>True</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>536 rows × 7 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6fc304e9-eb9f-4aad-9743-03e0fd04b331')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-6fc304e9-eb9f-4aad-9743-03e0fd04b331 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-6fc304e9-eb9f-4aad-9743-03e0fd04b331');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-1cf4191a-ef0d-4838-a921-f649ed6d13d0\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-1cf4191a-ef0d-4838-a921-f649ed6d13d0')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-1cf4191a-ef0d-4838-a921-f649ed6d13d0 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_676fbd6e-d708-4b88-9278-fe0e2d2a262d\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('val_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_676fbd6e-d708-4b88-9278-fe0e2d2a262d button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('val_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "val_df",
              "summary": "{\n  \"name\": \"val_df\",\n  \"rows\": 536,\n  \"fields\": [\n    {\n      \"column\": \"Plastic Type\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"Nylon\",\n          \"PLA_PCL_PBSA\",\n          \"PBAT_PBS_PBSA_PCL_PET_PHB_PLA_PHA\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Enzyme Name\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 12,\n        \"samples\": [\n          \"PETase\",\n          \"PHB_depolymerase\",\n          \"Polyamidase\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"protein_sequence\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 536,\n        \"samples\": [\n          \"MLRFDFPGVAIGAAEYEEGPTGATVIHFPAGARLAVDARGGAVSLSGGLDRVHAIVLAGGSGYGLEAVAGVSGALLERGGYRTGFADLPLVSSAVIYDFSARETAVYPDAALGRKALEAAVPGEFPLGRAGAGHGAVAGKVLGDITERGGQGAAFREFGDVRIAAVVVVNPFGVIVDRAGRVVRGCYDAQTGVRRHPEFDYQTVFAEQRPPVTEAGNTTIGAIVTNVRLSPVELNQFAKLVHNSMARLIRPFHTDYDGDTLFAVTTDEIDLPTVPGSSRGRLSVNATALGAIAAEVMWAAVLAATE\",\n          \"MNARSTGQHPARAPGAAPVEPTLDNWQAAPHNRWAFAHVGELLPTAAVSRRDPATPAEPAVLLDALAGRLPDLEQRLEETHTDAFLVLRGGEVVAERYRGGFAPDDRHLLMSVSKSLTGTVVGALEDEGRIDPAQPVTQYVPELAGSAYDGPSVQQVLDMQVGVDYSEDYTDPASAVQLHGRAAGWRPRRPGDPADLYEFLTTLRPDGRTGEFRYVSANTDVLAWIVERVTGRSYVEALSTYLWAPLGAERDATITVDTTGTGLAHGGVSATARDLARFGRMMLDGGVGPGGRVVSEQWVRDVLAGGAAEAYTAGGFTGTYPAGSYRNQWWHTGSERGVVSAIGIHGQTLWLDPLTDTVIVRLSSWPDPTNAHHHRLQLGLLLDVARALDAV\",\n          \"MSDVNLWQDATAQAELVRSGEISPTELVEATIAHVQAVNPEINAVIIPLFEKARREPELASGPFAGVPFLLKDLTVWSQGDIMTSSIKGMIESGYRADHDAYFAQRMRAAGFVLLGKTNTPEMGTIGTTEPLALGATRNPWNLGRSVGGSSGGSGAAVAAALSPVAHGSDAGGSVRIPASVCGVVGLKPTRGRISVGPLVTDEDNAGGFAHEGLLARSVRDIAALLDVVSGHDPGDTFVAPPRSRPYAQAISENPGSLRVGVVTHNPVGDFALDPECAAAAEGAAAALAALGHDVGDEYPEALGDSQFLRTYLTILGVAIAREIERIGELIGRPLTEDDVEWTSWIMVKRAGSVSGRDFAAALDELRAAAGKVARWWEAGLDLLITPTVTRATPEIGELKLPKGEDLEGRARDLPEGSLRMLAFTVPFNVTGQPAISLPIGQSSDGLPIGVQFVAAYGREDLLLQVAAQLEAALPWVARRPQLLAPSRAIRAA\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"smiles\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"NCCCCNCCCC(=O)O\",\n          \"[*]OC(C)C(=O)[*].[*]OCCCCC(=O)[*].[*]OCCCCOC(=O)CC(=O)O[*]\",\n          \"[*]OCCCCOC(=O)CC(=O)O[*].[*]OCCCCOC(=O)CC(=O)O[*].[*]OCCCCC(=O)[*].[*]OCCOC(=O)c1ccc(cc1)C(=O)O[*].[*]OC(C)CC(=O)[*].[*]OC(C)C(=O)[*].[*]OC(C)CC(=O)[*]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"protein_length\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 104,\n        \"min\": 108,\n        \"max\": 576,\n        \"num_unique_values\": 16,\n        \"samples\": [\n          479,\n          355,\n          318\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"synthetic\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          true\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"cluster\",\n      \"properties\": {\n        \"dtype\": \"int32\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "val_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "qxj9iuhLBlJZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "43290308-bea7-4f43-f812-db2ac3cf51a5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         Plastic Type       Enzyme Name  \\\n",
              "3126              PBS  PBS_depolymerase   \n",
              "3120              PBS  PBS_depolymerase   \n",
              "3430             PBSA            Lipase   \n",
              "3393             PBSA            Lipase   \n",
              "3381             PBSA            Lipase   \n",
              "...               ...               ...   \n",
              "3648  PLA_PHA_PES_PCL            Lipase   \n",
              "3640  PLA_PHA_PES_PCL            Lipase   \n",
              "3704  PLA_PHA_PES_PCL            Lipase   \n",
              "3646  PLA_PHA_PES_PCL            Lipase   \n",
              "3698  PLA_PHA_PES_PCL            Lipase   \n",
              "\n",
              "                                       protein_sequence  \\\n",
              "3126  MHLSRGACDRPFKKETTMTHTFSVRALLAAGALLASAAVSAQTNPY...   \n",
              "3120  MTLPLTRPEIPFKEETTMRVHFSVRALLAAGALLASAAVSAQTNPY...   \n",
              "3430  MVRSMRSRVVAAAVALAMSGAALAGTTAATTAATATAATAATAATA...   \n",
              "3393  MNLVGHSQGGLTSRYVAAVAPDLVASVTTIGTPHRGSEAADFVQSV...   \n",
              "3381  MNLVGHSQGGLTSRYVAAVAPDLVASVTTIGTPHRGSEFADFVQSI...   \n",
              "...                                                 ...   \n",
              "3648  MTETLLYRDMNRAQLDAAYNNTAAVPDFPGIYAAYQARSAAFYASA...   \n",
              "3640  MTMTLLYRDMNQAQLDAAYNNTQAVPDFPGIYAALQARSASFYASA...   \n",
              "3704  MSTLSWVRTVNRTLGWVAPGLVARKMRALFMTPRKRLPRDWELPLL...   \n",
              "3646  MTPTLLYRDMNQAQLDAAYNNTQAVPDFPGIYAAFQARSASFYASA...   \n",
              "3698  MSTLSWVRSVNGTLGLVAPTLVASKARRLFMTPRERLPRDWEAPLL...   \n",
              "\n",
              "                                                 smiles  protein_length  \\\n",
              "3126                           [*]OCCCCOC(=O)CC(=O)O[*]             304   \n",
              "3120                           [*]OCCCCOC(=O)CC(=O)O[*]             304   \n",
              "3430                           [*]OCCCCOC(=O)CC(=O)O[*]             370   \n",
              "3393                           [*]OCCCCOC(=O)CC(=O)O[*]             240   \n",
              "3381                           [*]OCCCCOC(=O)CC(=O)O[*]             240   \n",
              "...                                                 ...             ...   \n",
              "3648  [*]OC(C)C(=O)[*].[*]OC(C)CC(=O)[*].[*]OCCOC(=O...             275   \n",
              "3640  [*]OC(C)C(=O)[*].[*]OC(C)CC(=O)[*].[*]OCCOC(=O...             275   \n",
              "3704  [*]OC(C)C(=O)[*].[*]OC(C)CC(=O)[*].[*]OCCOC(=O...             277   \n",
              "3646  [*]OC(C)C(=O)[*].[*]OC(C)CC(=O)[*].[*]OCCOC(=O...             275   \n",
              "3698  [*]OC(C)C(=O)[*].[*]OC(C)CC(=O)[*].[*]OCCOC(=O...             277   \n",
              "\n",
              "     synthetic  cluster  \n",
              "3126      True        0  \n",
              "3120      True        0  \n",
              "3430      True        0  \n",
              "3393      True        0  \n",
              "3381      True        0  \n",
              "...        ...      ...  \n",
              "3648      True       22  \n",
              "3640      True       22  \n",
              "3704      True       22  \n",
              "3646      True       22  \n",
              "3698      True       22  \n",
              "\n",
              "[3980 rows x 7 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f43629e7-b7d3-4fc4-8e53-8d1a938c4b8c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Plastic Type</th>\n",
              "      <th>Enzyme Name</th>\n",
              "      <th>protein_sequence</th>\n",
              "      <th>smiles</th>\n",
              "      <th>protein_length</th>\n",
              "      <th>synthetic</th>\n",
              "      <th>cluster</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3126</th>\n",
              "      <td>PBS</td>\n",
              "      <td>PBS_depolymerase</td>\n",
              "      <td>MHLSRGACDRPFKKETTMTHTFSVRALLAAGALLASAAVSAQTNPY...</td>\n",
              "      <td>[*]OCCCCOC(=O)CC(=O)O[*]</td>\n",
              "      <td>304</td>\n",
              "      <td>True</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3120</th>\n",
              "      <td>PBS</td>\n",
              "      <td>PBS_depolymerase</td>\n",
              "      <td>MTLPLTRPEIPFKEETTMRVHFSVRALLAAGALLASAAVSAQTNPY...</td>\n",
              "      <td>[*]OCCCCOC(=O)CC(=O)O[*]</td>\n",
              "      <td>304</td>\n",
              "      <td>True</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3430</th>\n",
              "      <td>PBSA</td>\n",
              "      <td>Lipase</td>\n",
              "      <td>MVRSMRSRVVAAAVALAMSGAALAGTTAATTAATATAATAATAATA...</td>\n",
              "      <td>[*]OCCCCOC(=O)CC(=O)O[*]</td>\n",
              "      <td>370</td>\n",
              "      <td>True</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3393</th>\n",
              "      <td>PBSA</td>\n",
              "      <td>Lipase</td>\n",
              "      <td>MNLVGHSQGGLTSRYVAAVAPDLVASVTTIGTPHRGSEAADFVQSV...</td>\n",
              "      <td>[*]OCCCCOC(=O)CC(=O)O[*]</td>\n",
              "      <td>240</td>\n",
              "      <td>True</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3381</th>\n",
              "      <td>PBSA</td>\n",
              "      <td>Lipase</td>\n",
              "      <td>MNLVGHSQGGLTSRYVAAVAPDLVASVTTIGTPHRGSEFADFVQSI...</td>\n",
              "      <td>[*]OCCCCOC(=O)CC(=O)O[*]</td>\n",
              "      <td>240</td>\n",
              "      <td>True</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3648</th>\n",
              "      <td>PLA_PHA_PES_PCL</td>\n",
              "      <td>Lipase</td>\n",
              "      <td>MTETLLYRDMNRAQLDAAYNNTAAVPDFPGIYAAYQARSAAFYASA...</td>\n",
              "      <td>[*]OC(C)C(=O)[*].[*]OC(C)CC(=O)[*].[*]OCCOC(=O...</td>\n",
              "      <td>275</td>\n",
              "      <td>True</td>\n",
              "      <td>22</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3640</th>\n",
              "      <td>PLA_PHA_PES_PCL</td>\n",
              "      <td>Lipase</td>\n",
              "      <td>MTMTLLYRDMNQAQLDAAYNNTQAVPDFPGIYAALQARSASFYASA...</td>\n",
              "      <td>[*]OC(C)C(=O)[*].[*]OC(C)CC(=O)[*].[*]OCCOC(=O...</td>\n",
              "      <td>275</td>\n",
              "      <td>True</td>\n",
              "      <td>22</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3704</th>\n",
              "      <td>PLA_PHA_PES_PCL</td>\n",
              "      <td>Lipase</td>\n",
              "      <td>MSTLSWVRTVNRTLGWVAPGLVARKMRALFMTPRKRLPRDWELPLL...</td>\n",
              "      <td>[*]OC(C)C(=O)[*].[*]OC(C)CC(=O)[*].[*]OCCOC(=O...</td>\n",
              "      <td>277</td>\n",
              "      <td>True</td>\n",
              "      <td>22</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3646</th>\n",
              "      <td>PLA_PHA_PES_PCL</td>\n",
              "      <td>Lipase</td>\n",
              "      <td>MTPTLLYRDMNQAQLDAAYNNTQAVPDFPGIYAAFQARSASFYASA...</td>\n",
              "      <td>[*]OC(C)C(=O)[*].[*]OC(C)CC(=O)[*].[*]OCCOC(=O...</td>\n",
              "      <td>275</td>\n",
              "      <td>True</td>\n",
              "      <td>22</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3698</th>\n",
              "      <td>PLA_PHA_PES_PCL</td>\n",
              "      <td>Lipase</td>\n",
              "      <td>MSTLSWVRSVNGTLGLVAPTLVASKARRLFMTPRERLPRDWEAPLL...</td>\n",
              "      <td>[*]OC(C)C(=O)[*].[*]OC(C)CC(=O)[*].[*]OCCOC(=O...</td>\n",
              "      <td>277</td>\n",
              "      <td>True</td>\n",
              "      <td>22</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3980 rows × 7 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f43629e7-b7d3-4fc4-8e53-8d1a938c4b8c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f43629e7-b7d3-4fc4-8e53-8d1a938c4b8c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f43629e7-b7d3-4fc4-8e53-8d1a938c4b8c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-823f31ba-0f58-4237-888a-eda87fc7e01a\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-823f31ba-0f58-4237-888a-eda87fc7e01a')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-823f31ba-0f58-4237-888a-eda87fc7e01a button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_25ea0f87-50fe-4151-a9f3-fc02925df01e\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('train_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_25ea0f87-50fe-4151-a9f3-fc02925df01e button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('train_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "train_df",
              "summary": "{\n  \"name\": \"train_df\",\n  \"rows\": 3980,\n  \"fields\": [\n    {\n      \"column\": \"Plastic Type\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 33,\n        \"samples\": [\n          \"PBS_PBSA_PCL\",\n          \"PVA_O-PVA\",\n          \"PEG\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Enzyme Name\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 36,\n        \"samples\": [\n          \"PEG_aldehyde_dehydrogenase\",\n          \"Serine_hydrolase\",\n          \"PLA_depolymerase\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"protein_sequence\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3980,\n        \"samples\": [\n          \"MANTYERGPNPTDASETASSGPFSVSYENVTRFGADGFGGGTIYYPDENGTYGAVAISPGYTGTQASVAWLGERIASHGFVVITIDTNSTLDQPDSRARQLDAALDYMTNDASSPVRSRIDSSRLAVMGHSMGGGGSLEAASDRPDLKAAIPLTPWHLNRNWSSVRVPTLIIGAELDTIAPVNTHSLPFYNSLPTSIEKAYLELKGATHFAPNSDNSIIAKYSVAWLKRFVDNDTRYTQFLCPGPRDGLFGGVEEFRSNCPFKLELEHHHHHH\",\n          \"MPPLCCLPRRLAAAALLALATLVPLSAAAQTNPYQRGPDPTTRTLEDVRGPFRYASTNVSSGSGFGGGTIYYPTDVSGKVGAVAVVPGYLANQSSIAWWGPRLASHGFVVITLDTNSTSDQPASRSEQLNAALRQVVAQSRTRSSPIYGKVDPNRLAVMGWSMGGGGTLISARDNPSLKAAVPFAPWQTGANFSGVQVPTLVIGCENDTVAPISRHASPFYNSLSSSPHKAYLEINSGSHYCANTGNSRQALIGKAGVAWLKRFVDNDTRYSPFLCGNPRQADLRSNRLSEYRNNCPY\",\n          \"MENPYERGPDPTTASVEAARGPYAVSQGTVPGQFGSYGGGTIYYPTSTAQGKFGAVAIAPGFLSSQSSVAWLGPRIASHGFVVMTIDTGSIFDQPAQRGDQLLAALDYLTQRSSVRDRIDPNRLAVAGWSMGGGGSLEAAARRPSLKAAIPMAPWNLDKNWSRLTTPVLIVGGQNDLIAPVASHSVPFYNSIRSEKAYLELAGGSHFTATSANTPQAKLMISWLKRFVDNDTRYEQFICPLPSAGTSVSEYRSTCPY\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"smiles\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 24,\n        \"samples\": [\n          \"[*]OCCCCC(=O)[*].[*]OCCOC(=O)c1ccc(cc1)C(=O)O[*]\",\n          \"[*]OCCCCOC(=O)CC(=O)O[*].[*]OCCOC(=O)CC(=O)O[*].[*]OCCCCC(=O)[*]\",\n          \"[*]OCCCCOC(=O)CC(=O)O[*]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"protein_length\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 130,\n        \"min\": 129,\n        \"max\": 914,\n        \"num_unique_values\": 106,\n        \"samples\": [\n          215,\n          257,\n          342\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"synthetic\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          true\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"cluster\",\n      \"properties\": {\n        \"dtype\": \"int32\",\n        \"num_unique_values\": 17,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "train_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CsZYuP9onEl1"
      },
      "source": [
        "### setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "CpeO8QHj-_z5",
        "outputId": "1825c134-3e7e-41c0-ef15-e781cf4c025c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting Levenshtein\n",
            "  Downloading levenshtein-0.27.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Collecting rapidfuzz<4.0.0,>=3.9.0 (from Levenshtein)\n",
            "  Downloading rapidfuzz-3.12.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Downloading levenshtein-0.27.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (161 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m161.7/161.7 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rapidfuzz-3.12.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rapidfuzz, Levenshtein\n",
            "Successfully installed Levenshtein-0.27.1 rapidfuzz-3.12.2\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (0.8.1)\n",
            "Collecting einops_exts\n",
            "  Downloading einops_exts-0.0.4-py3-none-any.whl.metadata (621 bytes)\n",
            "Requirement already satisfied: einops>=0.4 in /usr/local/lib/python3.11/dist-packages (from einops_exts) (0.8.1)\n",
            "Downloading einops_exts-0.0.4-py3-none-any.whl (3.9 kB)\n",
            "Installing collected packages: einops_exts\n",
            "Successfully installed einops_exts-0.0.4\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m110.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m86.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m51.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m96.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install Levenshtein\n",
        "!pip install einops\n",
        "!pip install einops_exts\n",
        "!pip install torch\n",
        "!pip install transformers\n",
        "!pip install tqdm\n",
        "!pip install sentencepiece\n",
        "# !pip install fair-esm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "CYInyPD_--Vo",
        "outputId": "46b3951c-d499-4b88-b6f6-1f0cd21b6612"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "import pandas as pd\n",
        "import re\n",
        "import math\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "from einops import rearrange, repeat\n",
        "# import esm\n",
        "\n",
        "# Set up GPU if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# # Load ESM-2 model\n",
        "# esm_model, alphabet = esm.pretrained.esm2_t33_650M_UR50D()\n",
        "# batch_converter = alphabet.get_batch_converter()\n",
        "# esm_model = esm_model.to(device)  # Move to GPU if available\n",
        "# esm_model.eval()  # Set to evaluation mode\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B--CLs46QMLR"
      },
      "source": [
        "### data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "DyL8K36gQgPj"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0UGP9rWlQgn_",
        "outputId": "32d0b2e7-3493-4d8d-cca1-4a080c312b05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "aN6Of5EMJ2Ju"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "t6vR0DNfQRUZ"
      },
      "outputs": [],
      "source": [
        "def preprocess_snp_data(file_path):\n",
        "    snp_df = pd.read_csv(file_path)\n",
        "\n",
        "    # Basic preprocessing and length calculations\n",
        "    snp_df['smiles_length'] = snp_df['smiles'].apply(len)\n",
        "    snp_df['protein_length'] = snp_df['protein_sequence'].apply(len)\n",
        "\n",
        "    return snp_df\n",
        "\n",
        "def filter_datasets(dataset):\n",
        "    return dataset[\n",
        "        (dataset['smiles'].notna()) &\n",
        "        (dataset['protein_sequence'].notna()) &\n",
        "        (dataset['smiles_length'] > 0) &\n",
        "        (dataset['protein_length'] > 0)\n",
        "    ]\n",
        "\n",
        "class ProteinGenerationDataset(Dataset):\n",
        "    def __init__(self, dataframe, max_length):\n",
        "        self.dataframe = dataframe\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.dataframe.iloc[idx]\n",
        "        return row['smiles'], row['protein_sequence']\n",
        "\n",
        "def collate_fn(batch):\n",
        "    \"\"\"\n",
        "    Custom collate function to handle padding within batches.\n",
        "    Args:\n",
        "        batch: List of tuples (smiles, protein)\n",
        "    Returns:\n",
        "        Padded and batched tensors\n",
        "    \"\"\"\n",
        "    smiles, proteins = zip(*batch)\n",
        "\n",
        "    # SMILES strings don't need padding as PolyBERT handles that internally\n",
        "    smiles = list(smiles)\n",
        "\n",
        "    # Get max length in this batch for proteins (not exceeding dataset max_length)\n",
        "    max_protein_len = min(max(len(p) for p in proteins), max_length)\n",
        "\n",
        "    # Pad proteins to max length in batch\n",
        "    padded_proteins = []\n",
        "    protein_masks = []\n",
        "\n",
        "    for protein in proteins:\n",
        "        if len(protein) > max_protein_len:\n",
        "            padded = protein[:max_protein_len]\n",
        "            mask = [1] * max_protein_len\n",
        "        else:\n",
        "            padded = protein + ' ' * (max_protein_len - len(protein))\n",
        "            mask = [1] * len(protein) + [0] * (max_protein_len - len(protein))\n",
        "\n",
        "        padded_proteins.append(padded)\n",
        "        protein_masks.append(mask)\n",
        "\n",
        "    return {\n",
        "        'smiles': smiles,\n",
        "        'proteins': padded_proteins,\n",
        "        'protein_masks': torch.tensor(protein_masks, dtype=torch.bool)\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y84XLPeXQIEv"
      },
      "source": [
        "### utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "XEODw7FYQJEy"
      },
      "outputs": [],
      "source": [
        "# Model Components\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super().__init__()\n",
        "        position = torch.arange(max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "        pe = torch.zeros(max_len, 1, d_model)\n",
        "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:x.size(0)]\n",
        "\n",
        "class DoublePositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super().__init__()\n",
        "        # Use the full embedding dimension divided into two halves\n",
        "        self.d_model = d_model\n",
        "        half_dim = d_model // 2\n",
        "\n",
        "        # Create position encodings for both input and output positions\n",
        "        position = torch.arange(max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, half_dim, 2) * (-math.log(10000.0) / half_dim))\n",
        "\n",
        "        # Input position encodings\n",
        "        pe_input = torch.zeros(max_len, half_dim)\n",
        "        pe_input[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe_input[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        # Output position encodings\n",
        "        pe_output = torch.zeros(max_len, half_dim)\n",
        "        pe_output[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe_output[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        self.register_buffer('pe_input', pe_input)\n",
        "        self.register_buffer('pe_output', pe_output)\n",
        "\n",
        "    def forward(self, x, input_positions, output_positions):\n",
        "        batch_size, seq_length, _ = x.shape\n",
        "\n",
        "        # Create a tensor of zeros with the same shape as the input\n",
        "        pos_encoding = torch.zeros_like(x)\n",
        "\n",
        "        # For each item in the batch\n",
        "        for b in range(batch_size):\n",
        "            for t in range(seq_length):\n",
        "                # Get the input and output positions for this token\n",
        "                input_pos = input_positions[b, t] if input_positions is not None else t\n",
        "                output_pos = output_positions[b, t] if output_positions is not None else t\n",
        "\n",
        "                if input_pos < self.pe_input.size(0) and output_pos < self.pe_output.size(0):\n",
        "                    # Fill the first half with input position encoding\n",
        "                    pos_encoding[b, t, :self.d_model//2] = self.pe_input[input_pos]\n",
        "                    # Fill the second half with output position encoding\n",
        "                    pos_encoding[b, t, self.d_model//2:] = self.pe_output[output_pos]\n",
        "\n",
        "        return x + pos_encoding\n",
        "\n",
        "class PerceiverAttention(nn.Module):\n",
        "    def __init__(self, dim, dim_head=64, heads=8):\n",
        "        super().__init__()\n",
        "        self.scale = dim_head ** -0.5\n",
        "        self.heads = heads\n",
        "        inner_dim = dim_head * heads\n",
        "\n",
        "        self.norm_media = nn.LayerNorm(dim)\n",
        "        self.norm_latents = nn.LayerNorm(dim)\n",
        "        self.to_q = nn.Linear(dim, inner_dim, bias=False)\n",
        "        self.to_kv = nn.Linear(dim, inner_dim * 2, bias=False)\n",
        "        self.to_out = nn.Linear(inner_dim, dim, bias=False)\n",
        "\n",
        "    def forward(self, x, latents):\n",
        "        \"\"\"\n",
        "        x: [batch_size, seq_len_x, dim]\n",
        "        latents: [batch_size, seq_len_l, dim]\n",
        "        \"\"\"\n",
        "        batch_size = x.shape[0]\n",
        "\n",
        "        x = self.norm_media(x)\n",
        "        latents = self.norm_latents(latents)\n",
        "\n",
        "        # Ensure latents has correct batch size\n",
        "        if latents.size(0) != batch_size:\n",
        "            latents = latents.expand(batch_size, -1, -1)\n",
        "\n",
        "        q = self.to_q(latents)\n",
        "        q = rearrange(q, 'b n (h d) -> b h n d', h=self.heads)\n",
        "        q = q * self.scale\n",
        "\n",
        "        # Ensure proper concatenation\n",
        "        kv_input = torch.cat((x, latents), dim=1)  # concatenate along sequence dimension\n",
        "        k, v = self.to_kv(kv_input).chunk(2, dim=-1)\n",
        "        k = rearrange(k, 'b n (h d) -> b h n d', h=self.heads)\n",
        "        v = rearrange(v, 'b n (h d) -> b h n d', h=self.heads)\n",
        "\n",
        "        sim = torch.einsum('b h i d, b h j d -> b h i j', q, k)\n",
        "        attn = sim.softmax(dim=-1)\n",
        "        out = torch.einsum('b h i j, b h j d -> b h i d', attn, v)\n",
        "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
        "\n",
        "        return self.to_out(out)\n",
        "\n",
        "class GatedCrossAttentionBlock(nn.Module):\n",
        "    def __init__(self, dim, dim_head=64, heads=8, ff_mult=4):\n",
        "        super().__init__()\n",
        "        self.attn = PerceiverAttention(dim=dim, dim_head=dim_head, heads=heads)\n",
        "        self.attn_gate = nn.Parameter(torch.tensor([0.]))\n",
        "        self.ff = FeedForward(dim, mult=ff_mult)\n",
        "        self.ff_gate = nn.Parameter(torch.tensor([0.]))\n",
        "\n",
        "    def forward(self, x, media):\n",
        "        \"\"\"\n",
        "        x: [batch_size, seq_len_x, dim]\n",
        "        media: [batch_size, seq_len_m, dim]\n",
        "        \"\"\"\n",
        "        batch_size = x.shape[0]\n",
        "        target_batch_size = media.size(0)\n",
        "\n",
        "        # Handle batch size mismatch\n",
        "        if batch_size > target_batch_size:\n",
        "            media = media.expand(batch_size, -1, -1)\n",
        "        elif batch_size < target_batch_size:\n",
        "            x = x.expand(target_batch_size, -1, -1)\n",
        "\n",
        "        gate = self.attn_gate.tanh()\n",
        "        x = self.attn(media, x) * gate + x\n",
        "        x = self.ff(x) * self.ff_gate.tanh() + x\n",
        "        return x\n",
        "\n",
        "class PerceiverResampler(nn.Module):\n",
        "    def __init__(self, dim, depth, dim_head=64, heads=8, num_latents=64):\n",
        "        super().__init__()\n",
        "        # Initialize latents without batch dimension\n",
        "        self.latents = nn.Parameter(torch.randn(num_latents, dim))\n",
        "        self.layers = nn.ModuleList([])\n",
        "\n",
        "        for _ in range(depth):\n",
        "            self.layers.append(nn.ModuleList([\n",
        "                PerceiverAttention(dim=dim, dim_head=dim_head, heads=heads),\n",
        "                FeedForward(dim=dim)\n",
        "            ]))\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.shape[0]\n",
        "        # Expand latents to batch size\n",
        "        latents = repeat(self.latents, 'n d -> b n d', b=batch_size)\n",
        "\n",
        "        for attn, ff in self.layers:\n",
        "            latents = attn(x, latents) + latents\n",
        "            latents = ff(latents) + latents\n",
        "\n",
        "        return latents\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, dim, mult=4):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.LayerNorm(dim),\n",
        "            nn.Linear(dim, dim * mult, bias=False),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(dim * mult, dim, bias=False)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "# class PerceiverResampler(nn.Module):\n",
        "#     def __init__(self, dim, depth, dim_head=64, heads=8, num_latents=64):\n",
        "#         super().__init__()\n",
        "#         self.latents = nn.Parameter(torch.randn(num_latents, dim))\n",
        "#         self.layers = nn.ModuleList([])\n",
        "\n",
        "#         for _ in range(depth):\n",
        "#             self.layers.append(nn.ModuleList([\n",
        "#                 PerceiverAttention(dim=dim, dim_head=dim_head, heads=heads),\n",
        "#                 FeedForward(dim=dim)\n",
        "#             ]))\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         latents = repeat(self.latents, 'n d -> b n d', b=x.shape[0])\n",
        "\n",
        "#         for attn, ff in self.layers:\n",
        "#             latents = attn(x, latents) + latents\n",
        "#             latents = ff(latents) + latents\n",
        "\n",
        "#         return latents\n",
        "\n",
        "# class GatedCrossAttentionBlock(nn.Module):\n",
        "#     def __init__(self, dim, dim_head=64, heads=8, ff_mult=4):\n",
        "#         super().__init__()\n",
        "#         self.attn = PerceiverAttention(dim=dim, dim_head=dim_head, heads=heads)\n",
        "#         self.attn_gate = nn.Parameter(torch.tensor([0.]))\n",
        "#         self.ff = FeedForward(dim, mult=ff_mult)\n",
        "#         self.ff_gate = nn.Parameter(torch.tensor([0.]))\n",
        "\n",
        "#     def forward(self, x, media):\n",
        "#         gate = self.attn_gate.tanh()\n",
        "#         x = self.attn(media, x) * gate + x\n",
        "#         x = self.ff(x) * self.ff_gate.tanh() + x\n",
        "#         return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "daZm9aPyRUAM"
      },
      "source": [
        "### PolyBert Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "00Y8KtCzni07"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "rj3wpaKHRVmv"
      },
      "outputs": [],
      "source": [
        "# class PolyBERTEncoder(nn.Module):\n",
        "#     def __init__(self, output_dim):\n",
        "#         super().__init__()\n",
        "#         self.polybert = AutoModel.from_pretrained('kuelumbus/polyBERT')\n",
        "#         self.tokenizer = AutoTokenizer.from_pretrained('kuelumbus/polyBERT')\n",
        "#         self.output_dim = output_dim\n",
        "#         # Add a projection layer to match the required dimension\n",
        "#         self.projection = nn.Linear(self.polybert.config.hidden_size, output_dim)\n",
        "\n",
        "#     def mean_pooling(self, model_output, attention_mask):\n",
        "#         token_embeddings = model_output[0]\n",
        "#         input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
        "#         return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
        "\n",
        "#     def forward(self, smiles_strings):\n",
        "#         # Tokenize the SMILES strings\n",
        "#         encoded_input = self.tokenizer(smiles_strings,\n",
        "#                                      padding=True,\n",
        "#                                      truncation=True,\n",
        "#                                      return_tensors='pt').to(next(self.polybert.parameters()).device)\n",
        "\n",
        "#         # Get PolyBERT embeddings\n",
        "#         with torch.no_grad():\n",
        "#             model_output = self.polybert(**encoded_input)\n",
        "\n",
        "#         # Debug prints\n",
        "#         print(\"Model Output Keys:\", model_output.keys())  # Check available keys\n",
        "#         # print(\"Last Hidden State:\", model_output.last_hidden_state)\n",
        "#         print(\"Last Hidden State Shape:\", model_output.last_hidden_state.shape)\n",
        "\n",
        "#         # Pool the embeddings\n",
        "#         pooled_output = self.mean_pooling(model_output, encoded_input['attention_mask'])\n",
        "\n",
        "#         # print(\"Pooled Output:\", pooled_output)\n",
        "#         print(\"Pooled Output Shape:\", pooled_output.shape)\n",
        "\n",
        "#         # Project to required dimension\n",
        "#         projected_output = self.projection(pooled_output)\n",
        "\n",
        "#         return projected_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "-2VWy9ecrDNX"
      },
      "outputs": [],
      "source": [
        "class PolyBERTEncoder(nn.Module):\n",
        "    def __init__(self, output_dim):\n",
        "        super().__init__()\n",
        "        self.polybert = AutoModel.from_pretrained('kuelumbus/polyBERT')\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained('kuelumbus/polyBERT')\n",
        "        self.output_dim = output_dim\n",
        "        # Project each token embedding to required dimension\n",
        "        self.projection = nn.Linear(self.polybert.config.hidden_size, output_dim)\n",
        "\n",
        "    def forward(self, smiles_strings):\n",
        "        # Tokenize the SMILES strings\n",
        "        encoded_input = self.tokenizer(smiles_strings,\n",
        "                                     padding=True,\n",
        "                                     truncation=True,\n",
        "                                     return_tensors='pt').to(next(self.polybert.parameters()).device)\n",
        "\n",
        "        # Get PolyBERT embeddings\n",
        "        with torch.no_grad():\n",
        "            model_output = self.polybert(**encoded_input)\n",
        "\n",
        "        # Debug prints\n",
        "        # print(\"Model Output Keys:\", model_output.keys())\n",
        "        # print(\"Last Hidden State Shape:\", model_output.last_hidden_state.shape)  # [batch_size, seq_len, hidden_size]\n",
        "\n",
        "        # Get sequence embeddings\n",
        "        sequence_embeddings = model_output.last_hidden_state\n",
        "\n",
        "        # Project each token embedding to required dimension\n",
        "        projected_output = self.projection(sequence_embeddings)  # [batch_size, seq_len, output_dim]\n",
        "        # print(\"Projected Output Shape:\", projected_output.shape)\n",
        "\n",
        "        return projected_output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Pe8VJFxQSS7"
      },
      "source": [
        "### ProtFlamingo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "d3FCpZZidXy3"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "c1CcCRv-9vxW"
      },
      "outputs": [],
      "source": [
        "class SigmaProtFlamingo(nn.Module):\n",
        "    def __init__(self, model_path, max_len, cross_attn_every=3, dim_head=64, heads=8, perceiver_depth=2, perceiver_num_latents=64):\n",
        "        super().__init__()\n",
        "\n",
        "        self.protGPT2_model = GPT2LMHeadModel.from_pretrained(model_path)\n",
        "        self.protGPT2_tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
        "        self.max_len = max_len\n",
        "\n",
        "        if self.protGPT2_tokenizer.pad_token is None:\n",
        "            self.protGPT2_tokenizer.pad_token = self.protGPT2_tokenizer.eos_token\n",
        "            self.protGPT2_model.config.pad_token_id = self.protGPT2_model.config.eos_token_id\n",
        "\n",
        "        self.cross_attn_every = cross_attn_every\n",
        "\n",
        "        # PolyBERT encoder for SMILES strings\n",
        "        self.polybert_encoder = PolyBERTEncoder(self.protGPT2_model.config.n_embd)\n",
        "\n",
        "        # Replace single positional encoding with double positional encoding\n",
        "        self.positional_encoding = DoublePositionalEncoding(self.protGPT2_model.config.n_embd, max_len=max_len)\n",
        "\n",
        "        # Single perceiver resampler for SMILES embeddings\n",
        "        self.smiles_perceiver = PerceiverResampler(\n",
        "            dim=self.protGPT2_model.config.n_embd,\n",
        "            depth=perceiver_depth,\n",
        "            dim_head=dim_head,\n",
        "            heads=heads,\n",
        "            num_latents=perceiver_num_latents\n",
        "        )\n",
        "\n",
        "        # Cross attention layers\n",
        "        num_gpt_layers = len(self.protGPT2_model.transformer.h)\n",
        "        self.cross_attn = nn.ModuleList([\n",
        "            GatedCrossAttentionBlock(dim=self.protGPT2_model.config.n_embd, dim_head=dim_head, heads=heads)\n",
        "            for _ in range(num_gpt_layers)\n",
        "        ])\n",
        "\n",
        "        # Combine GPT layers with cross attention\n",
        "        self.layers = nn.ModuleList()\n",
        "        for i, block in enumerate(self.protGPT2_model.transformer.h):\n",
        "            self.layers.append(block)\n",
        "            if i % cross_attn_every == 0 and i != 0:\n",
        "                self.layers.append(GatedCrossAttentionBlock(dim=self.protGPT2_model.config.n_embd, dim_head=dim_head, heads=heads))\n",
        "\n",
        "    def forward(self, smiles_strings, order=None, targets=None, optimize=False, kv_cache=None, burst=False):\n",
        "        device = next(self.parameters()).device\n",
        "\n",
        "        # Get SMILES embeddings through PolyBERT\n",
        "        smiles_embeddings = self.polybert_encoder(smiles_strings)\n",
        "        processed_smiles = self.smiles_perceiver(smiles_embeddings)\n",
        "\n",
        "        # Initialize with start token\n",
        "        gpt_input = self.protGPT2_tokenizer.encode_plus(\n",
        "            \"<|endoftext|>\",\n",
        "            return_tensors=\"pt\",\n",
        "            padding='max_length',\n",
        "            max_length=self.max_len,\n",
        "            truncation=True\n",
        "        ).to(device)\n",
        "\n",
        "        input_ids = gpt_input.input_ids.long()\n",
        "        seq_length = input_ids.size(1)\n",
        "        batch_size = 1 if isinstance(smiles_strings, str) else len(smiles_strings)\n",
        "\n",
        "        hidden_states = self.protGPT2_model.transformer.wte(input_ids)\n",
        "\n",
        "        # If no order is provided, use left-to-right\n",
        "        if order is None:\n",
        "            order = torch.arange(seq_length, device=device).unsqueeze(0).repeat(batch_size, 1)\n",
        "\n",
        "        # Make sure order is the right length\n",
        "        if order.size(1) > seq_length:\n",
        "            order = order[:, :seq_length]\n",
        "        elif order.size(1) < seq_length:\n",
        "            # Pad order if needed\n",
        "            padding = torch.arange(order.size(1), seq_length, device=device).unsqueeze(0).repeat(batch_size, 1)\n",
        "            order = torch.cat([order, padding], dim=1)\n",
        "\n",
        "        # Map the input tokens according to the order\n",
        "        # When using random order, we need to reshuffle the input tokens\n",
        "        if not optimize and not burst:  # Only shuffle during training\n",
        "            reordered_input_ids = torch.zeros_like(input_ids)\n",
        "            for b in range(batch_size):\n",
        "                # Reorder the input tokens according to the order\n",
        "                reordered_input_ids[b] = input_ids[b, order[b]]\n",
        "\n",
        "            # Re-embed with reordered tokens\n",
        "            hidden_states = self.protGPT2_model.transformer.wte(reordered_input_ids)\n",
        "\n",
        "        # Get input and output positions from the order\n",
        "        # Input positions: the current position in the order\n",
        "        # Output positions: the next position in the order\n",
        "        input_positions = order\n",
        "        # Shift the order by 1 to get output positions (target positions)\n",
        "        output_positions = torch.roll(order, -1, dims=1)\n",
        "        # The last position wraps to the first position\n",
        "        output_positions[:, -1] = order[:, 0]\n",
        "\n",
        "        # Apply double positional encoding\n",
        "        hidden_states = self.positional_encoding(hidden_states, input_positions, output_positions)\n",
        "\n",
        "        # Create attention mask based on the order\n",
        "        attention_mask = gpt_input.attention_mask\n",
        "        num_heads = self.protGPT2_model.config.n_head\n",
        "\n",
        "        # Create 4D attention mask [batch_size, num_heads, seq_length, seq_length]\n",
        "        attention_mask = attention_mask.view(batch_size, 1, 1, seq_length)\n",
        "        attention_mask = attention_mask.expand(batch_size, num_heads, seq_length, seq_length)\n",
        "        attention_mask = attention_mask.to(dtype=hidden_states.dtype)\n",
        "\n",
        "        # Create causal mask based on the order\n",
        "        # A token at position i can attend to tokens at positions j where order[j] <= order[i]\n",
        "        # Vectorized causal mask creation\n",
        "        seq_indices = torch.arange(seq_length, device=device)\n",
        "        expanded_seq_indices_i = seq_indices.unsqueeze(1).expand(seq_length, seq_length)\n",
        "        expanded_seq_indices_j = seq_indices.unsqueeze(0).expand(seq_length, seq_length)\n",
        "\n",
        "        causal_mask = torch.zeros((batch_size, seq_length, seq_length), device=device)\n",
        "        for b in range(batch_size):\n",
        "            # Get order for this batch\n",
        "            order_b = order[b]\n",
        "            # Get order values at positions i and j\n",
        "            order_i = order_b[expanded_seq_indices_i]\n",
        "            order_j = order_b[expanded_seq_indices_j]\n",
        "            # Create mask where order_j <= order_i\n",
        "            causal_mask[b] = (order_j <= order_i).float()\n",
        "\n",
        "        # Reshape causal_mask to match attention_mask and combine them\n",
        "        causal_mask = causal_mask.unsqueeze(1)  # [batch_size, 1, seq_length, seq_length]\n",
        "        combined_mask = attention_mask * causal_mask\n",
        "\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            if isinstance(layer, GatedCrossAttentionBlock):\n",
        "                hidden_states = layer(hidden_states, processed_smiles)\n",
        "            else:\n",
        "                hidden_states = layer(hidden_states, attention_mask=combined_mask)[0]\n",
        "\n",
        "        # Get logits\n",
        "        logits = self.protGPT2_model.lm_head(hidden_states)\n",
        "\n",
        "        if targets is None:\n",
        "            if optimize:\n",
        "                # inference-time mini-optimization: only forward the lm_head on the very last position\n",
        "                return logits[:, [-1], :], None\n",
        "            return logits, None\n",
        "\n",
        "        # Compute loss against the targets\n",
        "        # If targets are provided in original order, we need to shuffle them to match our order\n",
        "        if targets is not None:\n",
        "            shuffled_targets = torch.zeros_like(targets)\n",
        "            for b in range(batch_size):\n",
        "                # Reorder the targets according to the order\n",
        "                shuffled_targets[b] = targets[b, order[b]]\n",
        "\n",
        "            loss = F.cross_entropy(\n",
        "                logits.view(-1, logits.size(-1)),\n",
        "                shuffled_targets.view(-1),\n",
        "                ignore_index=-1\n",
        "            )\n",
        "        else:\n",
        "            loss = None\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def custom_generate(self, smiles_string, max_length=200):\n",
        "        device = next(self.parameters()).device\n",
        "\n",
        "        # Get SMILES embeddings\n",
        "        smiles_embeddings = self.polybert_encoder(smiles_string)\n",
        "        processed_smiles = self.smiles_perceiver(smiles_embeddings)\n",
        "\n",
        "        # Initialize with start token\n",
        "        input_ids = torch.tensor([[self.protGPT2_tokenizer.bos_token_id]]).to(device)\n",
        "\n",
        "        # Autoregressive generation\n",
        "        for _ in range(max_length):\n",
        "            inputs_embeds = self.protGPT2_model.transformer.wte(input_ids)\n",
        "            inputs_embeds = self.positional_encoding(inputs_embeds)\n",
        "\n",
        "            hidden_states = inputs_embeds\n",
        "            cross_attn_idx = 0\n",
        "\n",
        "            for i, layer in enumerate(self.layers):\n",
        "                if isinstance(layer, GatedCrossAttentionBlock):\n",
        "                    hidden_states = layer(hidden_states, processed_smiles)\n",
        "                    cross_attn_idx += 1\n",
        "                else:\n",
        "                    hidden_states = layer(hidden_states, attention_mask=None)[0]\n",
        "\n",
        "            next_token_logits = self.protGPT2_model.lm_head(hidden_states[:, -1, :])\n",
        "            next_token = torch.argmax(next_token_logits, dim=-1).unsqueeze(0)\n",
        "\n",
        "            input_ids = torch.cat([input_ids, next_token], dim=-1)\n",
        "\n",
        "            if next_token.item() == self.protGPT2_tokenizer.eos_token_id:\n",
        "                break\n",
        "\n",
        "        return self.protGPT2_tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    def generate(self, smiles_string, max_length=50):\n",
        "        return self.custom_generate(smiles_string, max_length)\n",
        "\n",
        "    def state_dict(self):\n",
        "        state_dict = super().state_dict()\n",
        "        state_dict['smiles_perceiver'] = self.smiles_perceiver.state_dict()\n",
        "        state_dict['cross_attn'] = self.cross_attn.state_dict()\n",
        "        state_dict['polybert_encoder'] = self.polybert_encoder.state_dict()\n",
        "        return state_dict\n",
        "\n",
        "    def load_state_dict(self, state_dict):\n",
        "        smiles_perceiver_state = state_dict.pop('smiles_perceiver')\n",
        "        cross_attn_state = state_dict.pop('cross_attn')\n",
        "        polybert_encoder_state = state_dict.pop('polybert_encoder')\n",
        "\n",
        "        super().load_state_dict(state_dict)\n",
        "\n",
        "        self.smiles_perceiver.load_state_dict(smiles_perceiver_state)\n",
        "        self.cross_attn.load_state_dict(cross_attn_state)\n",
        "        self.polybert_encoder.load_state_dict(polybert_encoder_state)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "_wnL43mTZkIN"
      },
      "outputs": [],
      "source": [
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6o91I0EXQaF4"
      },
      "source": [
        "### training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "wWmS0warQa-n"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "import pandas as pd\n",
        "import os\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "M6WRIYHEfN6y"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import os\n",
        "import random\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import math\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "import Levenshtein\n",
        "\n",
        "\n",
        "\n",
        "def print_model_structure(model):\n",
        "    print(\"\\n===== MODEL STRUCTURE ANALYSIS =====\")\n",
        "\n",
        "    # 1. Check which layers have cross-attention\n",
        "    cross_attn_locations = []\n",
        "    for i, layer in enumerate(model.layers):\n",
        "        if isinstance(layer, GatedCrossAttentionBlock):\n",
        "            cross_attn_locations.append(i)\n",
        "\n",
        "    print(f\"\\n📌 CROSS-ATTENTION LAYERS:\")\n",
        "    print(f\"  Total cross-attention blocks: {len(cross_attn_locations)}\")\n",
        "    print(f\"  Located at positions: {cross_attn_locations}\")\n",
        "\n",
        "    # 2. Check parameter freezing by group\n",
        "    frozen_info = {}\n",
        "    trainable_info = {}\n",
        "    total_frozen = 0\n",
        "    total_trainable = 0\n",
        "\n",
        "    # Get lm_head status if it exists\n",
        "    lm_head_status = \"NOT FOUND\"\n",
        "    if hasattr(model.protGPT2_model, 'lm_head'):\n",
        "        if isinstance(model.protGPT2_model.lm_head, GatedCrossAttentionBlock):\n",
        "            lm_head_status = \"REPLACED with GatedCrossAttentionBlock\"\n",
        "        else:\n",
        "            lm_head_trainable = all(p.requires_grad for p in model.protGPT2_model.lm_head.parameters())\n",
        "            lm_head_status = \"TRAINABLE\" if lm_head_trainable else \"FROZEN\"\n",
        "\n",
        "            # Count parameters\n",
        "            lm_head_params = sum(p.numel() for p in model.protGPT2_model.lm_head.parameters())\n",
        "            if lm_head_trainable:\n",
        "                total_trainable += lm_head_params\n",
        "                trainable_info[\"lm_head\"] = lm_head_params\n",
        "            else:\n",
        "                total_frozen += lm_head_params\n",
        "                frozen_info[\"lm_head\"] = lm_head_params\n",
        "\n",
        "    # Check transformer layer status\n",
        "    for i in range(36):  # Assuming 36 transformer layers\n",
        "        layer_name = f\"transformer.h.{i}\"\n",
        "\n",
        "        # Find parameters for this layer\n",
        "        layer_params = []\n",
        "        for name, param in model.protGPT2_model.named_parameters():\n",
        "            if f\"transformer.h.{i}.\" in name:\n",
        "                layer_params.append(param)\n",
        "\n",
        "        if layer_params:\n",
        "            layer_trainable = all(p.requires_grad for p in layer_params)\n",
        "            layer_status = \"TRAINABLE\" if layer_trainable else \"FROZEN\"\n",
        "\n",
        "            # Count parameters\n",
        "            layer_param_count = sum(p.numel() for p in layer_params)\n",
        "            if layer_trainable:\n",
        "                total_trainable += layer_param_count\n",
        "                trainable_info[layer_name] = layer_param_count\n",
        "            else:\n",
        "                total_frozen += layer_param_count\n",
        "                frozen_info[layer_name] = layer_param_count\n",
        "\n",
        "    # Print layer freezing status\n",
        "    print(f\"\\n📌 LAYER FREEZING STATUS:\")\n",
        "    for i in range(36):\n",
        "        layer_name = f\"transformer.h.{i}\"\n",
        "        if layer_name in trainable_info:\n",
        "            print(f\"  Layer {i:2d}: ✅ TRAINABLE ({trainable_info[layer_name]:,} params)\")\n",
        "        elif layer_name in frozen_info:\n",
        "            print(f\"  Layer {i:2d}: ❄️ FROZEN ({frozen_info[layer_name]:,} params)\")\n",
        "        else:\n",
        "            print(f\"  Layer {i:2d}: ⚠️ NOT FOUND\")\n",
        "\n",
        "    print(f\"\\n  LM Head: {lm_head_status}\")\n",
        "\n",
        "    # Print overall stats\n",
        "    total_params = total_trainable + total_frozen\n",
        "    print(f\"\\n📌 PARAMETER SUMMARY:\")\n",
        "    print(f\"  Total parameters: {total_params:,}\")\n",
        "    print(f\"  Trainable parameters: {total_trainable:,} ({total_trainable/total_params:.2%})\")\n",
        "    print(f\"  Frozen parameters: {total_frozen:,} ({total_frozen/total_params:.2%})\")\n",
        "\n",
        "    # Cross-attention specific info\n",
        "    print(f\"\\n📌 CROSS-ATTENTION DETAILS:\")\n",
        "    cross_attn_count = 0\n",
        "    for i, layer in enumerate(model.layers):\n",
        "        if isinstance(layer, GatedCrossAttentionBlock):\n",
        "            cross_attn_count += 1\n",
        "            cross_attn_params = sum(p.numel() for p in layer.parameters())\n",
        "            trainable = all(p.requires_grad for p in layer.parameters())\n",
        "            print(f\"  Cross-Attention #{cross_attn_count} (index {i}): {'✅ TRAINABLE' if trainable else '❄️ FROZEN'} ({cross_attn_params:,} params)\")\n",
        "\n",
        "    print(\"\\n===================================\\n\")\n",
        "\n",
        "# Add this function to model class to get outputs without computing loss internally\n",
        "def add_forward_without_loss_to_model(model):\n",
        "    \"\"\"\n",
        "    Adds a new method to the model to get outputs without computing loss.\n",
        "    Call this function before starting training.\n",
        "    \"\"\"\n",
        "    def forward_without_loss(self, smiles_strings, targets=None):\n",
        "        \"\"\"Get model outputs without computing loss internally for SigmaProtFlamingo\"\"\"\n",
        "        device = next(self.parameters()).device\n",
        "\n",
        "        # Get SMILES embeddings through PolyBERT\n",
        "        smiles_embeddings = self.polybert_encoder(smiles_strings)\n",
        "        processed_smiles = self.smiles_perceiver(smiles_embeddings)\n",
        "\n",
        "        # Initialize with start token\n",
        "        gpt_input = self.protGPT2_tokenizer(\n",
        "            \"<|endoftext|>\",\n",
        "            return_tensors=\"pt\",\n",
        "            padding='max_length',\n",
        "            max_length=self.max_len,\n",
        "            truncation=True\n",
        "        ).to(device)\n",
        "\n",
        "        input_ids = gpt_input.input_ids.long()\n",
        "        seq_length = input_ids.size(1)\n",
        "        batch_size = 1 if isinstance(smiles_strings, str) else len(smiles_strings)\n",
        "\n",
        "        hidden_states = self.protGPT2_model.transformer.wte(input_ids)\n",
        "\n",
        "        # Use left-to-right order for training with AAR focus\n",
        "        order = torch.arange(seq_length, device=device).unsqueeze(0).repeat(batch_size, 1)\n",
        "\n",
        "        # Apply double positional encoding\n",
        "        input_positions = order\n",
        "        output_positions = torch.roll(order, -1, dims=1)\n",
        "        output_positions[:, -1] = order[:, 0]\n",
        "\n",
        "        hidden_states = self.positional_encoding(hidden_states, input_positions, output_positions)\n",
        "\n",
        "        # Create attention mask\n",
        "        attention_mask = gpt_input.attention_mask\n",
        "        num_heads = self.protGPT2_model.config.n_head\n",
        "\n",
        "        # Create 4D attention mask [batch_size, num_heads, seq_length, seq_length]\n",
        "        attention_mask = attention_mask.view(batch_size, 1, 1, seq_length)\n",
        "        attention_mask = attention_mask.expand(batch_size, num_heads, seq_length, seq_length)\n",
        "        attention_mask = attention_mask.to(dtype=hidden_states.dtype)\n",
        "\n",
        "        # Create causal mask based on left-to-right order\n",
        "        seq_indices = torch.arange(seq_length, device=device)\n",
        "        expanded_seq_indices_i = seq_indices.unsqueeze(1).expand(seq_length, seq_length)\n",
        "        expanded_seq_indices_j = seq_indices.unsqueeze(0).expand(seq_length, seq_length)\n",
        "\n",
        "        causal_mask = torch.zeros((batch_size, seq_length, seq_length), device=device)\n",
        "        for b in range(batch_size):\n",
        "            order_b = order[b]\n",
        "            order_i = order_b[expanded_seq_indices_i]\n",
        "            order_j = order_b[expanded_seq_indices_j]\n",
        "            causal_mask[b] = (order_j <= order_i).float()\n",
        "\n",
        "        # Reshape causal_mask to match attention_mask and combine them\n",
        "        causal_mask = causal_mask.unsqueeze(1)  # [batch_size, 1, seq_length, seq_length]\n",
        "        combined_mask = attention_mask * causal_mask\n",
        "\n",
        "        # Process through all layers\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            if isinstance(layer, GatedCrossAttentionBlock):\n",
        "                hidden_states = layer(hidden_states, processed_smiles)\n",
        "            else:\n",
        "                hidden_states = layer(hidden_states, attention_mask=combined_mask)[0]\n",
        "\n",
        "        # Get logits without computing loss\n",
        "        logits = self.protGPT2_model.lm_head(hidden_states)\n",
        "\n",
        "        return logits\n",
        "\n",
        "    # Add the method to the model\n",
        "    model.forward_without_loss = forward_without_loss.__get__(model, type(model))\n",
        "    return model\n",
        "\n",
        "def repetition_penalty_loss(predicted_token_ids, target_token_ids, pad_token_id):\n",
        "    \"\"\"\n",
        "    Penalizes repeated amino acid tokens in a sequence.\n",
        "    Returns a penalty value for consecutive repeated tokens.\n",
        "    \"\"\"\n",
        "    # Ignore padding tokens\n",
        "    mask = target_token_ids != pad_token_id\n",
        "\n",
        "    # Shift the sequence by one token to compare\n",
        "    prev_tokens = predicted_token_ids[:, :-1]\n",
        "    curr_tokens = predicted_token_ids[:, 1:]\n",
        "\n",
        "    # Compute a repetition mask (1 if consecutive tokens are the same, 0 otherwise)\n",
        "    repetition_mask = (prev_tokens == curr_tokens).float()\n",
        "\n",
        "    # Apply the mask to only count valid (non-pad) regions\n",
        "    valid_mask = mask[:, 1:].float()\n",
        "    repetition_penalty = (repetition_mask * valid_mask).sum() / (valid_mask.sum() + 1e-8)\n",
        "\n",
        "    return repetition_penalty\n",
        "\n",
        "def sequence_diversity_loss(predicted_logits, pad_token_id, vocab_size):\n",
        "    \"\"\"\n",
        "    Encourages diversity in token distribution across the sequence.\n",
        "    Penalizes sequences that use a limited set of amino acids.\n",
        "    \"\"\"\n",
        "    # Get predicted token ids\n",
        "    predicted_token_ids = torch.argmax(predicted_logits, dim=-1)\n",
        "\n",
        "    # Create mask to ignore padding\n",
        "    mask = predicted_token_ids != pad_token_id\n",
        "\n",
        "    # Count frequency of each token\n",
        "    batch_size = predicted_token_ids.size(0)\n",
        "    token_counts = torch.zeros(batch_size, vocab_size, device=predicted_logits.device)\n",
        "\n",
        "    # For each sequence in the batch\n",
        "    diversity_loss = 0.0\n",
        "    for b in range(batch_size):\n",
        "        # Count tokens in this sequence (excluding padding)\n",
        "        seq_tokens = predicted_token_ids[b][mask[b]]\n",
        "        if len(seq_tokens) == 0:\n",
        "            continue\n",
        "\n",
        "        # Count each token\n",
        "        for t in range(vocab_size):\n",
        "            token_counts[b, t] = (seq_tokens == t).sum()\n",
        "\n",
        "        # Normalize to get probability distribution\n",
        "        token_probs = token_counts[b] / (len(seq_tokens) + 1e-8)\n",
        "\n",
        "        # Calculate entropy (higher is more diverse)\n",
        "        # We negate entropy to make it a loss (lower is better)\n",
        "        non_zero_probs = token_probs[token_probs > 0]\n",
        "        entropy = -torch.sum(non_zero_probs * torch.log(non_zero_probs + 1e-8))\n",
        "\n",
        "        # Add to batch loss with negation (we want to maximize entropy = diversity)\n",
        "        diversity_loss += -entropy\n",
        "\n",
        "    # Average over batch\n",
        "    return diversity_loss / batch_size\n",
        "\n",
        "def sequence_similarity_loss(predicted_tokens, target_tokens, tokenizer):\n",
        "    \"\"\"\n",
        "    Calculate a loss based on sequence-level similarity.\n",
        "    Uses normalized Levenshtein distance.\n",
        "    \"\"\"\n",
        "    batch_size = predicted_tokens.shape[0]\n",
        "    total_distance = 0.0\n",
        "\n",
        "    for i in range(batch_size):\n",
        "        # Convert token IDs to strings, skipping padding and special tokens\n",
        "        pred_seq = tokenizer.decode(predicted_tokens[i], skip_special_tokens=True)\n",
        "        target_seq = tokenizer.decode(target_tokens[i], skip_special_tokens=True)\n",
        "\n",
        "        # Calculate normalized Levenshtein distance\n",
        "        if len(target_seq) > 0:\n",
        "            distance = Levenshtein.distance(pred_seq, target_seq) / max(len(target_seq), 1)\n",
        "            total_distance += distance\n",
        "\n",
        "    return total_distance / batch_size\n",
        "\n",
        "\n",
        "def apply_token_masking(input_ids, tokenizer, mask_prob=0.15):\n",
        "    masked_input_ids = input_ids.clone()\n",
        "    labels = input_ids.clone()\n",
        "\n",
        "    # Create mask for tokens that can be masked (exclude padding)\n",
        "    padding_mask = input_ids != tokenizer.pad_token_id\n",
        "\n",
        "    # Generate random mask with specified probability\n",
        "    random_mask = torch.rand(input_ids.shape, device=input_ids.device) < mask_prob\n",
        "\n",
        "    # Only apply masking to non-padding tokens\n",
        "    mask_indices = padding_mask & random_mask\n",
        "\n",
        "    # Replace masked tokens with mask token\n",
        "    if tokenizer.mask_token_id is not None:\n",
        "        mask_token_id = tokenizer.mask_token_id\n",
        "    else:\n",
        "        # If model doesn't have a mask token, use a special token or UNK token\n",
        "        mask_token_id = tokenizer.unk_token_id\n",
        "\n",
        "    masked_input_ids[mask_indices] = mask_token_id\n",
        "\n",
        "    # For the loss computation, we only want to predict the masked tokens\n",
        "    # Set labels to -100 for non-masked tokens (CrossEntropyLoss will ignore these)\n",
        "    labels[~mask_indices] = -100\n",
        "\n",
        "    return masked_input_ids, labels\n",
        "\n",
        "\n",
        "def train_with_improved_aar_objective(model, train_loader, val_loader, num_epochs, device,\n",
        "                           curriculum_steps=0, l2_reg=1e-5, sample_smiles=None):\n",
        "    # Add the forward_without_loss method to the model\n",
        "    model = add_forward_without_loss_to_model(model)\n",
        "\n",
        "    optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=5e-5, weight_decay=l2_reg)\n",
        "\n",
        "    # Use label smoothing to prevent overconfident predictions\n",
        "    criterion = nn.CrossEntropyLoss(\n",
        "        ignore_index=-100,  # Changed from pad_token_id to -100 for MLM\n",
        "        reduction='none',\n",
        "        label_smoothing=0.1\n",
        "    )\n",
        "\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "\n",
        "    # Hyperparameters for different loss components\n",
        "    lambda_rep = 0.2       # Weight for repetition penalty\n",
        "    # lambda_div = 0.1       # Weight for diversity loss\n",
        "    lambda_seq = 0.15      # Weight for sequence-level similarity loss\n",
        "\n",
        "    # MLM parameters\n",
        "    use_mlm = True\n",
        "    mlm_prob = 0.15\n",
        "\n",
        "    loss_log = []\n",
        "    new_checkpoint_dir = \"/content/drive/MyDrive/classes+projects/plastic_enzyme_project/2024/codes/sigma_ckpt_full_dataset_enhanced_metrics\"\n",
        "    os.makedirs(new_checkpoint_dir, exist_ok=True)\n",
        "\n",
        "    best_val_aar = 0.0  # Track best validation AAR instead of loss\n",
        "\n",
        "    vocab_size = model.protGPT2_model.config.vocab_size\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        total_correct = 0\n",
        "        total_tokens = 0\n",
        "\n",
        "        # Calculate curriculum ratio (if using curriculum learning)\n",
        "        curriculum_ratio = min(1.0, epoch / (num_epochs / 2)) if curriculum_steps > 0 else 1.0\n",
        "        print(f\"Curriculum ratio: {curriculum_ratio:.2f}\")\n",
        "\n",
        "        for batch_idx, batch in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")):\n",
        "            smiles_strings = batch['smiles']\n",
        "            proteins = batch['proteins']\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            target_encoding = model.protGPT2_tokenizer(\n",
        "                proteins,\n",
        "                return_tensors=\"pt\",\n",
        "                padding='max_length',\n",
        "                max_length=model.max_len,\n",
        "                truncation=True\n",
        "            ).to(device)\n",
        "\n",
        "            # Alternate between MLM and autoregressive training\n",
        "            use_masking_this_batch = use_mlm and (epoch % 2 == 0 or batch_idx % 2 == 0)\n",
        "            if use_masking_this_batch:\n",
        "                masked_input_ids, mlm_labels = apply_token_masking(\n",
        "                    target_encoding.input_ids,\n",
        "                    model.protGPT2_tokenizer,\n",
        "                    mask_prob=mlm_prob\n",
        "                )\n",
        "\n",
        "                # Get model outputs with masked input\n",
        "                outputs = model.forward_without_loss(smiles_strings, masked_input_ids)\n",
        "\n",
        "                # Calculate loss only on masked positions\n",
        "                token_loss = criterion(outputs.view(-1, outputs.size(-1)), mlm_labels.view(-1))\n",
        "                token_loss = token_loss.view(outputs.size(0), -1)\n",
        "\n",
        "                # Create mask for valid tokens (not -100)\n",
        "                valid_mask = mlm_labels != -100\n",
        "            else:\n",
        "                # Original approach without masking\n",
        "                outputs = model.forward_without_loss(smiles_strings, target_encoding.input_ids)\n",
        "                token_loss = criterion(outputs.view(-1, outputs.size(-1)), target_encoding.input_ids.view(-1))\n",
        "                token_loss = token_loss.view(outputs.size(0), -1)\n",
        "                valid_mask = target_encoding.input_ids != model.protGPT2_tokenizer.pad_token_id\n",
        "\n",
        "            # Calculate token-level accuracy\n",
        "            predicted_token_ids = torch.argmax(outputs, dim=-1)\n",
        "\n",
        "            # Ensure predicted_token_ids has the same shape\n",
        "            if predicted_token_ids.shape[1] < model.max_len:\n",
        "                predicted_token_ids = torch.nn.functional.pad(\n",
        "                    predicted_token_ids,\n",
        "                    (0, model.max_len - predicted_token_ids.shape[1]),\n",
        "                    value=model.protGPT2_tokenizer.pad_token_id\n",
        "                )\n",
        "\n",
        "            # For MLM, we only check accuracy on masked positions\n",
        "            if use_mlm:\n",
        "                original_ids = target_encoding.input_ids.clone()\n",
        "                mask_positions = mlm_labels != -100\n",
        "\n",
        "                # Compute accuracy only on masked positions\n",
        "                token_correct = (predicted_token_ids == original_ids) & mask_positions\n",
        "                pad_mask = mask_positions\n",
        "            else:\n",
        "                # Original accuracy calculation\n",
        "                pad_mask = target_encoding.input_ids != model.protGPT2_tokenizer.pad_token_id\n",
        "                token_correct = (predicted_token_ids == target_encoding.input_ids) & pad_mask\n",
        "\n",
        "            # Create a weighting mask that balances correct and incorrect predictions\n",
        "            incorrect_weight = 1.5 + curriculum_ratio * 0.5  # Weight increases from 1.5 to 2.0 over training\n",
        "            weight_mask = (~token_correct).float() * incorrect_weight + 1.0\n",
        "            weight_mask = weight_mask * valid_mask.float()  # Zero out padding or non-masked tokens\n",
        "\n",
        "            # Apply the weighting mask to the token losses\n",
        "            weighted_loss = (token_loss * weight_mask).sum() / (weight_mask.sum() + 1e-8)\n",
        "\n",
        "            # Calculate repetition penalty\n",
        "            rep_penalty = repetition_penalty_loss(\n",
        "                predicted_token_ids,\n",
        "                target_encoding.input_ids,\n",
        "                model.protGPT2_tokenizer.pad_token_id\n",
        "            )\n",
        "\n",
        "            # # Calculate diversity loss\n",
        "            # div_loss = sequence_diversity_loss(\n",
        "            #     outputs,\n",
        "            #     model.protGPT2_tokenizer.pad_token_id,\n",
        "            #     vocab_size\n",
        "            # )\n",
        "\n",
        "            # Calculate sequence-level similarity loss\n",
        "            seq_loss = sequence_similarity_loss(\n",
        "                predicted_token_ids,\n",
        "                target_encoding.input_ids,\n",
        "                model.protGPT2_tokenizer\n",
        "            )\n",
        "\n",
        "            # Combine all losses with appropriate weights\n",
        "            total_loss_val = (\n",
        "                weighted_loss +\n",
        "                lambda_rep * rep_penalty +\n",
        "                # lambda_div * div_loss +\n",
        "                lambda_seq * seq_loss\n",
        "            )\n",
        "\n",
        "            # Print all loss components every 50 batches for monitoring\n",
        "            if batch_idx % 50 == 0:\n",
        "                print(f\"  Loss components: CE={weighted_loss:.4f}, Rep={rep_penalty:.4f}, Seq={seq_loss:.4f}\")\n",
        "\n",
        "            # Use the combined loss for backpropagation\n",
        "            total_loss_val.backward()\n",
        "\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            # For logging purposes, calculate the standard loss\n",
        "            standard_loss = token_loss[valid_mask].mean().item() if valid_mask.sum() > 0 else 0\n",
        "            total_loss += standard_loss\n",
        "\n",
        "            # Calculate AAR metrics for logging\n",
        "            correct = token_correct.sum().item()\n",
        "            total = pad_mask.sum().item()\n",
        "            total_correct += correct\n",
        "            total_tokens += total\n",
        "\n",
        "            # Print batch statistics occasionally\n",
        "            if batch_idx % 10 == 0:\n",
        "                batch_aar = (correct / total * 100) if total > 0 else 0\n",
        "                print(f\"  Batch {batch_idx}: Loss={standard_loss:.4f}, AAR={batch_aar:.2f}%\")\n",
        "\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        amino_acid_recovery = total_correct / total_tokens * 100\n",
        "\n",
        "        # Prevent overflow when calculating perplexity\n",
        "        try:\n",
        "            perplexity = math.exp(avg_loss)\n",
        "        except OverflowError:\n",
        "            perplexity = float('inf')  # Return infinity if the loss is too high\n",
        "\n",
        "        print(f\"\\nEpoch {epoch+1}/{num_epochs}, Average Loss: {avg_loss:.4f}\")\n",
        "        print(f\"Perplexity: {perplexity}\")\n",
        "        print(f\"Amino Acid Recovery: {amino_acid_recovery:.2f}%\")\n",
        "\n",
        "        val_loss, val_perplexity, val_aar, val_results = validate_with_enhanced_metrics(model, val_loader, nn.CrossEntropyLoss(ignore_index=model.protGPT2_tokenizer.pad_token_id), device)\n",
        "        print(f\"Validation Loss: {val_loss:.4f}, Perplexity: {val_perplexity:.4f}, Amino Acid Recovery: {val_aar:.2f}%\")\n",
        "\n",
        "        # Save validation results\n",
        "        import json\n",
        "        json.dump(val_results, open(\"validation_results.json\", \"w\"), indent=4)\n",
        "\n",
        "        loss_log.append({\n",
        "            'epoch': epoch+1,\n",
        "            'train_loss': avg_loss,\n",
        "            'train_perplexity': perplexity,\n",
        "            'train_accuracy': amino_acid_recovery,\n",
        "            'val_loss': val_loss,\n",
        "            'val_perplexity': val_perplexity,\n",
        "            'val_accuracy': val_aar\n",
        "        })\n",
        "\n",
        "        checkpoint_path = os.path.join(new_checkpoint_dir, f\"sigma_epoch_{epoch+1}.pth\")\n",
        "\n",
        "        # Save checkpoint based on better AAR, but also consider perplexity\n",
        "        if val_aar > best_val_aar:\n",
        "            best_val_aar = val_aar\n",
        "            torch.save(model.state_dict(), checkpoint_path)\n",
        "            print(f\"Checkpoint saved at {checkpoint_path} (Validation AAR improved to {best_val_aar:.2f}%)\")\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "    loss_df = pd.DataFrame(loss_log)\n",
        "    loss_df.to_csv(\"/content/drive/MyDrive/classes+projects/plastic_enzyme_project/2024/codes/sigma_improved_aar_log.csv\", index=False)\n",
        "\n",
        "    # Plot training metrics\n",
        "    plt.figure(figsize=(15, 10))\n",
        "\n",
        "    # Filter out invalid values for plotting\n",
        "    loss_df['train_perplexity_plot'] = loss_df['train_perplexity'].apply(lambda x: x if x != -1 and x < 1000 else None)\n",
        "    loss_df['val_perplexity_plot'] = loss_df['val_perplexity'].apply(lambda x: x if x != -1 and x < 1000 else None)\n",
        "\n",
        "    plt.subplot(2, 2, 1)\n",
        "    plt.plot(loss_df['epoch'], loss_df['train_loss'], label='Train')\n",
        "    plt.plot(loss_df['epoch'], loss_df['val_loss'], label='Validation')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.title('Loss vs. Epoch')\n",
        "\n",
        "    plt.subplot(2, 2, 2)\n",
        "    plt.plot(loss_df['epoch'], loss_df['train_perplexity'], label='Train')\n",
        "    plt.plot(loss_df['epoch'], loss_df['val_perplexity'], label='Validation')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Perplexity')\n",
        "    plt.legend()\n",
        "    plt.title('Perplexity vs. Epoch')\n",
        "\n",
        "    plt.subplot(2, 2, 3)\n",
        "    plt.plot(loss_df['epoch'], loss_df['train_accuracy'], label='Train')\n",
        "    plt.plot(loss_df['epoch'], loss_df['val_accuracy'], label='Validation')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Amino Acid Recovery (%)')\n",
        "    plt.legend()\n",
        "    plt.title('AAR vs. Epoch')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(\"/content/drive/MyDrive/classes+projects/plastic_enzyme_project/2024/codes/sigma_improved_aar_training_metrics.png\")\n",
        "    plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "X_kb2PQmkBng"
      },
      "outputs": [],
      "source": [
        "def validate_with_enhanced_metrics(model, val_loader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    total_correct = 0\n",
        "    total_tokens = 0\n",
        "\n",
        "    # Track advanced metrics\n",
        "    total_levenshtein = 0\n",
        "    total_diversity = 0\n",
        "    total_repetition = 0\n",
        "    total_sequences = 0\n",
        "\n",
        "    saved_results = []  # Store ground truth vs predicted sequences and SMILES\n",
        "\n",
        "    with torch.no_grad():\n",
        "        sampled_batches = random.sample(range(len(val_loader)), min(50, len(val_loader)))\n",
        "\n",
        "        for i, batch in enumerate(val_loader):\n",
        "            smiles_strings = batch['smiles']\n",
        "            proteins = batch['proteins']\n",
        "            protein_masks = batch['protein_masks'].to(device)\n",
        "\n",
        "            target_encoding = model.protGPT2_tokenizer(\n",
        "                proteins,\n",
        "                return_tensors=\"pt\",\n",
        "                padding='max_length',\n",
        "                max_length=model.max_len,\n",
        "                truncation=True\n",
        "            ).to(device)\n",
        "\n",
        "            # For validation, we don't use MLM - we want to evaluate on full sequence prediction\n",
        "            outputs = model.forward_without_loss(smiles_strings, target_encoding.input_ids)\n",
        "\n",
        "            # Calculate loss on full sequence prediction\n",
        "            loss_fn = nn.CrossEntropyLoss(ignore_index=model.protGPT2_tokenizer.pad_token_id)\n",
        "            loss = loss_fn(outputs.view(-1, outputs.size(-1)), target_encoding.input_ids.view(-1))\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Amino Acid Recovery Calculation (excluding padding tokens)\n",
        "            predicted_token_ids = torch.argmax(outputs, dim=-1)\n",
        "            predicted_token_ids = torch.nn.functional.pad(\n",
        "                predicted_token_ids, (0, model.max_len - predicted_token_ids.shape[1]),\n",
        "                value=model.protGPT2_tokenizer.pad_token_id\n",
        "            )\n",
        "\n",
        "            mask = target_encoding.input_ids != model.protGPT2_tokenizer.pad_token_id\n",
        "            correct = (predicted_token_ids[mask] == target_encoding.input_ids[mask]).sum().item()\n",
        "            total = mask.sum().item()\n",
        "            total_correct += correct\n",
        "            total_tokens += total\n",
        "\n",
        "            # Calculate diversity for each sequence\n",
        "            for b in range(len(predicted_token_ids)):\n",
        "                pred_seq_tokens = predicted_token_ids[b][mask[b]]\n",
        "                if len(pred_seq_tokens) == 0:\n",
        "                    continue\n",
        "\n",
        "                # Count unique tokens\n",
        "                unique_tokens = torch.unique(pred_seq_tokens).size(0)\n",
        "                seq_diversity = unique_tokens / len(pred_seq_tokens)\n",
        "\n",
        "                # Measure repetition - look for consecutive repeated tokens\n",
        "                consecutive_repeats = 0\n",
        "                for t in range(1, len(pred_seq_tokens)):\n",
        "                    if pred_seq_tokens[t] == pred_seq_tokens[t-1]:\n",
        "                        consecutive_repeats += 1\n",
        "\n",
        "                normalized_repeats = consecutive_repeats / max(1, len(pred_seq_tokens) - 1)\n",
        "\n",
        "                # Decode sequences for Levenshtein distance\n",
        "                pred_seq = model.protGPT2_tokenizer.decode(predicted_token_ids[b], skip_special_tokens=True)\n",
        "                true_seq = model.protGPT2_tokenizer.decode(target_encoding.input_ids[b], skip_special_tokens=True)\n",
        "\n",
        "                # Calculate normalized Levenshtein distance\n",
        "                if len(true_seq) > 0:\n",
        "                    levenshtein_dist = Levenshtein.distance(pred_seq, true_seq) / len(true_seq)\n",
        "                    total_levenshtein += levenshtein_dist\n",
        "\n",
        "                total_diversity += seq_diversity\n",
        "                total_repetition += normalized_repeats\n",
        "                total_sequences += 1\n",
        "\n",
        "            # Save randomly selected ground truth vs predicted sequences and SMILES\n",
        "            if i in sampled_batches:\n",
        "                ground_truth = model.protGPT2_tokenizer.decode(target_encoding.input_ids[0], skip_special_tokens=True)\n",
        "                predicted = model.protGPT2_tokenizer.decode(predicted_token_ids[0], skip_special_tokens=True)\n",
        "\n",
        "                # Calculate per-sequence AAR for this example\n",
        "                seq_mask = target_encoding.input_ids[0] != model.protGPT2_tokenizer.pad_token_id\n",
        "                seq_correct = (predicted_token_ids[0][seq_mask] == target_encoding.input_ids[0][seq_mask]).sum().item()\n",
        "                seq_total = seq_mask.sum().item()\n",
        "                seq_aar = (seq_correct / seq_total * 100) if seq_total > 0 else 0\n",
        "\n",
        "                # Calculate additional metrics for this sample\n",
        "                # 1. Count longest repeated segment\n",
        "                pred_seq = model.protGPT2_tokenizer.decode(predicted_token_ids[0], skip_special_tokens=True)\n",
        "                longest_repeat = find_longest_repeat(pred_seq)\n",
        "\n",
        "                # 2. Calculate amino acid composition similarity\n",
        "                aa_comp_similarity = amino_acid_composition_similarity(ground_truth, predicted)\n",
        "\n",
        "                saved_results.append({\n",
        "                    'SMILES': smiles_strings[0],\n",
        "                    'Ground Truth': ground_truth,\n",
        "                    'Predicted': predicted,\n",
        "                    'Sequence AAR': f\"{seq_aar:.2f}%\",\n",
        "                    'Levenshtein Distance': f\"{Levenshtein.distance(predicted, ground_truth)}\",\n",
        "                    'Normalized Levenshtein': f\"{Levenshtein.distance(predicted, ground_truth) / max(1, len(ground_truth)):.4f}\",\n",
        "                    'Longest Repeat': longest_repeat,\n",
        "                    'AA Composition Similarity': f\"{aa_comp_similarity:.4f}\"\n",
        "                })\n",
        "\n",
        "    avg_loss = total_loss / len(val_loader)\n",
        "    amino_acid_recovery = total_correct / total_tokens * 100\n",
        "\n",
        "    # Calculate average advanced metrics\n",
        "    avg_levenshtein = total_levenshtein / total_sequences if total_sequences > 0 else 0\n",
        "    avg_diversity = total_diversity / total_sequences if total_sequences > 0 else 0\n",
        "    avg_repetition = total_repetition / total_sequences if total_sequences > 0 else 0\n",
        "\n",
        "    # Prevent overflow when calculating perplexity\n",
        "    try:\n",
        "        perplexity = math.exp(avg_loss)\n",
        "    except OverflowError:\n",
        "        perplexity = float('inf')  # Return infinity if the loss is too high\n",
        "\n",
        "    # Add advanced metrics to the results\n",
        "    evaluation_metrics = {\n",
        "        'loss': avg_loss,\n",
        "        'perplexity': perplexity,\n",
        "        'aar': amino_acid_recovery,\n",
        "        'levenshtein': avg_levenshtein,\n",
        "        'diversity': avg_diversity,\n",
        "        'repetition': avg_repetition\n",
        "    }\n",
        "\n",
        "    return avg_loss, perplexity, amino_acid_recovery, saved_results\n",
        "\n",
        "def find_longest_repeat(sequence):\n",
        "    \"\"\"Find the longest repeated substring in the sequence.\"\"\"\n",
        "    if not sequence:\n",
        "        return 0\n",
        "\n",
        "    longest = 0\n",
        "    current = 1\n",
        "\n",
        "    for i in range(1, len(sequence)):\n",
        "        if sequence[i] == sequence[i-1]:\n",
        "            current += 1\n",
        "        else:\n",
        "            longest = max(longest, current)\n",
        "            current = 1\n",
        "\n",
        "    longest = max(longest, current)\n",
        "    return longest\n",
        "\n",
        "def amino_acid_composition_similarity(seq1, seq2):\n",
        "    \"\"\"\n",
        "    Calculate the similarity between the amino acid compositions of two sequences.\n",
        "    Returns a value between 0 and 1, where 1 means identical composition.\n",
        "    \"\"\"\n",
        "    if not seq1 or not seq2:\n",
        "        return 0\n",
        "\n",
        "    # Count amino acids in each sequence\n",
        "    aa_count1 = {}\n",
        "    aa_count2 = {}\n",
        "\n",
        "    for aa in seq1:\n",
        "        aa_count1[aa] = aa_count1.get(aa, 0) + 1\n",
        "\n",
        "    for aa in seq2:\n",
        "        aa_count2[aa] = aa_count2.get(aa, 0) + 1\n",
        "\n",
        "    # Get the union of all amino acids\n",
        "    all_aas = set(aa_count1.keys()) | set(aa_count2.keys())\n",
        "\n",
        "    # Calculate cosine similarity\n",
        "    dot_product = sum(aa_count1.get(aa, 0) * aa_count2.get(aa, 0) for aa in all_aas)\n",
        "\n",
        "    norm1 = math.sqrt(sum(count**2 for count in aa_count1.values()))\n",
        "    norm2 = math.sqrt(sum(count**2 for count in aa_count2.values()))\n",
        "\n",
        "    if norm1 == 0 or norm2 == 0:\n",
        "        return 0\n",
        "\n",
        "    return dot_product / (norm1 * norm2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CfYBZUMhfOTB"
      },
      "source": [
        "### inference + training"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### all frozen _ 34+35 has cross_attn"
      ],
      "metadata": {
        "id": "xWlqqd7_tO8_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e-67RvdafPdi",
        "outputId": "1bd271ad-9e52-4b4d-f644-254af95c314c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Max sequence length: 914\n",
            "Total transformer blocks: 36\n",
            "Adding cross-attention after block 34\n",
            "Adding cross-attention after block 35\n",
            "Added 2 cross-attention blocks\n",
            "transformer.h.0.ln_1.weight\n",
            "Total number of transformer layers: 36\n",
            "\n",
            "===== MODEL STRUCTURE ANALYSIS =====\n",
            "\n",
            "📌 CROSS-ATTENTION LAYERS:\n",
            "  Total cross-attention blocks: 2\n",
            "  Located at positions: [35, 37]\n",
            "\n",
            "📌 LAYER FREEZING STATUS:\n",
            "  Layer  0: ❄️ FROZEN (19,677,440 params)\n",
            "  Layer  1: ❄️ FROZEN (19,677,440 params)\n",
            "  Layer  2: ❄️ FROZEN (19,677,440 params)\n",
            "  Layer  3: ❄️ FROZEN (19,677,440 params)\n",
            "  Layer  4: ❄️ FROZEN (19,677,440 params)\n",
            "  Layer  5: ❄️ FROZEN (19,677,440 params)\n",
            "  Layer  6: ❄️ FROZEN (19,677,440 params)\n",
            "  Layer  7: ❄️ FROZEN (19,677,440 params)\n",
            "  Layer  8: ❄️ FROZEN (19,677,440 params)\n",
            "  Layer  9: ❄️ FROZEN (19,677,440 params)\n",
            "  Layer 10: ❄️ FROZEN (19,677,440 params)\n",
            "  Layer 11: ❄️ FROZEN (19,677,440 params)\n",
            "  Layer 12: ❄️ FROZEN (19,677,440 params)\n",
            "  Layer 13: ❄️ FROZEN (19,677,440 params)\n",
            "  Layer 14: ❄️ FROZEN (19,677,440 params)\n",
            "  Layer 15: ❄️ FROZEN (19,677,440 params)\n",
            "  Layer 16: ❄️ FROZEN (19,677,440 params)\n",
            "  Layer 17: ❄️ FROZEN (19,677,440 params)\n",
            "  Layer 18: ❄️ FROZEN (19,677,440 params)\n",
            "  Layer 19: ❄️ FROZEN (19,677,440 params)\n",
            "  Layer 20: ❄️ FROZEN (19,677,440 params)\n",
            "  Layer 21: ❄️ FROZEN (19,677,440 params)\n",
            "  Layer 22: ❄️ FROZEN (19,677,440 params)\n",
            "  Layer 23: ❄️ FROZEN (19,677,440 params)\n",
            "  Layer 24: ❄️ FROZEN (19,677,440 params)\n",
            "  Layer 25: ❄️ FROZEN (19,677,440 params)\n",
            "  Layer 26: ❄️ FROZEN (19,677,440 params)\n",
            "  Layer 27: ❄️ FROZEN (19,677,440 params)\n",
            "  Layer 28: ❄️ FROZEN (19,677,440 params)\n",
            "  Layer 29: ❄️ FROZEN (19,677,440 params)\n",
            "  Layer 30: ❄️ FROZEN (19,677,440 params)\n",
            "  Layer 31: ❄️ FROZEN (19,677,440 params)\n",
            "  Layer 32: ❄️ FROZEN (19,677,440 params)\n",
            "  Layer 33: ❄️ FROZEN (19,677,440 params)\n",
            "  Layer 34: ❄️ FROZEN (19,677,440 params)\n",
            "  Layer 35: ❄️ FROZEN (19,677,440 params)\n",
            "\n",
            "  LM Head: FROZEN\n",
            "\n",
            "📌 PARAMETER SUMMARY:\n",
            "  Total parameters: 772,716,800\n",
            "  Trainable parameters: 0 (0.00%)\n",
            "  Frozen parameters: 772,716,800 (100.00%)\n",
            "\n",
            "📌 CROSS-ATTENTION DETAILS:\n",
            "  Cross-Attention #1 (index 35): ✅ TRAINABLE (15,736,322 params)\n",
            "  Cross-Attention #2 (index 37): ✅ TRAINABLE (15,736,322 params)\n",
            "\n",
            "===================================\n",
            "\n",
            "Total parameters: 1,429,527,700\n",
            "Trainable parameters: 655,497,620 (45.85%)\n"
          ]
        }
      ],
      "source": [
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Load and preprocess data\n",
        "train_data = preprocess_snp_data('/content/train_data.csv')\n",
        "val_data = preprocess_snp_data('/content/val_data.csv')\n",
        "\n",
        "# train_data = train_data.sample(frac=0.01, random_state=42)\n",
        "# val_data = val_data.sample(frac=0.01, random_state=42)\n",
        "\n",
        "train_data = filter_datasets(train_data)\n",
        "val_data = filter_datasets(val_data)\n",
        "\n",
        "# Calculate max sequence length\n",
        "max_length = max(\n",
        "    train_data['protein_length'].max(),\n",
        "    val_data['protein_length'].max()\n",
        ")\n",
        "max_length = min(max_length, 1024)  # Cap at 1024 or your desired maximum\n",
        "print(f\"Max sequence length: {max_length}\")\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = ProteinGenerationDataset(train_data, max_length)\n",
        "val_dataset = ProteinGenerationDataset(val_data, max_length)\n",
        "\n",
        "# Create dataloaders with custom collate function\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=1,  # Adjust based on your GPU memory\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=1,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "# # Initialize model with sigma-gpt capabilities\n",
        "# model = SigmaProtFlamingo(\n",
        "#     model_path='nferruz/ProtGPT2',\n",
        "#     max_len=max_length,\n",
        "#     cross_attn_every=2,\n",
        "#     dim_head=64,\n",
        "#     heads=8,\n",
        "#     perceiver_depth=2,\n",
        "#     perceiver_num_latents=64\n",
        "# ).to(device)\n",
        "\n",
        "\n",
        "# Initialize model with sigma-gpt capabilities but without any cross-attention initially\n",
        "model = SigmaProtFlamingo(\n",
        "    model_path='nferruz/ProtGPT2',\n",
        "    max_len=max_length,\n",
        "    cross_attn_every=999,\n",
        "    dim_head=64,\n",
        "    heads=8,\n",
        "    perceiver_depth=2,\n",
        "    perceiver_num_latents=64\n",
        ")  # Don't move to device yet\n",
        "\n",
        "# The model structure shows that model.layers contains the transformer blocks\n",
        "# model.protGPT2_model.transformer.h contains the GPT2Blocks\n",
        "\n",
        "# Let's recreate the layers list with cross-attention only after the last two blocks\n",
        "new_layers = []\n",
        "\n",
        "# First, get all the original transformer blocks\n",
        "transformer_blocks = model.protGPT2_model.transformer.h\n",
        "\n",
        "# Total number of transformer blocks\n",
        "num_blocks = len(transformer_blocks)\n",
        "print(f\"Total transformer blocks: {num_blocks}\")\n",
        "\n",
        "# Add each transformer block, with cross-attention after the last two blocks\n",
        "for i, block in enumerate(transformer_blocks):\n",
        "    # Add the transformer block\n",
        "    new_layers.append(block)\n",
        "\n",
        "    # Add cross-attention after the last two blocks\n",
        "    if i == num_blocks - 2 or i == num_blocks - 1:\n",
        "        print(f\"Adding cross-attention after block {i}\")\n",
        "        new_layers.append(GatedCrossAttentionBlock(\n",
        "            dim=model.protGPT2_model.config.n_embd,\n",
        "            dim_head=64,\n",
        "            heads=8\n",
        "        ))\n",
        "\n",
        "# Replace the model's layers with our new sequence\n",
        "model.layers = nn.ModuleList(new_layers)\n",
        "\n",
        "\n",
        "# Now move the entire model to the device after modifying it\n",
        "model = model.to(device)\n",
        "\n",
        "# Count how many cross-attention blocks were added\n",
        "cross_attn_count = sum(1 for layer in model.layers if isinstance(layer, GatedCrossAttentionBlock))\n",
        "print(f\"Added {cross_attn_count} cross-attention blocks\")\n",
        "\n",
        "\n",
        "# Add this line after replacing model.layers with new_layers and moving to device\n",
        "model = model.to(device)\n",
        "\n",
        "# Print more detailed layer structure first\n",
        "for name, _ in model.protGPT2_model.named_parameters():\n",
        "    if 'transformer.h' in name:\n",
        "        print(name)\n",
        "        break  # Just print one example to see the structure\n",
        "\n",
        "\n",
        "# Check the highest layer index in the model\n",
        "max_layer_idx = -1\n",
        "for name, _ in model.protGPT2_model.named_parameters():\n",
        "    if 'transformer.h.' in name:\n",
        "        # Extract the layer index which comes after 'transformer.h.'\n",
        "        parts = name.split('.')\n",
        "        if len(parts) > 2:\n",
        "            try:\n",
        "                layer_idx = int(parts[2])\n",
        "                max_layer_idx = max(max_layer_idx, layer_idx)\n",
        "            except ValueError:\n",
        "                continue\n",
        "\n",
        "print(f\"Total number of transformer layers: {max_layer_idx + 1}\")\n",
        "\n",
        "# Then modify the freezing code to match the actual structure\n",
        "# This assumes the layer indexing is inside the parameter names\n",
        "for name, param in model.protGPT2_model.named_parameters():\n",
        "  param.requires_grad = False  # Freeze everything else\n",
        "\n",
        "\n",
        "print_model_structure(model)\n",
        "\n",
        "\n",
        "###___________________________________________________________________________________\n",
        "\n",
        "# # Directly check if 'lm_head' exists as an attribute\n",
        "# if hasattr(model.protGPT2_model, 'lm_head'):\n",
        "#     print(\"lm_head exists as an attribute!\")\n",
        "#     print(model.protGPT2_model.lm_head)\n",
        "\n",
        "#     # Check if it has parameters\n",
        "#     if hasattr(model.protGPT2_model.lm_head, 'parameters'):\n",
        "#         print(\"lm_head has parameters!\")\n",
        "\n",
        "#         # Check requires_grad for lm_head manually\n",
        "#         for param in model.protGPT2_model.lm_head.parameters():\n",
        "#             print(f\"lm_head requires_grad: {param.requires_grad}\")\n",
        "#     else:\n",
        "#         print(\"WARNING: lm_head has no registered parameters!\")\n",
        "# else:\n",
        "#     print(\"WARNING: lm_head does not exist as an attribute!\")\n",
        "\n",
        "# # Unfreeze lm_head manually\n",
        "# if hasattr(model.protGPT2_model, 'lm_head'):\n",
        "#     for param in model.protGPT2_model.lm_head.parameters():\n",
        "#         param.requires_grad = True\n",
        "#     print(\"lm_head manually unfrozen!\")\n",
        "\n",
        "# # Verify if lm_head is now trainable\n",
        "# for param in model.protGPT2_model.lm_head.parameters():\n",
        "#     print(f\"lm_head requires_grad: {param.requires_grad}\")\n",
        "\n",
        "# Verify which parameters are trainable\n",
        "total_params = 0\n",
        "trainable_params = 0\n",
        "for name, param in model.named_parameters():\n",
        "    total_params += param.numel()\n",
        "    if param.requires_grad:\n",
        "        trainable_params += param.numel()\n",
        "\n",
        "print(f\"Total parameters: {total_params:,}\")\n",
        "print(f\"Trainable parameters: {trainable_params:,} ({trainable_params/total_params:.2%})\")\n",
        "\n",
        "num_epochs = 10\n",
        "\n",
        "# Training loop with curriculum learning\n",
        "# Start with 50% of sequences in left-to-right order and gradually increase to 100% random\n",
        "curriculum_steps = int(0.5 * num_epochs * len(train_loader))  # Curriculum over first half of training\n",
        "# print(\"Starting training with sigma-gpt capabilities...\")\n",
        "# train_with_improved_aar_objective(\n",
        "#     model,\n",
        "#     train_loader,\n",
        "#     val_loader,\n",
        "#     num_epochs,\n",
        "#     device,\n",
        "#     curriculum_steps=curriculum_steps\n",
        "# )\n",
        "\n",
        "###___________________________________________________________________________________\n",
        "\n",
        "# # Generate and evaluate\n",
        "# print(\"Generating proteins for test set...\")\n",
        "# test_results = generate_and_evaluate(model, test_loader, device)\n",
        "\n",
        "# # Save results\n",
        "# print(\"Saving results...\")\n",
        "# results_path = '/content/drive/MyDrive/classes+projects/plastic_enzyme_project/2024/codes/test_results.json'\n",
        "# with open(results_path, 'w') as f:\n",
        "#     json.dump(test_results, f, indent=2)\n",
        "\n",
        "# print(f\"Results saved to {results_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### all but lm head is frozen _ cross attn 34+35+lmhead"
      ],
      "metadata": {
        "id": "EqmrJpgOtnBW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "Hsqe5zYSJ5Bh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0bb6fcb9-402d-40eb-f2fe-0c881b5b781b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Max sequence length: 914\n",
            "Total transformer blocks: 36\n",
            "Adding cross-attention after block 34\n",
            "Adding cross-attention after block 35\n",
            "Added 2 cross-attention blocks\n",
            "transformer.h.0.ln_1.weight\n",
            "Total number of transformer layers: 36\n",
            "lm_head manually unfrozen!\n",
            "lm_head requires_grad: True\n",
            "\n",
            "===== MODEL STRUCTURE ANALYSIS =====\n",
            "\n",
            "📌 CROSS-ATTENTION LAYERS:\n",
            "  Total cross-attention blocks: 2\n",
            "  Located at positions: [35, 37]\n",
            "\n",
            "📌 LAYER FREEZING STATUS:\n",
            "  Layer  0: ❄️ FROZEN (19,677,440 params)\n",
            "  Layer  1: ❄️ FROZEN (19,677,440 params)\n",
            "  Layer  2: ❄️ FROZEN (19,677,440 params)\n",
            "  Layer  3: ❄️ FROZEN (19,677,440 params)\n",
            "  Layer  4: ❄️ FROZEN (19,677,440 params)\n",
            "  Layer  5: ❄️ FROZEN (19,677,440 params)\n",
            "  Layer  6: ❄️ FROZEN (19,677,440 params)\n",
            "  Layer  7: ❄️ FROZEN (19,677,440 params)\n",
            "  Layer  8: ❄️ FROZEN (19,677,440 params)\n",
            "  Layer  9: ❄️ FROZEN (19,677,440 params)\n",
            "  Layer 10: ❄️ FROZEN (19,677,440 params)\n",
            "  Layer 11: ❄️ FROZEN (19,677,440 params)\n",
            "  Layer 12: ❄️ FROZEN (19,677,440 params)\n",
            "  Layer 13: ❄️ FROZEN (19,677,440 params)\n",
            "  Layer 14: ❄️ FROZEN (19,677,440 params)\n",
            "  Layer 15: ❄️ FROZEN (19,677,440 params)\n",
            "  Layer 16: ❄️ FROZEN (19,677,440 params)\n",
            "  Layer 17: ❄️ FROZEN (19,677,440 params)\n",
            "  Layer 18: ❄️ FROZEN (19,677,440 params)\n",
            "  Layer 19: ❄️ FROZEN (19,677,440 params)\n",
            "  Layer 20: ❄️ FROZEN (19,677,440 params)\n",
            "  Layer 21: ❄️ FROZEN (19,677,440 params)\n",
            "  Layer 22: ❄️ FROZEN (19,677,440 params)\n",
            "  Layer 23: ❄️ FROZEN (19,677,440 params)\n",
            "  Layer 24: ❄️ FROZEN (19,677,440 params)\n",
            "  Layer 25: ❄️ FROZEN (19,677,440 params)\n",
            "  Layer 26: ❄️ FROZEN (19,677,440 params)\n",
            "  Layer 27: ❄️ FROZEN (19,677,440 params)\n",
            "  Layer 28: ❄️ FROZEN (19,677,440 params)\n",
            "  Layer 29: ❄️ FROZEN (19,677,440 params)\n",
            "  Layer 30: ❄️ FROZEN (19,677,440 params)\n",
            "  Layer 31: ❄️ FROZEN (19,677,440 params)\n",
            "  Layer 32: ❄️ FROZEN (19,677,440 params)\n",
            "  Layer 33: ❄️ FROZEN (19,677,440 params)\n",
            "  Layer 34: ❄️ FROZEN (19,677,440 params)\n",
            "  Layer 35: ❄️ FROZEN (19,677,440 params)\n",
            "\n",
            "  LM Head: TRAINABLE\n",
            "\n",
            "📌 PARAMETER SUMMARY:\n",
            "  Total parameters: 772,716,800\n",
            "  Trainable parameters: 64,328,960 (8.33%)\n",
            "  Frozen parameters: 708,387,840 (91.67%)\n",
            "\n",
            "📌 CROSS-ATTENTION DETAILS:\n",
            "  Cross-Attention #1 (index 35): ✅ TRAINABLE (15,736,322 params)\n",
            "  Cross-Attention #2 (index 37): ✅ TRAINABLE (15,736,322 params)\n",
            "\n",
            "===================================\n",
            "\n",
            "Total parameters: 1,429,527,700\n",
            "Trainable parameters: 719,826,580 (50.35%)\n"
          ]
        }
      ],
      "source": [
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Load and preprocess data\n",
        "train_data = preprocess_snp_data('/content/train_data.csv')\n",
        "val_data = preprocess_snp_data('/content/val_data.csv')\n",
        "\n",
        "# train_data = train_data.sample(frac=0.01, random_state=42)\n",
        "# val_data = val_data.sample(frac=0.01, random_state=42)\n",
        "\n",
        "train_data = filter_datasets(train_data)\n",
        "val_data = filter_datasets(val_data)\n",
        "\n",
        "# Calculate max sequence length\n",
        "max_length = max(\n",
        "    train_data['protein_length'].max(),\n",
        "    val_data['protein_length'].max()\n",
        ")\n",
        "max_length = min(max_length, 1024)  # Cap at 1024 or your desired maximum\n",
        "print(f\"Max sequence length: {max_length}\")\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = ProteinGenerationDataset(train_data, max_length)\n",
        "val_dataset = ProteinGenerationDataset(val_data, max_length)\n",
        "\n",
        "# Create dataloaders with custom collate function\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=1,  # Adjust based on your GPU memory\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=1,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "# # Initialize model with sigma-gpt capabilities\n",
        "# model = SigmaProtFlamingo(\n",
        "#     model_path='nferruz/ProtGPT2',\n",
        "#     max_len=max_length,\n",
        "#     cross_attn_every=2,\n",
        "#     dim_head=64,\n",
        "#     heads=8,\n",
        "#     perceiver_depth=2,\n",
        "#     perceiver_num_latents=64\n",
        "# ).to(device)\n",
        "\n",
        "\n",
        "# Initialize model with sigma-gpt capabilities but without any cross-attention initially\n",
        "model = SigmaProtFlamingo(\n",
        "    model_path='nferruz/ProtGPT2',\n",
        "    max_len=max_length,\n",
        "    cross_attn_every=999,\n",
        "    dim_head=64,\n",
        "    heads=8,\n",
        "    perceiver_depth=2,\n",
        "    perceiver_num_latents=64\n",
        ")  # Don't move to device yet\n",
        "\n",
        "# The model structure shows that model.layers contains the transformer blocks\n",
        "# model.protGPT2_model.transformer.h contains the GPT2Blocks\n",
        "\n",
        "# Let's recreate the layers list with cross-attention only after the last two blocks\n",
        "new_layers = []\n",
        "\n",
        "# First, get all the original transformer blocks\n",
        "transformer_blocks = model.protGPT2_model.transformer.h\n",
        "\n",
        "# Total number of transformer blocks\n",
        "num_blocks = len(transformer_blocks)\n",
        "print(f\"Total transformer blocks: {num_blocks}\")\n",
        "\n",
        "# Add each transformer block, with cross-attention after the last two blocks\n",
        "for i, block in enumerate(transformer_blocks):\n",
        "    # Add the transformer block\n",
        "    new_layers.append(block)\n",
        "\n",
        "    # Add cross-attention after the last two blocks\n",
        "    if i == num_blocks - 2 or i == num_blocks - 1:\n",
        "        print(f\"Adding cross-attention after block {i}\")\n",
        "        new_layers.append(GatedCrossAttentionBlock(\n",
        "            dim=model.protGPT2_model.config.n_embd,\n",
        "            dim_head=64,\n",
        "            heads=8\n",
        "        ))\n",
        "\n",
        "# Replace the model's layers with our new sequence\n",
        "model.layers = nn.ModuleList(new_layers)\n",
        "\n",
        "# Now move the entire model to the device after modifying it\n",
        "model = model.to(device)\n",
        "\n",
        "# Count how many cross-attention blocks were added\n",
        "cross_attn_count = sum(1 for layer in model.layers if isinstance(layer, GatedCrossAttentionBlock))\n",
        "print(f\"Added {cross_attn_count} cross-attention blocks\")\n",
        "\n",
        "\n",
        "# Print more detailed layer structure first\n",
        "for name, _ in model.protGPT2_model.named_parameters():\n",
        "    if 'transformer.h' in name:\n",
        "        print(name)\n",
        "        break  # Just print one example to see the structure\n",
        "\n",
        "\n",
        "# Check the highest layer index in the model\n",
        "max_layer_idx = -1\n",
        "for name, _ in model.protGPT2_model.named_parameters():\n",
        "    if 'transformer.h.' in name:\n",
        "        # Extract the layer index which comes after 'transformer.h.'\n",
        "        parts = name.split('.')\n",
        "        if len(parts) > 2:\n",
        "            try:\n",
        "                layer_idx = int(parts[2])\n",
        "                max_layer_idx = max(max_layer_idx, layer_idx)\n",
        "            except ValueError:\n",
        "                continue\n",
        "\n",
        "print(f\"Total number of transformer layers: {max_layer_idx + 1}\")\n",
        "\n",
        "# Then modify the freezing code to match the actual structure\n",
        "# This assumes the layer indexing is inside the parameter names\n",
        "for name, param in model.protGPT2_model.named_parameters():\n",
        "    param.requires_grad = False  # Freeze everything else\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "###___________________________________________________________________________________\n",
        "\n",
        "# # Directly check if 'lm_head' exists as an attribute\n",
        "# if hasattr(model.protGPT2_model, 'lm_head'):\n",
        "#     print(\"lm_head exists as an attribute!\")\n",
        "#     print(model.protGPT2_model.lm_head)\n",
        "\n",
        "#     # Check if it has parameters\n",
        "#     if hasattr(model.protGPT2_model.lm_head, 'parameters'):\n",
        "#         print(\"lm_head has parameters!\")\n",
        "\n",
        "#         # Check requires_grad for lm_head manually\n",
        "#         for param in model.protGPT2_model.lm_head.parameters():\n",
        "#             print(f\"lm_head requires_grad: {param.requires_grad}\")\n",
        "#     else:\n",
        "#         print(\"WARNING: lm_head has no registered parameters!\")\n",
        "# else:\n",
        "#     print(\"WARNING: lm_head does not exist as an attribute!\")\n",
        "\n",
        "# Unfreeze lm_head manually\n",
        "if hasattr(model.protGPT2_model, 'lm_head'):\n",
        "    for param in model.protGPT2_model.lm_head.parameters():\n",
        "        param.requires_grad = True\n",
        "    print(\"lm_head manually unfrozen!\")\n",
        "\n",
        "# Verify if lm_head is now trainable\n",
        "for param in model.protGPT2_model.lm_head.parameters():\n",
        "    print(f\"lm_head requires_grad: {param.requires_grad}\")\n",
        "\n",
        "\n",
        "print_model_structure(model)\n",
        "\n",
        "# Verify which parameters are trainable\n",
        "total_params = 0\n",
        "trainable_params = 0\n",
        "for name, param in model.named_parameters():\n",
        "    total_params += param.numel()\n",
        "    if param.requires_grad:\n",
        "        trainable_params += param.numel()\n",
        "\n",
        "print(f\"Total parameters: {total_params:,}\")\n",
        "print(f\"Trainable parameters: {trainable_params:,} ({trainable_params/total_params:.2%})\")\n",
        "\n",
        "num_epochs = 10\n",
        "\n",
        "# Training loop with curriculum learning\n",
        "# Start with 50% of sequences in left-to-right order and gradually increase to 100% random\n",
        "curriculum_steps = int(0.5 * num_epochs * len(train_loader))  # Curriculum over first half of training\n",
        "# print(\"Starting training with sigma-gpt capabilities...\")\n",
        "# train_with_improved_aar_objective(\n",
        "#     model,\n",
        "#     train_loader,\n",
        "#     val_loader,\n",
        "#     num_epochs,\n",
        "#     device,\n",
        "#     curriculum_steps=curriculum_steps\n",
        "# )\n",
        "\n",
        "###___________________________________________________________________________________\n",
        "\n",
        "# # Generate and evaluate\n",
        "# print(\"Generating proteins for test set...\")\n",
        "# test_results = generate_and_evaluate(model, test_loader, device)\n",
        "\n",
        "# # Save results\n",
        "# print(\"Saving results...\")\n",
        "# results_path = '/content/drive/MyDrive/classes+projects/plastic_enzyme_project/2024/codes/test_results.json'\n",
        "# with open(results_path, 'w') as f:\n",
        "#     json.dump(test_results, f, indent=2)\n",
        "\n",
        "# print(f\"Results saved to {results_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### unfreeze 34+35 and do cross attn with 34+35"
      ],
      "metadata": {
        "id": "Wmqg3zrXtsP-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Load and preprocess data\n",
        "train_data = preprocess_snp_data('/content/train_data.csv')\n",
        "val_data = preprocess_snp_data('/content/val_data.csv')\n",
        "\n",
        "# train_data = train_data.sample(frac=0.01, random_state=42)\n",
        "# val_data = val_data.sample(frac=0.01, random_state=42)\n",
        "\n",
        "train_data = filter_datasets(train_data)\n",
        "val_data = filter_datasets(val_data)\n",
        "\n",
        "# Calculate max sequence length\n",
        "max_length = max(\n",
        "    train_data['protein_length'].max(),\n",
        "    val_data['protein_length'].max()\n",
        ")\n",
        "max_length = min(max_length, 1024)  # Cap at 1024 or your desired maximum\n",
        "print(f\"Max sequence length: {max_length}\")\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = ProteinGenerationDataset(train_data, max_length)\n",
        "val_dataset = ProteinGenerationDataset(val_data, max_length)\n",
        "\n",
        "# Create dataloaders with custom collate function\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=1,  # Adjust based on your GPU memory\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=1,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "# # Initialize model with sigma-gpt capabilities\n",
        "# model = SigmaProtFlamingo(\n",
        "#     model_path='nferruz/ProtGPT2',\n",
        "#     max_len=max_length,\n",
        "#     cross_attn_every=2,\n",
        "#     dim_head=64,\n",
        "#     heads=8,\n",
        "#     perceiver_depth=2,\n",
        "#     perceiver_num_latents=64\n",
        "# ).to(device)\n",
        "\n",
        "\n",
        "# Initialize model with sigma-gpt capabilities but without any cross-attention initially\n",
        "model = SigmaProtFlamingo(\n",
        "    model_path='nferruz/ProtGPT2',\n",
        "    max_len=max_length,\n",
        "    cross_attn_every=999,\n",
        "    dim_head=64,\n",
        "    heads=8,\n",
        "    perceiver_depth=2,\n",
        "    perceiver_num_latents=64\n",
        ")  # Don't move to device yet\n",
        "\n",
        "# The model structure shows that model.layers contains the transformer blocks\n",
        "# model.protGPT2_model.transformer.h contains the GPT2Blocks\n",
        "\n",
        "# Let's recreate the layers list with cross-attention only after the last two blocks\n",
        "new_layers = []\n",
        "\n",
        "# First, get all the original transformer blocks\n",
        "transformer_blocks = model.protGPT2_model.transformer.h\n",
        "\n",
        "# Total number of transformer blocks\n",
        "num_blocks = len(transformer_blocks)\n",
        "print(f\"Total transformer blocks: {num_blocks}\")\n",
        "\n",
        "# Add each transformer block, with cross-attention after the last two blocks\n",
        "for i, block in enumerate(transformer_blocks):\n",
        "    # Add the transformer block\n",
        "    new_layers.append(block)\n",
        "\n",
        "    # Add cross-attention after the last two blocks\n",
        "    if i == num_blocks - 2 or i == num_blocks - 1:\n",
        "        print(f\"Adding cross-attention after block {i}\")\n",
        "        new_layers.append(GatedCrossAttentionBlock(\n",
        "            dim=model.protGPT2_model.config.n_embd,\n",
        "            dim_head=64,\n",
        "            heads=8\n",
        "        ))\n",
        "\n",
        "# Replace the model's layers with our new sequence\n",
        "model.layers = nn.ModuleList(new_layers)\n",
        "\n",
        "# Now move the entire model to the device after modifying it\n",
        "model = model.to(device)\n",
        "\n",
        "# Count how many cross-attention blocks were added\n",
        "cross_attn_count = sum(1 for layer in model.layers if isinstance(layer, GatedCrossAttentionBlock))\n",
        "print(f\"Added {cross_attn_count} cross-attention blocks\")\n",
        "\n",
        "# Print more detailed layer structure first\n",
        "for name, _ in model.protGPT2_model.named_parameters():\n",
        "    if 'transformer.h' in name:\n",
        "        print(name)\n",
        "        break  # Just print one example to see the structure\n",
        "\n",
        "\n",
        "# Check the highest layer index in the model\n",
        "max_layer_idx = -1\n",
        "for name, _ in model.protGPT2_model.named_parameters():\n",
        "    if 'transformer.h.' in name:\n",
        "        # Extract the layer index which comes after 'transformer.h.'\n",
        "        parts = name.split('.')\n",
        "        if len(parts) > 2:\n",
        "            try:\n",
        "                layer_idx = int(parts[2])\n",
        "                max_layer_idx = max(max_layer_idx, layer_idx)\n",
        "            except ValueError:\n",
        "                continue\n",
        "\n",
        "print(f\"Total number of transformer layers: {max_layer_idx + 1}\")\n",
        "\n",
        "# Then modify the freezing code to match the actual structure\n",
        "# This assumes the layer indexing is inside the parameter names\n",
        "for name, param in model.protGPT2_model.named_parameters():\n",
        "    if 'lm_head' in name or 'transformer.h.34' in name or 'transformer.h.35' in name:\n",
        "        param.requires_grad = True  # Unfreeze\n",
        "    else:\n",
        "        param.requires_grad = False  # Freeze everything else\n",
        "\n",
        "\n",
        "\n",
        "print_model_structure(model)\n",
        "\n",
        "\n",
        "###___________________________________________________________________________________\n",
        "\n",
        "# # Directly check if 'lm_head' exists as an attribute\n",
        "# if hasattr(model.protGPT2_model, 'lm_head'):\n",
        "#     print(\"lm_head exists as an attribute!\")\n",
        "#     print(model.protGPT2_model.lm_head)\n",
        "\n",
        "#     # Check if it has parameters\n",
        "#     if hasattr(model.protGPT2_model.lm_head, 'parameters'):\n",
        "#         print(\"lm_head has parameters!\")\n",
        "\n",
        "#         # Check requires_grad for lm_head manually\n",
        "#         for param in model.protGPT2_model.lm_head.parameters():\n",
        "#             print(f\"lm_head requires_grad: {param.requires_grad}\")\n",
        "#     else:\n",
        "#         print(\"WARNING: lm_head has no registered parameters!\")\n",
        "# else:\n",
        "#     print(\"WARNING: lm_head does not exist as an attribute!\")\n",
        "\n",
        "# # Unfreeze lm_head manually\n",
        "# if hasattr(model.protGPT2_model, 'lm_head'):\n",
        "#     for param in model.protGPT2_model.lm_head.parameters():\n",
        "#         param.requires_grad = True\n",
        "#     print(\"lm_head manually unfrozen!\")\n",
        "\n",
        "# # Verify if lm_head is now trainable\n",
        "# for param in model.protGPT2_model.lm_head.parameters():\n",
        "#     print(f\"lm_head requires_grad: {param.requires_grad}\")\n",
        "\n",
        "# Verify which parameters are trainable\n",
        "total_params = 0\n",
        "trainable_params = 0\n",
        "for name, param in model.named_parameters():\n",
        "    total_params += param.numel()\n",
        "    if param.requires_grad:\n",
        "        trainable_params += param.numel()\n",
        "\n",
        "print(f\"Total parameters: {total_params:,}\")\n",
        "print(f\"Trainable parameters: {trainable_params:,} ({trainable_params/total_params:.2%})\")\n",
        "\n",
        "num_epochs = 10\n",
        "\n",
        "# Training loop with curriculum learning\n",
        "# Start with 50% of sequences in left-to-right order and gradually increase to 100% random\n",
        "curriculum_steps = int(0.5 * num_epochs * len(train_loader))  # Curriculum over first half of training\n",
        "# print(\"Starting training with sigma-gpt capabilities...\")\n",
        "# train_with_improved_aar_objective(\n",
        "#     model,\n",
        "#     train_loader,\n",
        "#     val_loader,\n",
        "#     num_epochs,\n",
        "#     device,\n",
        "#     curriculum_steps=curriculum_steps\n",
        "# )\n",
        "\n",
        "###___________________________________________________________________________________\n",
        "\n",
        "# # Generate and evaluate\n",
        "# print(\"Generating proteins for test set...\")\n",
        "# test_results = generate_and_evaluate(model, test_loader, device)\n",
        "\n",
        "# # Save results\n",
        "# print(\"Saving results...\")\n",
        "# results_path = '/content/drive/MyDrive/classes+projects/plastic_enzyme_project/2024/codes/test_results.json'\n",
        "# with open(results_path, 'w') as f:\n",
        "#     json.dump(test_results, f, indent=2)\n",
        "\n",
        "# print(f\"Results saved to {results_path}\")\n"
      ],
      "metadata": {
        "id": "dupiZgjctzja",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4ea50cf-0d2c-4239-dab2-9945e72cc3c4"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Max sequence length: 914\n",
            "Total transformer blocks: 36\n",
            "Adding cross-attention after block 34\n",
            "Adding cross-attention after block 35\n",
            "Added 2 cross-attention blocks\n",
            "transformer.h.0.ln_1.weight\n",
            "Total number of transformer layers: 36\n",
            "\n",
            "===== MODEL STRUCTURE ANALYSIS =====\n",
            "\n",
            "📌 CROSS-ATTENTION LAYERS:\n",
            "  Total cross-attention blocks: 2\n",
            "  Located at positions: [35, 37]\n",
            "\n",
            "📌 LAYER FREEZING STATUS:\n",
            "  Layer  0: ❄️ FROZEN (19,677,440 params)\n",
            "  Layer  1: ❄️ FROZEN (19,677,440 params)\n",
            "  Layer  2: ❄️ FROZEN (19,677,440 params)\n",
            "  Layer  3: ❄️ FROZEN (19,677,440 params)\n",
            "  Layer  4: ❄️ FROZEN (19,677,440 params)\n",
            "  Layer  5: ❄️ FROZEN (19,677,440 params)\n",
            "  Layer  6: ❄️ FROZEN (19,677,440 params)\n",
            "  Layer  7: ❄️ FROZEN (19,677,440 params)\n",
            "  Layer  8: ❄️ FROZEN (19,677,440 params)\n",
            "  Layer  9: ❄️ FROZEN (19,677,440 params)\n",
            "  Layer 10: ❄️ FROZEN (19,677,440 params)\n",
            "  Layer 11: ❄️ FROZEN (19,677,440 params)\n",
            "  Layer 12: ❄️ FROZEN (19,677,440 params)\n",
            "  Layer 13: ❄️ FROZEN (19,677,440 params)\n",
            "  Layer 14: ❄️ FROZEN (19,677,440 params)\n",
            "  Layer 15: ❄️ FROZEN (19,677,440 params)\n",
            "  Layer 16: ❄️ FROZEN (19,677,440 params)\n",
            "  Layer 17: ❄️ FROZEN (19,677,440 params)\n",
            "  Layer 18: ❄️ FROZEN (19,677,440 params)\n",
            "  Layer 19: ❄️ FROZEN (19,677,440 params)\n",
            "  Layer 20: ❄️ FROZEN (19,677,440 params)\n",
            "  Layer 21: ❄️ FROZEN (19,677,440 params)\n",
            "  Layer 22: ❄️ FROZEN (19,677,440 params)\n",
            "  Layer 23: ❄️ FROZEN (19,677,440 params)\n",
            "  Layer 24: ❄️ FROZEN (19,677,440 params)\n",
            "  Layer 25: ❄️ FROZEN (19,677,440 params)\n",
            "  Layer 26: ❄️ FROZEN (19,677,440 params)\n",
            "  Layer 27: ❄️ FROZEN (19,677,440 params)\n",
            "  Layer 28: ❄️ FROZEN (19,677,440 params)\n",
            "  Layer 29: ❄️ FROZEN (19,677,440 params)\n",
            "  Layer 30: ❄️ FROZEN (19,677,440 params)\n",
            "  Layer 31: ❄️ FROZEN (19,677,440 params)\n",
            "  Layer 32: ❄️ FROZEN (19,677,440 params)\n",
            "  Layer 33: ❄️ FROZEN (19,677,440 params)\n",
            "  Layer 34: ✅ TRAINABLE (19,677,440 params)\n",
            "  Layer 35: ✅ TRAINABLE (19,677,440 params)\n",
            "\n",
            "  LM Head: FROZEN\n",
            "\n",
            "📌 PARAMETER SUMMARY:\n",
            "  Total parameters: 772,716,800\n",
            "  Trainable parameters: 39,354,880 (5.09%)\n",
            "  Frozen parameters: 733,361,920 (94.91%)\n",
            "\n",
            "📌 CROSS-ATTENTION DETAILS:\n",
            "  Cross-Attention #1 (index 35): ✅ TRAINABLE (15,736,322 params)\n",
            "  Cross-Attention #2 (index 37): ✅ TRAINABLE (15,736,322 params)\n",
            "\n",
            "===================================\n",
            "\n",
            "Total parameters: 1,429,527,700\n",
            "Trainable parameters: 694,852,500 (48.61%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### unfreeze 34+35+lm head and add cross attn to those"
      ],
      "metadata": {
        "id": "KPoO9a6Xt3FL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Load and preprocess data\n",
        "train_data = preprocess_snp_data('/content/train_data.csv')\n",
        "val_data = preprocess_snp_data('/content/val_data.csv')\n",
        "\n",
        "# train_data = train_data.sample(frac=0.01, random_state=42)\n",
        "# val_data = val_data.sample(frac=0.01, random_state=42)\n",
        "\n",
        "train_data = filter_datasets(train_data)\n",
        "val_data = filter_datasets(val_data)\n",
        "\n",
        "# Calculate max sequence length\n",
        "max_length = max(\n",
        "    train_data['protein_length'].max(),\n",
        "    val_data['protein_length'].max()\n",
        ")\n",
        "max_length = min(max_length, 1024)  # Cap at 1024 or your desired maximum\n",
        "print(f\"Max sequence length: {max_length}\")\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = ProteinGenerationDataset(train_data, max_length)\n",
        "val_dataset = ProteinGenerationDataset(val_data, max_length)\n",
        "\n",
        "# Create dataloaders with custom collate function\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=1,  # Adjust based on your GPU memory\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=1,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "# # Initialize model with sigma-gpt capabilities\n",
        "# model = SigmaProtFlamingo(\n",
        "#     model_path='nferruz/ProtGPT2',\n",
        "#     max_len=max_length,\n",
        "#     cross_attn_every=2,\n",
        "#     dim_head=64,\n",
        "#     heads=8,\n",
        "#     perceiver_depth=2,\n",
        "#     perceiver_num_latents=64\n",
        "# ).to(device)\n",
        "\n",
        "\n",
        "# Initialize model with sigma-gpt capabilities but without any cross-attention initially\n",
        "model = SigmaProtFlamingo(\n",
        "    model_path='nferruz/ProtGPT2',\n",
        "    max_len=max_length,\n",
        "    cross_attn_every=999,\n",
        "    dim_head=64,\n",
        "    heads=8,\n",
        "    perceiver_depth=2,\n",
        "    perceiver_num_latents=64\n",
        ")  # Don't move to device yet\n",
        "\n",
        "# The model structure shows that model.layers contains the transformer blocks\n",
        "# model.protGPT2_model.transformer.h contains the GPT2Blocks\n",
        "\n",
        "# Let's recreate the layers list with cross-attention only after the last two blocks\n",
        "new_layers = []\n",
        "\n",
        "# First, get all the original transformer blocks\n",
        "transformer_blocks = model.protGPT2_model.transformer.h\n",
        "\n",
        "# Total number of transformer blocks\n",
        "num_blocks = len(transformer_blocks)\n",
        "print(f\"Total transformer blocks: {num_blocks}\")\n",
        "\n",
        "# Add each transformer block, with cross-attention after the last two blocks\n",
        "for i, block in enumerate(transformer_blocks):\n",
        "    # Add the transformer block\n",
        "    new_layers.append(block)\n",
        "\n",
        "    # Add cross-attention after the last two blocks\n",
        "    if i == num_blocks - 2 or i == num_blocks - 1:\n",
        "        print(f\"Adding cross-attention after block {i}\")\n",
        "        new_layers.append(GatedCrossAttentionBlock(\n",
        "            dim=model.protGPT2_model.config.n_embd,\n",
        "            dim_head=64,\n",
        "            heads=8\n",
        "        ))\n",
        "\n",
        "# Replace the model's layers with our new sequence\n",
        "model.layers = nn.ModuleList(new_layers)\n",
        "\n",
        "# Now move the entire model to the device after modifying it\n",
        "model = model.to(device)\n",
        "\n",
        "# Count how many cross-attention blocks were added\n",
        "cross_attn_count = sum(1 for layer in model.layers if isinstance(layer, GatedCrossAttentionBlock))\n",
        "print(f\"Added {cross_attn_count} cross-attention blocks\")\n",
        "\n",
        "# Print more detailed layer structure first\n",
        "for name, _ in model.protGPT2_model.named_parameters():\n",
        "    if 'transformer.h' in name:\n",
        "        print(name)\n",
        "        break  # Just print one example to see the structure\n",
        "\n",
        "\n",
        "# Check the highest layer index in the model\n",
        "max_layer_idx = -1\n",
        "for name, _ in model.protGPT2_model.named_parameters():\n",
        "    if 'transformer.h.' in name:\n",
        "        # Extract the layer index which comes after 'transformer.h.'\n",
        "        parts = name.split('.')\n",
        "        if len(parts) > 2:\n",
        "            try:\n",
        "                layer_idx = int(parts[2])\n",
        "                max_layer_idx = max(max_layer_idx, layer_idx)\n",
        "            except ValueError:\n",
        "                continue\n",
        "\n",
        "print(f\"Total number of transformer layers: {max_layer_idx + 1}\")\n",
        "\n",
        "# Then modify the freezing code to match the actual structure\n",
        "# This assumes the layer indexing is inside the parameter names\n",
        "for name, param in model.protGPT2_model.named_parameters():\n",
        "    if 'lm_head' in name or 'transformer.h.34' in name or 'transformer.h.35' in name:\n",
        "        param.requires_grad = True  # Unfreeze\n",
        "    else:\n",
        "        param.requires_grad = False  # Freeze everything else\n",
        "\n",
        "##___________________________________________________________________________________\n",
        "\n",
        "# Directly check if 'lm_head' exists as an attribute\n",
        "if hasattr(model.protGPT2_model, 'lm_head'):\n",
        "    print(\"lm_head exists as an attribute!\")\n",
        "    print(model.protGPT2_model.lm_head)\n",
        "\n",
        "    # Check if it has parameters\n",
        "    if hasattr(model.protGPT2_model.lm_head, 'parameters'):\n",
        "        print(\"lm_head has parameters!\")\n",
        "\n",
        "        # Check requires_grad for lm_head manually\n",
        "        for param in model.protGPT2_model.lm_head.parameters():\n",
        "            print(f\"lm_head requires_grad: {param.requires_grad}\")\n",
        "    else:\n",
        "        print(\"WARNING: lm_head has no registered parameters!\")\n",
        "else:\n",
        "    print(\"WARNING: lm_head does not exist as an attribute!\")\n",
        "\n",
        "# Unfreeze lm_head manually\n",
        "if hasattr(model.protGPT2_model, 'lm_head'):\n",
        "    for param in model.protGPT2_model.lm_head.parameters():\n",
        "        param.requires_grad = True\n",
        "    print(\"lm_head manually unfrozen!\")\n",
        "\n",
        "# Verify if lm_head is now trainable\n",
        "for param in model.protGPT2_model.lm_head.parameters():\n",
        "    print(f\"lm_head requires_grad: {param.requires_grad}\")\n",
        "\n",
        "# Verify which parameters are trainable\n",
        "total_params = 0\n",
        "trainable_params = 0\n",
        "for name, param in model.named_parameters():\n",
        "    total_params += param.numel()\n",
        "    if param.requires_grad:\n",
        "        trainable_params += param.numel()\n",
        "\n",
        "print(f\"Total parameters: {total_params:,}\")\n",
        "print(f\"Trainable parameters: {trainable_params:,} ({trainable_params/total_params:.2%})\")\n",
        "\n",
        "num_epochs = 10\n",
        "\n",
        "\n",
        "print_model_structure(model)\n",
        "\n",
        "\n",
        "# Training loop with curriculum learning\n",
        "# Start with 50% of sequences in left-to-right order and gradually increase to 100% random\n",
        "curriculum_steps = int(0.5 * num_epochs * len(train_loader))  # Curriculum over first half of training\n",
        "print(\"Starting training with sigma-gpt capabilities...\")\n",
        "train_with_improved_aar_objective(\n",
        "    model,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    num_epochs,\n",
        "    device,\n",
        "    curriculum_steps=curriculum_steps\n",
        ")\n",
        "\n",
        "###___________________________________________________________________________________\n",
        "\n",
        "# # Generate and evaluate\n",
        "# print(\"Generating proteins for test set...\")\n",
        "# test_results = generate_and_evaluate(model, test_loader, device)\n",
        "\n",
        "# # Save results\n",
        "# print(\"Saving results...\")\n",
        "# results_path = '/content/drive/MyDrive/classes+projects/plastic_enzyme_project/2024/codes/test_results.json'\n",
        "# with open(results_path, 'w') as f:\n",
        "#     json.dump(test_results, f, indent=2)\n",
        "\n",
        "# print(f\"Results saved to {results_path}\")\n"
      ],
      "metadata": {
        "id": "E7jy-MKlt6L5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a5aa47b-debb-4439-b489-c7cf41497425"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Max sequence length: 914\n",
            "Total transformer blocks: 36\n",
            "Adding cross-attention after block 34\n",
            "Adding cross-attention after block 35\n",
            "Added 2 cross-attention blocks\n",
            "transformer.h.0.ln_1.weight\n",
            "Total number of transformer layers: 36\n",
            "lm_head exists as an attribute!\n",
            "Linear(in_features=1280, out_features=50257, bias=False)\n",
            "lm_head has parameters!\n",
            "lm_head requires_grad: False\n",
            "lm_head manually unfrozen!\n",
            "lm_head requires_grad: True\n",
            "Total parameters: 1,429,527,700\n",
            "Trainable parameters: 759,181,460 (53.11%)\n",
            "\n",
            "===== MODEL STRUCTURE ANALYSIS =====\n",
            "\n",
            "📌 CROSS-ATTENTION LAYERS:\n",
            "  Total cross-attention blocks: 2\n",
            "  Located at positions: [35, 37]\n",
            "\n",
            "📌 LAYER FREEZING STATUS:\n",
            "  Layer  0: ❄️ FROZEN (19,677,440 params)\n",
            "  Layer  1: ❄️ FROZEN (19,677,440 params)\n",
            "  Layer  2: ❄️ FROZEN (19,677,440 params)\n",
            "  Layer  3: ❄️ FROZEN (19,677,440 params)\n",
            "  Layer  4: ❄️ FROZEN (19,677,440 params)\n",
            "  Layer  5: ❄️ FROZEN (19,677,440 params)\n",
            "  Layer  6: ❄️ FROZEN (19,677,440 params)\n",
            "  Layer  7: ❄️ FROZEN (19,677,440 params)\n",
            "  Layer  8: ❄️ FROZEN (19,677,440 params)\n",
            "  Layer  9: ❄️ FROZEN (19,677,440 params)\n",
            "  Layer 10: ❄️ FROZEN (19,677,440 params)\n",
            "  Layer 11: ❄️ FROZEN (19,677,440 params)\n",
            "  Layer 12: ❄️ FROZEN (19,677,440 params)\n",
            "  Layer 13: ❄️ FROZEN (19,677,440 params)\n",
            "  Layer 14: ❄️ FROZEN (19,677,440 params)\n",
            "  Layer 15: ❄️ FROZEN (19,677,440 params)\n",
            "  Layer 16: ❄️ FROZEN (19,677,440 params)\n",
            "  Layer 17: ❄️ FROZEN (19,677,440 params)\n",
            "  Layer 18: ❄️ FROZEN (19,677,440 params)\n",
            "  Layer 19: ❄️ FROZEN (19,677,440 params)\n",
            "  Layer 20: ❄️ FROZEN (19,677,440 params)\n",
            "  Layer 21: ❄️ FROZEN (19,677,440 params)\n",
            "  Layer 22: ❄️ FROZEN (19,677,440 params)\n",
            "  Layer 23: ❄️ FROZEN (19,677,440 params)\n",
            "  Layer 24: ❄️ FROZEN (19,677,440 params)\n",
            "  Layer 25: ❄️ FROZEN (19,677,440 params)\n",
            "  Layer 26: ❄️ FROZEN (19,677,440 params)\n",
            "  Layer 27: ❄️ FROZEN (19,677,440 params)\n",
            "  Layer 28: ❄️ FROZEN (19,677,440 params)\n",
            "  Layer 29: ❄️ FROZEN (19,677,440 params)\n",
            "  Layer 30: ❄️ FROZEN (19,677,440 params)\n",
            "  Layer 31: ❄️ FROZEN (19,677,440 params)\n",
            "  Layer 32: ❄️ FROZEN (19,677,440 params)\n",
            "  Layer 33: ❄️ FROZEN (19,677,440 params)\n",
            "  Layer 34: ✅ TRAINABLE (19,677,440 params)\n",
            "  Layer 35: ✅ TRAINABLE (19,677,440 params)\n",
            "\n",
            "  LM Head: TRAINABLE\n",
            "\n",
            "📌 PARAMETER SUMMARY:\n",
            "  Total parameters: 772,716,800\n",
            "  Trainable parameters: 103,683,840 (13.42%)\n",
            "  Frozen parameters: 669,032,960 (86.58%)\n",
            "\n",
            "📌 CROSS-ATTENTION DETAILS:\n",
            "  Cross-Attention #1 (index 35): ✅ TRAINABLE (15,736,322 params)\n",
            "  Cross-Attention #2 (index 37): ✅ TRAINABLE (15,736,322 params)\n",
            "\n",
            "===================================\n",
            "\n",
            "Starting training with sigma-gpt capabilities...\n",
            "Curriculum ratio: 0.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 1/10:   0%|          | 0/3980 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Loss components: CE=3724.5242, Rep=0.2105, Seq=7.0833\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 1/10:   0%|          | 1/3980 [00:02<2:38:17,  2.39s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch 0: Loss=3724.5242, AAR=0.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/10:   0%|          | 11/3980 [00:07<33:49,  1.96it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch 10: Loss=4470.9326, AAR=0.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/10:   1%|          | 21/3980 [00:12<32:03,  2.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch 20: Loss=2587.9871, AAR=0.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/10:   1%|          | 31/3980 [00:17<32:59,  2.00it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch 30: Loss=3171.5122, AAR=0.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/10:   1%|          | 41/3980 [00:22<32:14,  2.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch 40: Loss=2025.2548, AAR=0.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/10:   1%|▏         | 51/3980 [00:26<31:59,  2.05it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Loss components: CE=1903.2087, Rep=0.0430, Seq=8.9424\n",
            "  Batch 50: Loss=1903.2090, AAR=0.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/10:   2%|▏         | 61/3980 [00:31<31:47,  2.05it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch 60: Loss=1626.1965, AAR=0.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/10:   2%|▏         | 71/3980 [00:36<31:38,  2.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch 70: Loss=1828.9250, AAR=0.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/10:   2%|▏         | 81/3980 [00:41<31:38,  2.05it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch 80: Loss=2046.9462, AAR=0.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/10:   2%|▏         | 91/3980 [00:46<31:52,  2.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch 90: Loss=1727.7098, AAR=0.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/10:   3%|▎         | 101/3980 [00:51<31:28,  2.05it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Loss components: CE=697.2571, Rep=0.0102, Seq=8.3209\n",
            "  Batch 100: Loss=697.2571, AAR=0.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/10:   3%|▎         | 111/3980 [00:56<31:50,  2.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch 110: Loss=1760.5056, AAR=0.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/10:   3%|▎         | 121/3980 [01:01<31:52,  2.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch 120: Loss=1324.5081, AAR=0.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/10:   3%|▎         | 131/3980 [01:06<31:07,  2.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch 130: Loss=1711.7574, AAR=0.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/10:   4%|▎         | 141/3980 [01:10<30:47,  2.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch 140: Loss=1618.8890, AAR=0.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/10:   4%|▍         | 151/3980 [01:15<31:35,  2.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Loss components: CE=894.1496, Rep=0.0206, Seq=8.4175\n",
            "  Batch 150: Loss=894.1496, AAR=0.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/10:   4%|▍         | 161/3980 [01:20<30:47,  2.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch 160: Loss=1078.1924, AAR=0.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/10:   4%|▍         | 171/3980 [01:25<31:39,  2.01it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch 170: Loss=1226.4070, AAR=0.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/10:   5%|▍         | 181/3980 [01:30<30:58,  2.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch 180: Loss=821.2664, AAR=0.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/10:   5%|▍         | 191/3980 [01:35<30:34,  2.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch 190: Loss=794.2172, AAR=0.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/10:   5%|▌         | 201/3980 [01:40<30:37,  2.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Loss components: CE=1057.6624, Rep=0.0333, Seq=10.5076\n",
            "  Batch 200: Loss=1057.6625, AAR=0.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/10:   5%|▌         | 211/3980 [01:45<30:14,  2.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch 210: Loss=912.4845, AAR=0.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/10:   6%|▌         | 221/3980 [01:50<30:44,  2.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch 220: Loss=1064.3438, AAR=0.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/10:   6%|▌         | 231/3980 [01:55<31:03,  2.01it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch 230: Loss=896.9280, AAR=0.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/10:   6%|▌         | 241/3980 [02:00<30:28,  2.05it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch 240: Loss=590.0178, AAR=0.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/10:   6%|▋         | 251/3980 [02:04<30:24,  2.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Loss components: CE=1153.0062, Rep=0.0127, Seq=10.0645\n",
            "  Batch 250: Loss=1153.0063, AAR=0.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/10:   7%|▋         | 261/3980 [02:09<30:18,  2.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch 260: Loss=1551.5060, AAR=0.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/10:   7%|▋         | 271/3980 [02:14<30:05,  2.05it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch 270: Loss=682.8907, AAR=0.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/10:   7%|▋         | 281/3980 [02:19<30:10,  2.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch 280: Loss=1056.3319, AAR=0.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/10:   7%|▋         | 291/3980 [02:24<30:07,  2.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch 290: Loss=1675.0182, AAR=0.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/10:   8%|▊         | 301/3980 [02:29<29:52,  2.05it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Loss components: CE=957.1619, Rep=0.0061, Seq=5.0447\n",
            "  Batch 300: Loss=957.1618, AAR=0.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/10:   8%|▊         | 311/3980 [02:34<29:38,  2.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch 310: Loss=597.6630, AAR=0.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/10:   8%|▊         | 321/3980 [02:39<29:39,  2.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch 320: Loss=968.7035, AAR=0.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/10:   8%|▊         | 331/3980 [02:44<30:03,  2.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch 330: Loss=505.2824, AAR=0.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/10:   9%|▊         | 341/3980 [02:48<29:27,  2.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch 340: Loss=920.2732, AAR=0.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/10:   9%|▉         | 351/3980 [02:53<29:21,  2.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Loss components: CE=833.8425, Rep=0.0000, Seq=9.2194\n",
            "  Batch 350: Loss=833.8425, AAR=0.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/10:   9%|▉         | 361/3980 [02:58<29:34,  2.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch 360: Loss=845.9545, AAR=0.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/10:   9%|▉         | 371/3980 [03:03<29:18,  2.05it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch 370: Loss=973.6693, AAR=0.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/10:  10%|▉         | 381/3980 [03:08<29:40,  2.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch 380: Loss=899.8007, AAR=0.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/10:  10%|▉         | 391/3980 [03:13<28:59,  2.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch 390: Loss=801.7186, AAR=0.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/10:  10%|█         | 401/3980 [03:18<29:14,  2.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Loss components: CE=1251.2941, Rep=0.0102, Seq=8.1574\n",
            "  Batch 400: Loss=1251.2941, AAR=0.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/10:  10%|█         | 411/3980 [03:23<28:44,  2.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch 410: Loss=586.0203, AAR=0.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/10:  11%|█         | 421/3980 [03:27<28:39,  2.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch 420: Loss=658.6631, AAR=0.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/10:  11%|█         | 431/3980 [03:32<28:29,  2.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch 430: Loss=591.3684, AAR=0.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/10:  11%|█         | 441/3980 [03:37<29:09,  2.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch 440: Loss=679.3245, AAR=0.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/10:  11%|█▏        | 451/3980 [03:42<28:54,  2.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Loss components: CE=921.2969, Rep=0.0112, Seq=9.3034\n",
            "  Batch 450: Loss=921.2969, AAR=0.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLZXFhNq9xvv"
      },
      "source": [
        "### generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vXlyfekIsTlB"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Load the trained model\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Path to model checkpoint\n",
        "checkpoint_path = \"/content/drive/MyDrive/classes+projects/plastic_enzyme_project/2024/codes/sigma_checkpoint.pth\"\n",
        "\n",
        "# Load\n",
        "model = SigmaProtFlamingo(\n",
        "    model_path='nferruz/ProtGPT2',\n",
        "    max_len=914,  # Ensure this matches the training max_len\n",
        "    cross_attn_every=3,\n",
        "    dim_head=64,\n",
        "    heads=8,\n",
        "    perceiver_depth=2,\n",
        "    perceiver_num_latents=64\n",
        ").to(device)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OS0GAkWg-aV3"
      },
      "outputs": [],
      "source": [
        "ProteinGenerationDataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JHBmfbMT-FPX"
      },
      "outputs": [],
      "source": [
        "# Load trained weights\n",
        "model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
        "model.eval()\n",
        "\n",
        "# Load test data\n",
        "test_data = preprocess_snp_data('/content/augmented_test.csv')\n",
        "test_data = filter_datasets(test_data)\n",
        "\n",
        "# Create test dataset and dataloader\n",
        "test_dataset = ProteinGenerationDataset(test_data,max_length = 914 )\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=1,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sgmyq7fJ8kAb"
      },
      "outputs": [],
      "source": [
        "def generate_autoregressively(model, smiles_string, max_length=914, temperature=1.0, random_order=False):\n",
        "    \"\"\"Generate protein autoregressively, with option to use random order\"\"\"\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    # Get SMILES embeddings\n",
        "    smiles_embeddings = model.polybert_encoder([smiles_string])\n",
        "    processed_smiles = model.smiles_perceiver(smiles_embeddings)\n",
        "\n",
        "    # Initialize with start token\n",
        "    input_ids = torch.tensor([[model.protGPT2_tokenizer.bos_token_id]], device=device)\n",
        "\n",
        "    # If using random order, generate a random permutation\n",
        "    if random_order:\n",
        "        order = torch.randperm(max_length, device=device).unsqueeze(0)\n",
        "    else:\n",
        "        order = torch.arange(max_length, device=device).unsqueeze(0)\n",
        "\n",
        "    # Track the current positions in the order\n",
        "    current_pos = 0\n",
        "\n",
        "    # Generated sequence in order's positions\n",
        "    generated_sequence = torch.full((1, max_length), model.protGPT2_tokenizer.pad_token_id, device=device)\n",
        "    generated_sequence[0, 0] = model.protGPT2_tokenizer.bos_token_id  # Start token\n",
        "\n",
        "    while current_pos < max_length - 1:\n",
        "        # Get the next position in the order\n",
        "        next_pos = current_pos + 1\n",
        "\n",
        "        # Forward pass to get next token prediction\n",
        "        with torch.no_grad():\n",
        "            # Use only the sequence up to the current position\n",
        "            current_order = order[:, :next_pos]\n",
        "            current_sequence = generated_sequence[:, current_order[0]]\n",
        "\n",
        "            # Get logits for the next token\n",
        "            logits, _ = model(\n",
        "                smiles_string,\n",
        "                order=current_order,\n",
        "                optimize=True\n",
        "            )\n",
        "\n",
        "            # Apply temperature and sample\n",
        "            logits = logits[0, -1, :] / temperature\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            next_token = torch.multinomial(probs, num_samples=1).item()\n",
        "\n",
        "            # Add the token to the generated sequence at the next position in the order\n",
        "            generated_sequence[0, order[0, next_pos]] = next_token\n",
        "\n",
        "            # Check for EOS token\n",
        "            if next_token == model.protGPT2_tokenizer.eos_token_id:\n",
        "                break\n",
        "\n",
        "            current_pos = next_pos\n",
        "\n",
        "    # Decode the generated sequence\n",
        "    generated_ids = generated_sequence[0].tolist()\n",
        "    print('generated_ids',generated_ids)\n",
        "    # Remove padding tokens\n",
        "    generated_ids = [id for id in generated_ids if id != model.protGPT2_tokenizer.pad_token_id]\n",
        "    seq = model.protGPT2_tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
        "    print('seq',seq)\n",
        "    print(\"autoregressive gen done...\")\n",
        "    return seq\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Koy4bjRHu5hx"
      },
      "outputs": [],
      "source": [
        "def generate_with_rejection_sampling(model, smiles_string, max_length=914, num_orders=5, temperature=1.0):\n",
        "    \"\"\"Generate protein using token-based rejection sampling with proper MH acceptance ratio\"\"\"\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    # Get SMILES embeddings\n",
        "    smiles_embeddings = model.polybert_encoder([smiles_string])\n",
        "    processed_smiles = model.smiles_perceiver(smiles_embeddings)\n",
        "\n",
        "    # Initialize with start token\n",
        "    prompt = torch.tensor([[model.protGPT2_tokenizer.bos_token_id]], device=device)\n",
        "\n",
        "    # Initialize full sequence with padding\n",
        "    full_seq = torch.full((1, max_length), model.protGPT2_tokenizer.pad_token_id, device=device)\n",
        "    full_seq[:, 0] = model.protGPT2_tokenizer.bos_token_id  # Start token\n",
        "\n",
        "    # Track positions that have been filled\n",
        "    filled_positions = {0}  # Start with position 0 filled\n",
        "\n",
        "    while len(filled_positions) < max_length:\n",
        "        remaining_positions = [i for i in range(max_length) if i not in filled_positions]\n",
        "        if not remaining_positions:\n",
        "            break\n",
        "\n",
        "        # Step 1: Sample tokens at all remaining positions from marginal distribution\n",
        "        # This is our proposal distribution p(x̃)\n",
        "        candidate_tokens = {}\n",
        "        proposal_probs = {}  # Store the probability of each proposal\n",
        "\n",
        "        for pos in remaining_positions:\n",
        "            # Create current filled sequence context\n",
        "            current_context = torch.ones((1, max_length), device=device) * model.protGPT2_tokenizer.pad_token_id\n",
        "            for filled_pos in filled_positions:\n",
        "                current_context[0, filled_pos] = full_seq[0, filled_pos]\n",
        "\n",
        "            # Get logits for this position given current context\n",
        "            with torch.no_grad():\n",
        "                # Order that puts this position last\n",
        "                context_order = torch.tensor([list(filled_positions) + [pos]], device=device)\n",
        "\n",
        "                logits = get_logits_for_position(model, current_context, context_order, smiles_string, pos)\n",
        "\n",
        "                # Sample a token and record its probability\n",
        "                logits = logits / temperature\n",
        "                probs = F.softmax(logits, dim=-1)\n",
        "                token_dist = torch.distributions.Categorical(probs)\n",
        "                token = token_dist.sample().item()\n",
        "\n",
        "                candidate_tokens[pos] = token\n",
        "                proposal_probs[pos] = probs[0, token].item()\n",
        "\n",
        "        # Step 2: Evaluate acceptance under different orders\n",
        "        best_order_acceptances = []\n",
        "\n",
        "        for _ in range(num_orders):\n",
        "            # Create a random permutation of remaining positions\n",
        "            eval_order = random.sample(remaining_positions, len(remaining_positions))\n",
        "\n",
        "            accepted_tokens = []\n",
        "            accepted_positions = []\n",
        "            acceptance_ratios = []\n",
        "\n",
        "            # Try to accept tokens in this order\n",
        "            for pos in eval_order:\n",
        "                # Create sequence with previously accepted tokens\n",
        "                temp_seq = full_seq.clone()\n",
        "                for acc_pos in accepted_positions:\n",
        "                    temp_seq[0, acc_pos] = candidate_tokens[acc_pos]\n",
        "\n",
        "                # Get conditional probability q(x̃|X,x̃σ<i)\n",
        "                filled_plus_accepted = list(filled_positions) + accepted_positions\n",
        "                context_order = torch.tensor([filled_plus_accepted + [pos]], device=device)\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    cond_logits = get_logits_for_position(\n",
        "                        model, temp_seq, context_order, processed_smiles, pos\n",
        "                    )\n",
        "\n",
        "                    cond_probs = F.softmax(cond_logits / temperature, dim=-1)\n",
        "                    cond_prob = cond_probs[0, candidate_tokens[pos]].item()\n",
        "\n",
        "                # Compute acceptance ratio r = q(x̃i|X,x̃σ<i) / p(x̃i|X)\n",
        "                # Where p(x̃i|X) is the proposal probability\n",
        "                acceptance_ratio = min(1.0, cond_prob / proposal_probs[pos])\n",
        "\n",
        "                # Decide whether to accept\n",
        "                if random.random() < acceptance_ratio:\n",
        "                    accepted_tokens.append(candidate_tokens[pos])\n",
        "                    accepted_positions.append(pos)\n",
        "                    acceptance_ratios.append(acceptance_ratio)\n",
        "                else:\n",
        "                    # Stop at first rejection\n",
        "                    break\n",
        "\n",
        "            best_order_acceptances.append((accepted_positions, accepted_tokens, acceptance_ratios))\n",
        "\n",
        "        # Step 3: Dynamic token acceptance\n",
        "        best_order_idx = -1\n",
        "        max_accepted = -1\n",
        "        min_sequence_idx = -1\n",
        "\n",
        "        for idx, (accepted_positions, _, acceptance_ratios) in enumerate(best_order_acceptances):\n",
        "            if len(accepted_positions) > max_accepted:\n",
        "                max_accepted = len(accepted_positions)\n",
        "                best_order_idx = idx\n",
        "                # Find the minimum position in the sequence where we see a rejection\n",
        "                if len(accepted_positions) < len(remaining_positions):\n",
        "                    min_sequence_idx = len(accepted_positions)\n",
        "                else:\n",
        "                    min_sequence_idx = len(remaining_positions)\n",
        "\n",
        "        # No need to calculate min across orders if all orders accept all tokens\n",
        "        if min_sequence_idx == -1:\n",
        "            min_sequence_idx = len(remaining_positions)\n",
        "\n",
        "        # Get the best order\n",
        "        best_order = best_order_acceptances[best_order_idx]\n",
        "        accepted_positions, accepted_tokens, _ = best_order\n",
        "\n",
        "        # Limit acceptance to positions before the minimum rejection\n",
        "        accepted_positions = accepted_positions[:min_sequence_idx]\n",
        "        accepted_tokens = accepted_tokens[:min_sequence_idx]\n",
        "\n",
        "        # Update the sequence with accepted tokens\n",
        "        for pos, token in zip(accepted_positions, accepted_tokens):\n",
        "            full_seq[0, pos] = token\n",
        "            filled_positions.add(pos)\n",
        "\n",
        "            # Check for EOS token\n",
        "            if token == model.protGPT2_tokenizer.eos_token_id:\n",
        "                break\n",
        "\n",
        "    # Decode the generated sequence\n",
        "    result = model.protGPT2_tokenizer.decode(\n",
        "        [t for t in full_seq[0].tolist() if t != model.protGPT2_tokenizer.pad_token_id],\n",
        "        skip_special_tokens=True\n",
        "    )\n",
        "    return result\n",
        "\n",
        "def get_logits_for_position(model, sequence, order, smiles_string, target_position):\n",
        "    \"\"\"Helper function to get logits for a specific position\"\"\"\n",
        "    # Run model forward pass\n",
        "    logits, _ = model(\n",
        "        smiles_string,  # Pass the SMILES string\n",
        "        order=order,\n",
        "        optimize=True\n",
        "    )\n",
        "\n",
        "    # Return logits for target position (last position in the order)\n",
        "    return logits[:, -1, :]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IhpdI0vMXdUp"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2azHLfyMXUyR"
      },
      "outputs": [],
      "source": [
        "def evaluate_on_unique_smiles(model, test_loader, device, output_file=\"generated_proteins_comparison.json\"):\n",
        "    \"\"\"Generate proteins using both methods on unique SMILES from test set\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Collect unique SMILES from the test loader\n",
        "    unique_smiles = set()\n",
        "    for batch in test_loader:\n",
        "        unique_smiles.update(batch['smiles'])\n",
        "\n",
        "    unique_smiles = list(unique_smiles)  # Convert to list\n",
        "    print(f\"Found {len(unique_smiles)} unique SMILES in test set\")\n",
        "\n",
        "    results = []\n",
        "\n",
        "    # Generate proteins using both methods and time each generation\n",
        "    for i, smiles in enumerate(tqdm(unique_smiles, desc=\"Generating proteins\")):\n",
        "        # Track time for autoregressive generation\n",
        "        start_time = time.time()\n",
        "        print('autoregressive generations...')\n",
        "        print(f\"Generating protein for SMILES: {smiles}\")\n",
        "        ar_protein = generate_autoregressively(model, smiles, max_length=914, temperature=1.0, random_order=False)\n",
        "        print(ar_protein)\n",
        "        ar_time = time.time() - start_time\n",
        "\n",
        "        # Track time for rejection sampling\n",
        "        start_time = time.time()\n",
        "        print('rejection sampling generations...')\n",
        "        print(f\"Generating protein for SMILES: {smiles}\")\n",
        "        rs_protein = generate_with_rejection_sampling(model, smiles, max_length=914, num_orders=5, temperature=1.0)\n",
        "        print(rs_protein)\n",
        "        rs_time = time.time() - start_time\n",
        "\n",
        "        results.append({\n",
        "            'SMILES': smiles,\n",
        "            'Autoregressive': {\n",
        "                'protein': ar_protein,\n",
        "                'time_seconds': ar_time\n",
        "            },\n",
        "            'Rejection_Sampling': {\n",
        "                'protein': rs_protein,\n",
        "                'time_seconds': rs_time\n",
        "            }\n",
        "        })\n",
        "\n",
        "        # Print progress occasionally\n",
        "        if (i + 1) % 5 == 0:\n",
        "            print(f\"\\nCompleted {i+1}/{len(unique_smiles)}\")\n",
        "            print(f\"Example - SMILES: {smiles}\")\n",
        "            print(f\"Autoregressive: {ar_protein[:50]}... ({ar_time:.2f}s)\")\n",
        "            print(f\"Rejection Sampling: {rs_protein[:50]}... ({rs_time:.2f}s)\")\n",
        "\n",
        "    # Save results to JSON\n",
        "    with open(output_file, 'w') as f:\n",
        "        json.dump(results, f, indent=2)\n",
        "\n",
        "    # Calculate and print average times\n",
        "    ar_times = [r['Autoregressive']['time_seconds'] for r in results]\n",
        "    rs_times = [r['Rejection_Sampling']['time_seconds'] for r in results]\n",
        "\n",
        "    print(f\"\\nGeneration complete!\")\n",
        "    print(f\"Average autoregressive generation time: {np.mean(ar_times):.2f}s\")\n",
        "    print(f\"Average rejection sampling generation time: {np.mean(rs_times):.2f}s\")\n",
        "    print(f\"Speed improvement: {np.mean(ar_times)/np.mean(rs_times):.2f}x\")\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2xKIwu3LXe7K"
      },
      "outputs": [],
      "source": [
        "results = evaluate_on_unique_smiles(model, test_loader, device, output_file=\"sigma_gpt_comparison_results.json\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "ovRLzgZl65LR",
        "CsZYuP9onEl1",
        "B--CLs46QMLR",
        "Y84XLPeXQIEv",
        "daZm9aPyRUAM",
        "0Pe8VJFxQSS7",
        "6o91I0EXQaF4",
        "sLZXFhNq9xvv"
      ],
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}