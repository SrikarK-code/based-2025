{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NIBs8NvVH9fr"
      },
      "source": [
        "# scgpt + molgpt pipeline...."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfQbO5eHdGTg"
      },
      "source": [
        "## installs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VqZkQdRrdLHk",
        "outputId": "27dae762-b7a9-453d-b38a-43c9c974faf5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: anndata in /usr/local/lib/python3.11/dist-packages (0.11.3)\n",
            "Requirement already satisfied: array-api-compat!=1.5,>1.4 in /usr/local/lib/python3.11/dist-packages (from anndata) (1.11.1)\n",
            "Requirement already satisfied: h5py>=3.7 in /usr/local/lib/python3.11/dist-packages (from anndata) (3.12.1)\n",
            "Requirement already satisfied: natsort in /usr/local/lib/python3.11/dist-packages (from anndata) (8.4.0)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from anndata) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from anndata) (24.2)\n",
            "Requirement already satisfied: pandas!=2.1.0rc0,!=2.1.2,>=1.4 in /usr/local/lib/python3.11/dist-packages (from anndata) (2.2.2)\n",
            "Requirement already satisfied: scipy>1.8 in /usr/local/lib/python3.11/dist-packages (from anndata) (1.14.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas!=2.1.0rc0,!=2.1.2,>=1.4->anndata) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas!=2.1.0rc0,!=2.1.2,>=1.4->anndata) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas!=2.1.0rc0,!=2.1.2,>=1.4->anndata) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas!=2.1.0rc0,!=2.1.2,>=1.4->anndata) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install anndata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LJqzFFASdMTV",
        "outputId": "a2df7815-bb4b-4386-ea3e-837c0b4e17e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scanpy in /usr/local/lib/python3.11/dist-packages (1.11.0)\n",
            "Requirement already satisfied: anndata>=0.8 in /usr/local/lib/python3.11/dist-packages (from scanpy) (0.11.3)\n",
            "Requirement already satisfied: h5py>=3.7 in /usr/local/lib/python3.11/dist-packages (from scanpy) (3.12.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from scanpy) (1.4.2)\n",
            "Requirement already satisfied: legacy-api-wrap>=1.4 in /usr/local/lib/python3.11/dist-packages (from scanpy) (1.4.1)\n",
            "Requirement already satisfied: matplotlib>=3.6 in /usr/local/lib/python3.11/dist-packages (from scanpy) (3.10.0)\n",
            "Requirement already satisfied: natsort in /usr/local/lib/python3.11/dist-packages (from scanpy) (8.4.0)\n",
            "Requirement already satisfied: networkx>=2.7 in /usr/local/lib/python3.11/dist-packages (from scanpy) (3.4.2)\n",
            "Requirement already satisfied: numba>=0.57 in /usr/local/lib/python3.11/dist-packages (from scanpy) (0.60.0)\n",
            "Requirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.11/dist-packages (from scanpy) (2.0.2)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from scanpy) (24.2)\n",
            "Requirement already satisfied: pandas>=1.5 in /usr/local/lib/python3.11/dist-packages (from scanpy) (2.2.2)\n",
            "Requirement already satisfied: patsy!=1.0.0 in /usr/local/lib/python3.11/dist-packages (from scanpy) (1.0.1)\n",
            "Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.11/dist-packages (from scanpy) (0.5.13)\n",
            "Requirement already satisfied: scikit-learn<1.6.0,>=1.1 in /usr/local/lib/python3.11/dist-packages (from scanpy) (1.5.2)\n",
            "Requirement already satisfied: scipy>=1.8 in /usr/local/lib/python3.11/dist-packages (from scanpy) (1.14.1)\n",
            "Requirement already satisfied: seaborn>=0.13 in /usr/local/lib/python3.11/dist-packages (from scanpy) (0.13.2)\n",
            "Requirement already satisfied: session-info2 in /usr/local/lib/python3.11/dist-packages (from scanpy) (0.1.2)\n",
            "Requirement already satisfied: statsmodels>=0.13 in /usr/local/lib/python3.11/dist-packages (from scanpy) (0.14.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from scanpy) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from scanpy) (4.12.2)\n",
            "Requirement already satisfied: umap-learn!=0.5.0,>=0.5 in /usr/local/lib/python3.11/dist-packages (from scanpy) (0.5.7)\n",
            "Requirement already satisfied: array-api-compat!=1.5,>1.4 in /usr/local/lib/python3.11/dist-packages (from anndata>=0.8->scanpy) (1.11.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->scanpy) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->scanpy) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->scanpy) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->scanpy) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->scanpy) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->scanpy) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->scanpy) (2.8.2)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.57->scanpy) (0.43.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.5->scanpy) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.5->scanpy) (2025.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<1.6.0,>=1.1->scanpy) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.6->scanpy) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install scanpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T0GJDnbjelFH",
        "outputId": "00014bec-1364-465d-8af5-aae2ef83ad44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: rdkit in /usr/local/lib/python3.11/dist-packages (2024.9.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rdkit) (2.0.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from rdkit) (11.1.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install rdkit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lEQuXuRp6fvY",
        "outputId": "5022a8eb-8e99-4235-93e0-a6718eab459f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement deeplife-mlinfra (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for deeplife-mlinfra\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install deeplife-mlinfra\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZPsvyAScZDa"
      },
      "source": [
        "## scripts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zt4YmCK_BM3Y",
        "outputId": "678cc96d-3cca-4c94-fe9d-cfaf02637ddd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting scgpt_smiles_model.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile scgpt_smiles_model.py\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "import anndata\n",
        "import scanpy as sc\n",
        "import os\n",
        "import numpy as np\n",
        "from typing import Dict, List, Optional, Tuple, Union\n",
        "import math\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from molgpt_loader import load_mol_gpt_model\n",
        "\n",
        "\n",
        "# Model Components\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super().__init__()\n",
        "        position = torch.arange(max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "        pe = torch.zeros(max_len, 1, d_model)\n",
        "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:x.size(0)]\n",
        "\n",
        "class DoublePositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super().__init__()\n",
        "        # Use the full embedding dimension divided into two halves\n",
        "        self.d_model = d_model\n",
        "        half_dim = d_model // 2\n",
        "\n",
        "        # Create position encodings for both input and output positions\n",
        "        position = torch.arange(max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, half_dim, 2) * (-math.log(10000.0) / half_dim))\n",
        "\n",
        "        # Input position encodings\n",
        "        pe_input = torch.zeros(max_len, half_dim)\n",
        "        pe_input[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe_input[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        # Output position encodings\n",
        "        pe_output = torch.zeros(max_len, half_dim)\n",
        "        pe_output[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe_output[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        self.register_buffer('pe_input', pe_input)\n",
        "        self.register_buffer('pe_output', pe_output)\n",
        "\n",
        "    def forward(self, x, input_positions, output_positions):\n",
        "        batch_size, seq_length, _ = x.shape\n",
        "\n",
        "        # Create a tensor of zeros with the same shape as the input\n",
        "        pos_encoding = torch.zeros_like(x)\n",
        "\n",
        "        # For each item in the batch\n",
        "        for b in range(batch_size):\n",
        "            for t in range(seq_length):\n",
        "                # Get the input and output positions for this token\n",
        "                input_pos = input_positions[b, t] if input_positions is not None else t\n",
        "                output_pos = output_positions[b, t] if output_positions is not None else t\n",
        "\n",
        "                if input_pos < self.pe_input.size(0) and output_pos < self.pe_output.size(0):\n",
        "                    # Fill the first half with input position encoding\n",
        "                    pos_encoding[b, t, :self.d_model//2] = self.pe_input[input_pos]\n",
        "                    # Fill the second half with output position encoding\n",
        "                    pos_encoding[b, t, self.d_model//2:] = self.pe_output[output_pos]\n",
        "\n",
        "        return x + pos_encoding\n",
        "\n",
        "class PerceiverAttention(nn.Module):\n",
        "    def __init__(self, dim, dim_head=64, heads=8):\n",
        "        super().__init__()\n",
        "        self.scale = dim_head ** -0.5\n",
        "        self.heads = heads\n",
        "        inner_dim = dim_head * heads\n",
        "\n",
        "        self.norm_media = nn.LayerNorm(dim)\n",
        "        self.norm_latents = nn.LayerNorm(dim)\n",
        "        self.to_q = nn.Linear(dim, inner_dim, bias=False)\n",
        "        self.to_kv = nn.Linear(dim, inner_dim * 2, bias=False)\n",
        "        self.to_out = nn.Linear(inner_dim, dim, bias=False)\n",
        "\n",
        "    def forward(self, x, latents):\n",
        "        \"\"\"\n",
        "        x: [batch_size, seq_len_x, dim]\n",
        "        latents: [batch_size, seq_len_l, dim]\n",
        "        \"\"\"\n",
        "        batch_size = x.shape[0]\n",
        "\n",
        "        x = self.norm_media(x)\n",
        "        latents = self.norm_latents(latents)\n",
        "\n",
        "        # Ensure latents has correct batch size\n",
        "        if latents.size(0) != batch_size:\n",
        "            latents = latents.expand(batch_size, -1, -1)\n",
        "\n",
        "        q = self.to_q(latents)\n",
        "        # Handle rearrange function\n",
        "        q = q.view(batch_size, -1, self.heads, q.size(-1) // self.heads).transpose(1, 2)\n",
        "        q = q * self.scale\n",
        "\n",
        "        # Ensure proper concatenation\n",
        "        kv_input = torch.cat((x, latents), dim=1)  # concatenate along sequence dimension\n",
        "        k, v = self.to_kv(kv_input).chunk(2, dim=-1)\n",
        "        # Handle rearrange function\n",
        "        k = k.view(batch_size, -1, self.heads, k.size(-1) // self.heads).transpose(1, 2)\n",
        "        v = v.view(batch_size, -1, self.heads, v.size(-1) // self.heads).transpose(1, 2)\n",
        "\n",
        "        sim = torch.einsum('b h i d, b h j d -> b h i j', q, k)\n",
        "        attn = sim.softmax(dim=-1)\n",
        "        out = torch.einsum('b h i j, b h j d -> b h i d', attn, v)\n",
        "        # Handle rearrange function\n",
        "        out = out.transpose(1, 2).contiguous().view(batch_size, -1, self.heads * out.size(-1))\n",
        "\n",
        "        return self.to_out(out)\n",
        "\n",
        "class GatedCrossAttentionBlock(nn.Module):\n",
        "    def __init__(self, dim, dim_head=64, heads=8, ff_mult=4):\n",
        "        super().__init__()\n",
        "        self.attn = PerceiverAttention(dim=dim, dim_head=dim_head, heads=heads)\n",
        "        self.attn_gate = nn.Parameter(torch.tensor([0.]))\n",
        "        self.ff = FeedForward(dim, mult=ff_mult)\n",
        "        self.ff_gate = nn.Parameter(torch.tensor([0.]))\n",
        "\n",
        "    def forward(self, x, media):\n",
        "        \"\"\"\n",
        "        x: [batch_size, seq_len_x, dim]\n",
        "        media: [batch_size, seq_len_m, dim]\n",
        "        \"\"\"\n",
        "        batch_size = x.shape[0]\n",
        "        target_batch_size = media.size(0)\n",
        "\n",
        "        # Handle batch size mismatch\n",
        "        if batch_size > target_batch_size:\n",
        "            media = media.expand(batch_size, -1, -1)\n",
        "        elif batch_size < target_batch_size:\n",
        "            x = x.expand(target_batch_size, -1, -1)\n",
        "\n",
        "        gate = self.attn_gate.tanh()\n",
        "        x = self.attn(media, x) * gate + x\n",
        "        x = self.ff(x) * self.ff_gate.tanh() + x\n",
        "        return x\n",
        "\n",
        "class PerceiverResampler(nn.Module):\n",
        "    def __init__(self, dim, depth, dim_head=64, heads=8, num_latents=64):\n",
        "        super().__init__()\n",
        "        # Initialize latents without batch dimension\n",
        "        self.latents = nn.Parameter(torch.randn(num_latents, dim))\n",
        "        self.layers = nn.ModuleList([])\n",
        "\n",
        "        for _ in range(depth):\n",
        "            self.layers.append(nn.ModuleList([\n",
        "                PerceiverAttention(dim=dim, dim_head=dim_head, heads=heads),\n",
        "                FeedForward(dim=dim)\n",
        "            ]))\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.shape[0]\n",
        "        if len(x.shape) == 2:\n",
        "          x = x.unsqueeze(1)  # [batch_size, embedding_dim] -> [batch_size, 1, embedding_dim]\n",
        "\n",
        "        # Expand latents to batch size (handle repeat function)\n",
        "        latents = self.latents.unsqueeze(0).expand(batch_size, -1, -1)\n",
        "\n",
        "        for attn, ff in self.layers:\n",
        "            latents = attn(x, latents) + latents\n",
        "            latents = ff(latents) + latents\n",
        "\n",
        "        return latents\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, dim, mult=4):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.LayerNorm(dim),\n",
        "            nn.Linear(dim, dim * mult, bias=False),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(dim * mult, dim, bias=False)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import AllChem, Descriptors, Lipinski, QED\n",
        "from rdkit.Chem import DataStructs\n",
        "import numpy as np\n",
        "from scipy.spatial.distance import cosine\n",
        "\n",
        "class MoleculeMetrics:\n",
        "    @staticmethod\n",
        "    def is_valid_smiles(smiles):\n",
        "        \"\"\"Check if a SMILES string represents a valid molecule.\"\"\"\n",
        "        mol = Chem.MolFromSmiles(smiles)\n",
        "        return mol is not None\n",
        "\n",
        "    @staticmethod\n",
        "    def tanimoto_similarity(smiles1, smiles2):\n",
        "        \"\"\"Calculate Tanimoto similarity between two SMILES strings.\"\"\"\n",
        "        mol1 = Chem.MolFromSmiles(smiles1)\n",
        "        mol2 = Chem.MolFromSmiles(smiles2)\n",
        "\n",
        "        if mol1 is None or mol2 is None:\n",
        "            return 0.0\n",
        "\n",
        "        fp1 = AllChem.GetMorganFingerprintAsBitVect(mol1, 2, nBits=2048)\n",
        "        fp2 = AllChem.GetMorganFingerprintAsBitVect(mol2, 2, nBits=2048)\n",
        "\n",
        "        return DataStructs.TanimotoSimilarity(fp1, fp2)\n",
        "\n",
        "    @staticmethod\n",
        "    def compute_fingerprint(smiles):\n",
        "        \"\"\"Compute Morgan fingerprint for a molecule.\"\"\"\n",
        "        mol = Chem.MolFromSmiles(smiles)\n",
        "        if mol is None:\n",
        "            return None\n",
        "        return AllChem.GetMorganFingerprintAsBitVect(mol, 2, nBits=2048)\n",
        "\n",
        "    @staticmethod\n",
        "    def fingerprint_to_numpy(fingerprint):\n",
        "        \"\"\"Convert RDKit fingerprint to numpy array.\"\"\"\n",
        "        if fingerprint is None:\n",
        "            return None\n",
        "        arr = np.zeros((1,))\n",
        "        DataStructs.ConvertToNumpyArray(fingerprint, arr)\n",
        "        return arr\n",
        "\n",
        "    @staticmethod\n",
        "    def calc_cosine_distance(fp1, fp2):\n",
        "        \"\"\"Calculate cosine distance between two fingerprints.\"\"\"\n",
        "        if fp1 is None or fp2 is None:\n",
        "            return 1.0  # Maximum distance\n",
        "        return cosine(fp1, fp2)\n",
        "\n",
        "    @staticmethod\n",
        "    def calculate_druglikeness_metrics(smiles):\n",
        "        \"\"\"Calculate druglikeness using Lipinski's Rule of 5 and QED.\"\"\"\n",
        "        mol = Chem.MolFromSmiles(smiles)\n",
        "        if mol is None:\n",
        "            return {\n",
        "                'valid': 0,\n",
        "                'mw': None,\n",
        "                'logp': None,\n",
        "                'hba': None,\n",
        "                'hbd': None,\n",
        "                'rotatable_bonds': None,\n",
        "                'qed': None,\n",
        "                'lipinski_violations': None,\n",
        "                'rule_of_5': 0\n",
        "            }\n",
        "\n",
        "        # Calculate druglikeness properties\n",
        "        mw = Descriptors.MolWt(mol)\n",
        "        logp = Descriptors.MolLogP(mol)\n",
        "        hba = Descriptors.NumHAcceptors(mol)\n",
        "        hbd = Descriptors.NumHDonors(mol)\n",
        "        rotatable_bonds = Descriptors.NumRotatableBonds(mol)\n",
        "        qed = QED.qed(mol)\n",
        "\n",
        "        # Count Lipinski violations\n",
        "        lipinski_violations = 0\n",
        "        if mw > 500: lipinski_violations += 1\n",
        "        if logp > 5: lipinski_violations += 1\n",
        "        if hba > 10: lipinski_violations += 1\n",
        "        if hbd > 5: lipinski_violations += 1\n",
        "\n",
        "        # Rule of 5 pass (0 or 1 violations is considered a pass)\n",
        "        rule_of_5 = 1 if lipinski_violations <= 1 else 0\n",
        "\n",
        "        return {\n",
        "            'valid': 1,\n",
        "            'mw': mw,\n",
        "            'logp': logp,\n",
        "            'hba': hba,\n",
        "            'hbd': hbd,\n",
        "            'rotatable_bonds': rotatable_bonds,\n",
        "            'qed': qed,\n",
        "            'lipinski_violations': lipinski_violations,\n",
        "            'rule_of_5': rule_of_5\n",
        "        }\n",
        "\n",
        "    @staticmethod\n",
        "    def evaluate_molecule_pair(ground_truth, generated):\n",
        "        \"\"\"Evaluate a pair of ground truth and generated SMILES strings.\"\"\"\n",
        "        # Validate molecules\n",
        "        gt_valid = MoleculeMetrics.is_valid_smiles(ground_truth)\n",
        "        gen_valid = MoleculeMetrics.is_valid_smiles(generated)\n",
        "\n",
        "        # Calculate similarity if both are valid\n",
        "        similarity = 0.0\n",
        "        if gt_valid and gen_valid:\n",
        "            similarity = MoleculeMetrics.tanimoto_similarity(ground_truth, generated)\n",
        "\n",
        "        # Get druglikeness metrics for generated molecule\n",
        "        gen_metrics = MoleculeMetrics.calculate_druglikeness_metrics(generated)\n",
        "\n",
        "        # Create result dictionary\n",
        "        result = {\n",
        "            'ground_truth': ground_truth,\n",
        "            'generated': generated,\n",
        "            'gt_valid': 1 if gt_valid else 0,\n",
        "            'gen_valid': 1 if gen_valid else 0,\n",
        "            'tanimoto_similarity': similarity,\n",
        "            **gen_metrics\n",
        "        }\n",
        "\n",
        "        return result\n",
        "\n",
        "    @staticmethod\n",
        "    def aggregate_metrics(results):\n",
        "        \"\"\"Aggregate metrics across multiple molecule pairs.\"\"\"\n",
        "        metrics = {\n",
        "            'total_pairs': len(results),\n",
        "            'gt_valid_ratio': np.mean([r['gt_valid'] for r in results]),\n",
        "            'gen_valid_ratio': np.mean([r['gen_valid'] for r in results]),\n",
        "            'avg_tanimoto': np.mean([r['tanimoto_similarity'] for r in results]),\n",
        "            'avg_qed': np.mean([r['qed'] for r in results if r['qed'] is not None]),\n",
        "            'rule_of_5_pass_ratio': np.mean([r['rule_of_5'] for r in results if r['rule_of_5'] is not None]),\n",
        "            'avg_lipinski_violations': np.mean([r['lipinski_violations'] for r in results if r['lipinski_violations'] is not None])\n",
        "        }\n",
        "\n",
        "        # Calculate additional statistics\n",
        "        valid_indices = [i for i, r in enumerate(results) if r['gen_valid'] == 1]\n",
        "        if valid_indices:\n",
        "            valid_results = [results[i] for i in valid_indices]\n",
        "            metrics.update({\n",
        "                'valid_molecules': len(valid_results),\n",
        "                'avg_mw': np.mean([r['mw'] for r in valid_results]),\n",
        "                'avg_logp': np.mean([r['logp'] for r in valid_results]),\n",
        "                'avg_hba': np.mean([r['hba'] for r in valid_results]),\n",
        "                'avg_hbd': np.mean([r['hbd'] for r in valid_results]),\n",
        "                'avg_rotatable_bonds': np.mean([r['rotatable_bonds'] for r in valid_results])\n",
        "            })\n",
        "\n",
        "        return metrics\n",
        "\n",
        "\n",
        "class SCGPTSmilesGPT(nn.Module):\n",
        "    def __init__(self, max_len,\n",
        "                 cross_attn_every=3, dim_head=64, heads=8,\n",
        "                 perceiver_depth=2, perceiver_num_latents=64,\n",
        "                 working_dir=\"./scgpt_smiles_workdir\",\n",
        "                 embedding_dim=512):\n",
        "        super().__init__()\n",
        "\n",
        "        # Load molGPT model\n",
        "        self.smilesgpt_model, self.smilesgpt_tokenizer = load_mol_gpt_model()\n",
        "        self.max_len = max_len\n",
        "        self.working_dir = working_dir\n",
        "\n",
        "        if self.smilesgpt_tokenizer.pad_token is None:\n",
        "            self.smilesgpt_tokenizer.pad_token = self.smilesgpt_tokenizer.eos_token\n",
        "            self.smilesgpt_model.config.pad_token_id = self.smilesgpt_model.config.eos_token_id\n",
        "\n",
        "        self.cross_attn_every = cross_attn_every\n",
        "\n",
        "        # Add projection layer to adapt scGPT embedding dimension to molGPT dimension if needed\n",
        "        self.projection = nn.Linear(embedding_dim, self.smilesgpt_model.config.n_embd)\n",
        "\n",
        "        self.positional_encoding = DoublePositionalEncoding(\n",
        "            self.smilesgpt_model.config.n_embd,\n",
        "            max_len=max_len\n",
        "        )\n",
        "\n",
        "        self.cell_perceiver = PerceiverResampler(\n",
        "            dim=self.smilesgpt_model.config.n_embd,\n",
        "            depth=perceiver_depth,\n",
        "            dim_head=dim_head,\n",
        "            heads=heads,\n",
        "            num_latents=perceiver_num_latents\n",
        "        )\n",
        "\n",
        "        num_gpt_layers = len(self.smilesgpt_model.transformer.h)\n",
        "        self.cross_attn = nn.ModuleList([\n",
        "            GatedCrossAttentionBlock(\n",
        "                dim=self.smilesgpt_model.config.n_embd,\n",
        "                dim_head=dim_head,\n",
        "                heads=heads\n",
        "            )\n",
        "            for _ in range(num_gpt_layers)\n",
        "        ])\n",
        "\n",
        "        # # Modified initialization with specific layer targeting\n",
        "        # self.layers = nn.ModuleList()\n",
        "        # num_layers = len(self.smilesgpt_model.transformer.h)\n",
        "        # targeted_layers = [num_layers-2, num_layers-1]  # 11th and 12th layer (0-indexed)\n",
        "\n",
        "        # for i, block in enumerate(self.smilesgpt_model.transformer.h):\n",
        "        #     self.layers.append(block)\n",
        "        #     if i in targeted_layers:\n",
        "        #         print(f\"Adding cross-attention after transformer layer {i+1}/{num_layers}\")\n",
        "        #         self.layers.append(\n",
        "        #             GatedCrossAttentionBlock(\n",
        "        #                 dim=self.smilesgpt_model.config.n_embd,\n",
        "        #                 dim_head=dim_head,\n",
        "        #                 heads=heads\n",
        "        #             )\n",
        "        #         )\n",
        "\n",
        "        # # Add an additional cross-attention before LM head\n",
        "        # print(f\"Adding cross-attention before LM head\")\n",
        "        # self.final_cross_attn = GatedCrossAttentionBlock(\n",
        "        #     dim=self.smilesgpt_model.config.n_embd,\n",
        "        #     dim_head=dim_head,\n",
        "        #     heads=heads\n",
        "        # )\n",
        "\n",
        "        # Replace this entire section in __init__\n",
        "        self.layers = nn.ModuleList()\n",
        "        for i, block in enumerate(self.smilesgpt_model.transformer.h):\n",
        "            self.layers.append(block)\n",
        "            # Don't add any cross-attention blocks here\n",
        "\n",
        "        # Now add cross-attention blocks only after the specified layers\n",
        "        num_layers = len(self.smilesgpt_model.transformer.h)\n",
        "        layer_indices_for_cross_attn = [num_layers-2, num_layers-1]  # Last two layers\n",
        "\n",
        "        for i in layer_indices_for_cross_attn:\n",
        "            # Get the correct index in our layers list\n",
        "            idx = i\n",
        "            # Insert cross-attention after this layer\n",
        "            print(f\"Adding cross-attention after transformer layer {i+1}/{num_layers}\")\n",
        "            self.layers.insert(idx+1, GatedCrossAttentionBlock(\n",
        "                dim=self.smilesgpt_model.config.n_embd,\n",
        "                dim_head=dim_head,\n",
        "                heads=heads\n",
        "            ))\n",
        "\n",
        "        # Add final cross-attention before LM head\n",
        "        print(f\"Adding cross-attention before LM head\")\n",
        "        self.final_cross_attn = GatedCrossAttentionBlock(\n",
        "            dim=self.smilesgpt_model.config.n_embd,\n",
        "            dim_head=dim_head,\n",
        "            heads=heads\n",
        "        )\n",
        "\n",
        "    def forward(self, cell_embeddings, targets=None, optimize=False, order=None, use_masking=False):\n",
        "        device = next(self.parameters()).device\n",
        "\n",
        "        cell_embeddings = self.projection(cell_embeddings)\n",
        "        processed_cells = self.cell_perceiver(cell_embeddings)\n",
        "\n",
        "        batch_size = cell_embeddings.size(0)\n",
        "\n",
        "        if use_masking and targets is not None:\n",
        "            masked_inputs, mask = apply_masking(targets, self.smilesgpt_tokenizer)\n",
        "            hidden_states = self.smilesgpt_model.transformer.wte(masked_inputs)\n",
        "            seq_length = masked_inputs.size(1)\n",
        "\n",
        "            attention_mask = torch.ones((batch_size, seq_length), device=device)\n",
        "            pad_mask = (masked_inputs == self.smilesgpt_tokenizer.pad_token_id)\n",
        "            attention_mask[pad_mask] = 0\n",
        "        else:\n",
        "            gpt_input = self.smilesgpt_tokenizer.encode_plus(\n",
        "                \"\",\n",
        "                return_tensors=\"pt\",\n",
        "                padding='max_length',\n",
        "                max_length=self.max_len,\n",
        "                truncation=True\n",
        "            ).to(device)\n",
        "\n",
        "            input_ids = gpt_input.input_ids.long()\n",
        "            seq_length = input_ids.size(1)\n",
        "            hidden_states = self.smilesgpt_model.transformer.wte(input_ids)\n",
        "            attention_mask = gpt_input.attention_mask\n",
        "\n",
        "            if not optimize and not use_masking:\n",
        "                if order is None:\n",
        "                    order = torch.arange(seq_length, device=device).unsqueeze(0).repeat(batch_size, 1)\n",
        "\n",
        "                if order.size(1) > seq_length:\n",
        "                    order = order[:, :seq_length]\n",
        "                elif order.size(1) < seq_length:\n",
        "                    padding = torch.arange(order.size(1), seq_length, device=device).unsqueeze(0).repeat(batch_size, 1)\n",
        "                    order = torch.cat([order, padding], dim=1)\n",
        "\n",
        "                reordered_input_ids = torch.zeros_like(input_ids)\n",
        "                for b in range(batch_size):\n",
        "                    reordered_input_ids[b] = input_ids[b, order[b]]\n",
        "\n",
        "                hidden_states = self.smilesgpt_model.transformer.wte(reordered_input_ids)\n",
        "\n",
        "        if order is None:\n",
        "            order = torch.arange(seq_length, device=device).unsqueeze(0).repeat(batch_size, 1)\n",
        "\n",
        "        input_positions = order\n",
        "        output_positions = torch.roll(order, -1, dims=1)\n",
        "        output_positions[:, -1] = order[:, 0]\n",
        "\n",
        "        hidden_states = self.positional_encoding(hidden_states, input_positions, output_positions)\n",
        "\n",
        "        num_heads = self.smilesgpt_model.config.n_head\n",
        "\n",
        "        attention_mask = attention_mask.view(batch_size, 1, 1, seq_length)\n",
        "        attention_mask = attention_mask.expand(batch_size, num_heads, seq_length, seq_length)\n",
        "        attention_mask = attention_mask.to(dtype=hidden_states.dtype)\n",
        "\n",
        "        seq_indices = torch.arange(seq_length, device=device)\n",
        "        expanded_seq_indices_i = seq_indices.unsqueeze(1).expand(seq_length, seq_length)\n",
        "        expanded_seq_indices_j = seq_indices.unsqueeze(0).expand(seq_length, seq_length)\n",
        "\n",
        "        causal_mask = torch.zeros((batch_size, seq_length, seq_length), device=device)\n",
        "        for b in range(batch_size):\n",
        "            order_b = order[b]\n",
        "            order_i = order_b[expanded_seq_indices_i]\n",
        "            order_j = order_b[expanded_seq_indices_j]\n",
        "            causal_mask[b] = (order_j <= order_i).float()\n",
        "\n",
        "        if use_masking:\n",
        "            combined_mask = attention_mask\n",
        "        else:\n",
        "            causal_mask = causal_mask.unsqueeze(1)\n",
        "            combined_mask = attention_mask * causal_mask\n",
        "\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            if isinstance(layer, GatedCrossAttentionBlock):\n",
        "                hidden_states = layer(hidden_states, processed_cells)\n",
        "            else:\n",
        "                hidden_states = layer(hidden_states, attention_mask=combined_mask)[0]\n",
        "\n",
        "        # Apply final cross-attention before LM head\n",
        "        hidden_states = self.final_cross_attn(hidden_states, processed_cells)\n",
        "\n",
        "        logits = self.smilesgpt_model.lm_head(hidden_states)\n",
        "\n",
        "        if targets is None:\n",
        "            if optimize:\n",
        "                return logits[:, [-1], :], None\n",
        "            return logits, None\n",
        "\n",
        "        if targets is not None:\n",
        "            if use_masking:\n",
        "                loss = F.cross_entropy(\n",
        "                    logits.view(-1, logits.size(-1)),\n",
        "                    targets.view(-1),\n",
        "                    ignore_index=self.smilesgpt_tokenizer.pad_token_id\n",
        "                )\n",
        "            else:\n",
        "                shuffled_targets = torch.zeros_like(targets)\n",
        "                for b in range(batch_size):\n",
        "                    shuffled_targets[b] = targets[b, order[b]]\n",
        "\n",
        "                loss = F.cross_entropy(\n",
        "                    logits.view(-1, logits.size(-1)),\n",
        "                    shuffled_targets.view(-1),\n",
        "                    ignore_index=self.smilesgpt_tokenizer.pad_token_id\n",
        "                )\n",
        "        else:\n",
        "            loss = None\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate_smiles(self, cell_embedding, max_length=200, top_p=0.95):\n",
        "        device = next(self.parameters()).device\n",
        "\n",
        "        if cell_embedding.dim() == 1:\n",
        "            cell_embedding = cell_embedding.unsqueeze(0)\n",
        "\n",
        "\n",
        "        print(f\"Cell embeddings shape: {cell_embedding.shape}\")\n",
        "\n",
        "        # Project cell embeddings to match GPT embedding dimension if necessary\n",
        "        cell_embedding = self.projection(cell_embedding)\n",
        "        processed_cells = self.cell_perceiver(cell_embedding)\n",
        "\n",
        "        # Start with BOS token\n",
        "        input_ids = torch.tensor([[self.smilesgpt_tokenizer.bos_token_id]]).to(device)\n",
        "\n",
        "        for _ in range(max_length):\n",
        "            inputs_embeds = self.smilesgpt_model.transformer.wte(input_ids)\n",
        "            inputs_embeds = self.positional_encoding(inputs_embeds, None, None)\n",
        "\n",
        "            hidden_states = inputs_embeds\n",
        "\n",
        "            for i, layer in enumerate(self.layers):\n",
        "                if isinstance(layer, GatedCrossAttentionBlock):\n",
        "                    hidden_states = layer(hidden_states, processed_cells)\n",
        "                else:\n",
        "                    hidden_states = layer(hidden_states, attention_mask=None)[0]\n",
        "\n",
        "            next_token_logits = self.smilesgpt_model.lm_head(hidden_states[:, -1, :])\n",
        "\n",
        "            filtered_logits = top_p_filtering(next_token_logits, top_p=top_p)\n",
        "            probs = F.softmax(filtered_logits, dim=-1)\n",
        "            next_token = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "            input_ids = torch.cat([input_ids, next_token], dim=-1)\n",
        "\n",
        "            if next_token.item() == self.smilesgpt_tokenizer.eos_token_id:\n",
        "                break\n",
        "\n",
        "        return self.smilesgpt_tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    def state_dict(self):\n",
        "        state_dict = super().state_dict()\n",
        "        state_dict['cell_perceiver'] = self.cell_perceiver.state_dict()\n",
        "        state_dict['cross_attn'] = self.cross_attn.state_dict()\n",
        "        return state_dict\n",
        "\n",
        "    def load_state_dict(self, state_dict):\n",
        "        cell_perceiver_state = state_dict.pop('cell_perceiver')\n",
        "        cross_attn_state = state_dict.pop('cross_attn')\n",
        "\n",
        "        super().load_state_dict(state_dict)\n",
        "\n",
        "        self.cell_perceiver.load_state_dict(cell_perceiver_state)\n",
        "        self.cross_attn.load_state_dict(cross_attn_state)\n",
        "\n",
        "\n",
        "def prepare_all_drugs_dataset(adata_dir, smiles_path, tokenizer, max_length,\n",
        "                           working_dir, batch_size=25):\n",
        "    import scanpy as sc\n",
        "    import pandas as pd\n",
        "\n",
        "    # Load SMILES data\n",
        "    smiles_df = pd.read_csv(smiles_path)\n",
        "\n",
        "    all_pairs = []\n",
        "\n",
        "    # Get all h5ad files in the directory\n",
        "    drug_files = [f for f in os.listdir(adata_dir) if f.endswith('.h5ad')]\n",
        "    print(f\"Found {len(drug_files)} drug files in {adata_dir}\")\n",
        "\n",
        "    for drug_file in drug_files:\n",
        "        drug_path = os.path.join(adata_dir, drug_file)\n",
        "        try:\n",
        "            # Extract drug name from filename\n",
        "            import re\n",
        "            # Regular expression to extract the drug name\n",
        "            match = re.search(r'([A-Za-z0-9-]+)_block', drug_file)\n",
        "\n",
        "            if match:\n",
        "                drug_name = match.group(1)\n",
        "            else:\n",
        "                drug_name = None  # Or handle the case when no match is found\n",
        "            print(f\"Processing {drug_name} from {drug_file}\")\n",
        "\n",
        "            # Skip if drug not in SMILES data\n",
        "            if not any(smiles_df['drug'] == drug_name):\n",
        "                print(f\"No SMILES data for {drug_name}, skipping\")\n",
        "                continue\n",
        "\n",
        "            # Load the h5ad file with pre-computed embeddings\n",
        "            drug_adata = sc.read_h5ad(drug_path)\n",
        "\n",
        "            # Get the precomputed differential embedding\n",
        "            diff_embedding = torch.tensor(drug_adata.obsm[\"X_scGPT\"].mean(axis=0))\n",
        "\n",
        "            # Get SMILES for this drug\n",
        "            drug_smiles_df = smiles_df[smiles_df['drug'] == drug_name]\n",
        "\n",
        "            for _, row in drug_smiles_df.iterrows():\n",
        "                all_pairs.append(\n",
        "                    SCGPTSmilesPair(\n",
        "                        cell_embedding=diff_embedding,\n",
        "                        smiles_string=row['SMILES']\n",
        "                    )\n",
        "                )\n",
        "\n",
        "            print(f\"Added {len(drug_smiles_df)} pairs for {drug_name}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {drug_file}: {e}\")\n",
        "            continue\n",
        "\n",
        "    print(f\"Created a total of {len(all_pairs)} cell-SMILES pairs\")\n",
        "    return all_pairs\n",
        "\n",
        "\n",
        "def top_p_filtering(logits, top_p=0.9, filter_value=-float('Inf')):\n",
        "    sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
        "    cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "\n",
        "    sorted_indices_to_remove = cumulative_probs > top_p\n",
        "    sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
        "    sorted_indices_to_remove[..., 0] = 0\n",
        "\n",
        "    indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
        "    logits[indices_to_remove] = filter_value\n",
        "\n",
        "    return logits\n",
        "\n",
        "\n",
        "class SCGPTSmilesPair:\n",
        "    def __init__(self, cell_embedding, smiles_string):\n",
        "        self.cell_embedding = cell_embedding\n",
        "        self.smiles_string = smiles_string\n",
        "\n",
        "\n",
        "class CellSmilesDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, pairs, tokenizer, max_length):\n",
        "        self.pairs = pairs\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        pair = self.pairs[idx]\n",
        "\n",
        "        smiles_encoding = self.tokenizer(\n",
        "            pair.smiles_string,\n",
        "            max_length=self.max_length,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"cell_embedding\": pair.cell_embedding,\n",
        "            \"smiles_tokens\": smiles_encoding.input_ids.squeeze(0),\n",
        "            \"attention_mask\": smiles_encoding.attention_mask.squeeze(0)\n",
        "        }\n",
        "\n",
        "\n",
        "def collate_batch(batch):\n",
        "    cell_embeddings = torch.stack([item[\"cell_embedding\"] for item in batch])\n",
        "    smiles_tokens = torch.stack([item[\"smiles_tokens\"] for item in batch])\n",
        "    attention_masks = torch.stack([item[\"attention_mask\"] for item in batch])\n",
        "\n",
        "    return {\n",
        "        \"cell_embeddings\": cell_embeddings,\n",
        "        \"smiles_tokens\": smiles_tokens,\n",
        "        \"attention_masks\": attention_masks\n",
        "    }\n",
        "\n",
        "\n",
        "def apply_masking(input_ids, tokenizer, mask_probability=0.15):\n",
        "    \"\"\"\n",
        "    Apply masked language modeling to the input tokens.\n",
        "    Uses 'M' as a mask token if no mask token is defined.\n",
        "\n",
        "    Args:\n",
        "        input_ids (torch.Tensor): Input token IDs of shape [batch_size, seq_len]\n",
        "        tokenizer: The tokenizer\n",
        "        mask_probability (float): Probability of masking a token\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Masked input IDs\n",
        "        torch.Tensor: Mask indicating which positions were masked\n",
        "    \"\"\"\n",
        "    # Make a copy of the input_ids\n",
        "    masked_input_ids = input_ids.clone()\n",
        "    batch_size, seq_len = input_ids.shape\n",
        "    device = input_ids.device\n",
        "\n",
        "    # Create a mask for tokens that can be masked (exclude padding and special tokens)\n",
        "    pad_token_id = tokenizer.pad_token_id\n",
        "    eos_token_id = tokenizer.eos_token_id\n",
        "    bos_token_id = tokenizer.bos_token_id\n",
        "\n",
        "    # Get mask token ID or create one using 'M'\n",
        "    if hasattr(tokenizer, 'mask_token_id') and tokenizer.mask_token_id is not None:\n",
        "        mask_token_id = tokenizer.mask_token_id\n",
        "    else:\n",
        "        # Try to get ID for 'M' token or use a fallback token\n",
        "        try:\n",
        "            # Get token ID for 'M' (assuming this is unused in SMILES)\n",
        "            mask_token_id = tokenizer.convert_tokens_to_ids('M')\n",
        "            # If the tokenizer returns a special \"unknown\" value, use a different approach\n",
        "            if mask_token_id == tokenizer.unk_token_id:\n",
        "                # Try to get a token ID by encoding\n",
        "                mask_encoded = tokenizer.encode('M', add_special_tokens=False)\n",
        "                if mask_encoded:\n",
        "                    mask_token_id = mask_encoded[0]\n",
        "                else:\n",
        "                    # Last resort: use a random token ID that's not a special token\n",
        "                    # This is just for training purposes, it doesn't have to be meaningful\n",
        "                    mask_token_id = 10  # Arbitrary token ID\n",
        "                    print('WARNING: USING ARBITRARY 10 TOKEN')\n",
        "        except:\n",
        "            # If all else fails, use a fallback token ID\n",
        "            mask_token_id = 10  # Arbitrary token ID\n",
        "            print('WARNING: USING ARBITRARY 10 TOKEN')\n",
        "\n",
        "    special_tokens = torch.tensor([pad_token_id, eos_token_id, bos_token_id], device=device)\n",
        "    mask_candidates = ~torch.isin(input_ids, special_tokens)\n",
        "\n",
        "    # Randomly select tokens to mask based on mask_probability\n",
        "    rand = torch.rand(input_ids.shape, device=device)\n",
        "    mask = (rand < mask_probability) & mask_candidates\n",
        "\n",
        "    # Apply mask token to selected positions\n",
        "    masked_input_ids[mask] = mask_token_id\n",
        "\n",
        "    return masked_input_ids, mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v9B7yzcOjAsk",
        "outputId": "e8404918-1cca-48a1-a7aa-f4e5bdf050dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting improved_training.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile improved_training.py\n",
        "\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "import anndata\n",
        "import scanpy as sc\n",
        "import os\n",
        "import numpy as np\n",
        "from typing import Dict, List, Optional, Tuple, Union\n",
        "import math\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from molgpt_loader import load_mol_gpt_model\n",
        "\n",
        "from model_freezing import (\n",
        "    freeze_model_layers, print_trainable_parameters,\n",
        "    gradual_unfreezing_schedule\n",
        ")\n",
        "\n",
        "from scgpt_smiles_model import (\n",
        "    GatedCrossAttentionBlock\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "def train_scgpt_smiles_gpt_improved(model, train_loader, val_loader, num_epochs, device,\n",
        "                           l2_reg=1e-5, learning_rate=1e-4, checkpoints_dir=\"/content/drive/MyDrive/Colab Notebooks/esm cell state/\",\n",
        "                           use_mlm=True, use_gradual_unfreezing=True, mlm_probability=0.15):\n",
        "    \"\"\"\n",
        "    Improved training function with support for MLM and gradual unfreezing\n",
        "\n",
        "    Args:\n",
        "        model: The SCGPTSmilesGPT model\n",
        "        train_loader: DataLoader for training data\n",
        "        val_loader: DataLoader for validation data\n",
        "        num_epochs: Number of training epochs\n",
        "        device: Device to train on (cuda/cpu)\n",
        "        l2_reg: L2 regularization strength\n",
        "        learning_rate: Learning rate\n",
        "        checkpoints_dir: Directory to save checkpoints\n",
        "        use_mlm: Whether to use masked language modeling\n",
        "        use_gradual_unfreezing: Whether to gradually unfreeze layers\n",
        "        mlm_probability: Probability for token masking in MLM\n",
        "    \"\"\"\n",
        "    os.makedirs(checkpoints_dir, exist_ok=True)\n",
        "\n",
        "    # Start with all layers frozen except the cross-attention, perceiver, and projection layers\n",
        "    if use_gradual_unfreezing:\n",
        "        freeze_model_layers(model, freeze_all=True,\n",
        "                           unfreeze_layers=['projection', 'cell_perceiver', 'cross_attn', 'lm_head'])\n",
        "        print(\"Initial trainable parameters:\")\n",
        "        print_trainable_parameters(model)\n",
        "\n",
        "    optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()),\n",
        "                                 lr=learning_rate, weight_decay=l2_reg)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    training_log = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Update which layers are trainable based on current epoch\n",
        "        if use_gradual_unfreezing:\n",
        "            gradual_unfreezing_schedule(model, epoch, num_epochs)\n",
        "            print(f\"\\nEpoch {epoch+1}/{num_epochs} - Trainable parameters:\")\n",
        "            print_model_structure_and_trainable_status(model)\n",
        "\n",
        "            # Recreate optimizer with updated trainable parameters\n",
        "            optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()),\n",
        "                                        lr=learning_rate, weight_decay=l2_reg)\n",
        "            scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs - epoch)\n",
        "\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        total_correct = 0\n",
        "        total_tokens = 0\n",
        "\n",
        "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
        "            cell_embeddings = batch[\"cell_embeddings\"].to(device)\n",
        "            smiles_tokens = batch[\"smiles_tokens\"].to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Alternate between MLM and autoregressive training if both are used\n",
        "            use_masking_this_batch = False # use_mlm and (epoch % 2 == 0)\n",
        "\n",
        "            outputs, loss = model(cell_embeddings, targets=smiles_tokens,\n",
        "                                 use_masking=use_masking_this_batch)\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            predicted_token_ids = torch.argmax(outputs, dim=-1)\n",
        "            mask = smiles_tokens != model.smilesgpt_tokenizer.pad_token_id\n",
        "            correct = (predicted_token_ids[mask] == smiles_tokens[mask]).sum().item()\n",
        "\n",
        "            total = mask.sum().item()\n",
        "            total_correct += correct\n",
        "            total_tokens += total\n",
        "\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        token_accuracy = total_correct / total_tokens * 100\n",
        "        perplexity = math.exp(avg_loss)\n",
        "\n",
        "        print(f\"\\nEpoch {epoch+1}/{num_epochs}, Avg Loss: {avg_loss:.4f}, Perplexity: {perplexity:.4f}\")\n",
        "        print(f\"Token Accuracy: {token_accuracy:.2f}%\")\n",
        "\n",
        "        val_loss, val_perplexity, val_accuracy = validate_model(model, val_loader, device)\n",
        "        print(f\"Validation Loss: {val_loss:.4f}, Perplexity: {val_perplexity:.4f}, Accuracy: {val_accuracy:.2f}%\")\n",
        "\n",
        "        training_log.append({\n",
        "            'epoch': epoch+1,\n",
        "            'train_loss': avg_loss,\n",
        "            'train_perplexity': perplexity,\n",
        "            'train_accuracy': token_accuracy,\n",
        "            'val_loss': val_loss,\n",
        "            'val_perplexity': val_perplexity,\n",
        "            'val_accuracy': val_accuracy\n",
        "        })\n",
        "\n",
        "        # # Every 5 epochs, generate some samples to check quality\n",
        "        # if epoch % 5 == 0 or epoch == num_epochs - 1:\n",
        "        #     print(\"\\nGenerating sample SMILES strings:\")\n",
        "        #     # Get a few random samples from the validation set\n",
        "        #     sample_indices = np.random.choice(len(val_loader.dataset),\n",
        "        #                                      min(3, len(val_loader.dataset)),\n",
        "        #                                      replace=False)\n",
        "        #     for idx in sample_indices:\n",
        "        #         sample = val_loader.dataset[idx]\n",
        "        #         cell_embedding = sample[\"cell_embedding\"].unsqueeze(0).to(device)\n",
        "        #         ground_truth = model.smilesgpt_tokenizer.decode(sample[\"smiles_tokens\"])\n",
        "\n",
        "        #         generated = model.generate_smiles(cell_embedding)\n",
        "\n",
        "        #         print(f\"Ground truth: {ground_truth}\")\n",
        "        #         print(f\"Generated: {generated}\")\n",
        "        #         print(f\"Valid molecule: {MoleculeMetrics.is_valid_smiles(generated)}\")\n",
        "        #         print(\"-\" * 40)\n",
        "\n",
        "        checkpoint_path = os.path.join(checkpoints_dir, f\"scgptsmilesgpt_epoch_{epoch+1}.pth\")\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            torch.save(model.state_dict(), checkpoint_path)\n",
        "            print(f\"Checkpoint saved at {checkpoint_path} (Validation loss improved)\")\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "    loss_df = pd.DataFrame(training_log)\n",
        "    loss_df.to_csv(os.path.join(checkpoints_dir, \"training_log.csv\"), index=False)\n",
        "\n",
        "    return loss_df\n",
        "\n",
        "\n",
        "\n",
        "def generate_and_evaluate_improved(model, test_data, device=None):\n",
        "    \"\"\"\n",
        "    Generate SMILES strings and evaluate them using comprehensive metrics\n",
        "\n",
        "    Args:\n",
        "        model: The trained model\n",
        "        test_data: Test dataset\n",
        "        device: Device to run inference on\n",
        "    \"\"\"\n",
        "    if device is None:\n",
        "        device = next(model.parameters()).device\n",
        "\n",
        "    model.eval()\n",
        "    results = []\n",
        "\n",
        "    for pair in tqdm(test_data, desc=\"Generating SMILES\"):\n",
        "        cell_embedding = pair.cell_embedding.to(device)\n",
        "        ground_truth = pair.smiles_string\n",
        "\n",
        "        generated_smiles = model.generate_smiles(cell_embedding)\n",
        "\n",
        "        # Evaluate using comprehensive metrics\n",
        "        result = MoleculeMetrics.evaluate_molecule_pair(ground_truth, generated_smiles)\n",
        "        results.append(result)\n",
        "\n",
        "    # Aggregate metrics\n",
        "    aggregate_metrics = MoleculeMetrics.aggregate_metrics(results)\n",
        "\n",
        "    print(\"\\nEvaluation Results:\")\n",
        "    for metric_name, value in aggregate_metrics.items():\n",
        "        print(f\"{metric_name}: {value:.4f}\")\n",
        "\n",
        "    return results, aggregate_metrics\n",
        "\n",
        "\n",
        "def print_model_structure_and_trainable_status(model):\n",
        "    print(\"\\n=== MODEL STRUCTURE AND TRAINABLE STATUS ===\")\n",
        "\n",
        "    # DETAILED TRANSFORMER LAYERS ANALYSIS\n",
        "    print(\"\\n--- GPT Transformer Layers ---\")\n",
        "    # Directly access the transformer layers for accurate count\n",
        "    num_layers = len(model.smilesgpt_model.transformer.h)\n",
        "    print(f\"Total GPT transformer layers: {num_layers}\")\n",
        "\n",
        "    for i, layer in enumerate(model.smilesgpt_model.transformer.h):\n",
        "        trainable = all(param.requires_grad for param in layer.parameters())\n",
        "        print(f\"GPT Layer {i}: Trainable = {trainable}\")\n",
        "\n",
        "    # CROSS-ATTENTION ANALYSIS\n",
        "    print(\"\\n--- Cross-Attention Blocks ---\")\n",
        "    cross_attn_count = 0\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, GatedCrossAttentionBlock):\n",
        "            cross_attn_count += 1\n",
        "            trainable = all(param.requires_grad for param in module.parameters())\n",
        "            print(f\"Cross-Attention Block {cross_attn_count}: Trainable = {trainable}\")\n",
        "\n",
        "            location = \"Unknown location\"\n",
        "            if 'layers' in name:\n",
        "                try:\n",
        "                    index = int(name.split('.')[1])\n",
        "                    location = f\"After layer index {index}\"\n",
        "                except:\n",
        "                    location = f\"In layers (exact position unclear): {name}\"\n",
        "            elif 'final_cross_attn' in name:\n",
        "                location = \"Before LM head\"\n",
        "\n",
        "            print(f\"  Location: {location}\")\n",
        "            print(f\"  Full path: {name}\")\n",
        "\n",
        "    # LM HEAD ANALYSIS\n",
        "    print(\"\\n--- LM Head ---\")\n",
        "    lm_head_trainable = all(param.requires_grad for param in model.smilesgpt_model.lm_head.parameters())\n",
        "    print(f\"LM Head: Trainable = {lm_head_trainable}\")\n",
        "\n",
        "    # PARAMETER SUMMARY\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print(f\"\\nTotal Parameters: {total_params:,}\")\n",
        "    print(f\"Trainable Parameters: {trainable_params:,} ({trainable_params/total_params:.2%})\")\n",
        "\n",
        "    # Print details about what's happening in gradual_unfreezing_schedule\n",
        "    print(\"\\n--- Unfreezing Target Details ---\")\n",
        "    num_gpt_layers = len(model.smilesgpt_model.transformer.h)\n",
        "    print(f\"Total GPT layers found: {num_gpt_layers}\")\n",
        "    print(f\"Layers targeted for unfreezing: {num_gpt_layers-2} and {num_gpt_layers-1}\")\n",
        "\n",
        "\n",
        "def validate_model(model, val_loader, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    total_correct = 0\n",
        "    total_tokens = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            cell_embeddings = batch[\"cell_embeddings\"].to(device)\n",
        "            smiles_tokens = batch[\"smiles_tokens\"].to(device)\n",
        "\n",
        "            outputs, loss = model(cell_embeddings, targets=smiles_tokens)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            predicted_token_ids = torch.argmax(outputs, dim=-1)\n",
        "            mask = smiles_tokens != model.smilesgpt_tokenizer.pad_token_id\n",
        "            correct = (predicted_token_ids[mask] == smiles_tokens[mask]).sum().item()\n",
        "\n",
        "            total = mask.sum().item()\n",
        "            total_correct += correct\n",
        "            total_tokens += total\n",
        "\n",
        "    avg_loss = total_loss / len(val_loader)\n",
        "    token_accuracy = total_correct / total_tokens * 100\n",
        "    perplexity = math.exp(avg_loss)\n",
        "\n",
        "    return avg_loss, perplexity, token_accuracy\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DTt4Z6EqfsQm",
        "outputId": "c41e3840-5d29-4f66-e6db-443b15c7972d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting evaluation_visualization.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile evaluation_visualization.py\n",
        "\n",
        "from scgpt_smiles_model import (\n",
        "    GatedCrossAttentionBlock\n",
        ")\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import Draw\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.decomposition import PCA\n",
        "import os\n",
        "\n",
        "\n",
        "def visualize_training_metrics(log_df, save_path=None):\n",
        "    \"\"\"Visualize training and validation metrics over epochs.\"\"\"\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "    # Plot loss\n",
        "    axes[0, 0].plot(log_df['epoch'], log_df['train_loss'], label='Training')\n",
        "    axes[0, 0].plot(log_df['epoch'], log_df['val_loss'], label='Validation')\n",
        "    axes[0, 0].set_xlabel('Epoch')\n",
        "    axes[0, 0].set_ylabel('Loss')\n",
        "    axes[0, 0].set_title('Loss vs. Epoch')\n",
        "    axes[0, 0].legend()\n",
        "\n",
        "    # Plot perplexity\n",
        "    axes[0, 1].plot(log_df['epoch'], log_df['train_perplexity'], label='Training')\n",
        "    axes[0, 1].plot(log_df['epoch'], log_df['val_perplexity'], label='Validation')\n",
        "    axes[0, 1].set_xlabel('Epoch')\n",
        "    axes[0, 1].set_ylabel('Perplexity')\n",
        "    axes[0, 1].set_title('Perplexity vs. Epoch')\n",
        "    axes[0, 1].legend()\n",
        "\n",
        "    # Plot accuracy\n",
        "    axes[1, 0].plot(log_df['epoch'], log_df['train_accuracy'], label='Training')\n",
        "    axes[1, 0].plot(log_df['epoch'], log_df['val_accuracy'], label='Validation')\n",
        "    axes[1, 0].set_xlabel('Epoch')\n",
        "    axes[1, 0].set_ylabel('Token Accuracy (%)')\n",
        "    axes[1, 0].set_title('Token Accuracy vs. Epoch')\n",
        "    axes[1, 0].legend()\n",
        "\n",
        "    # Plot learning rate if available\n",
        "    if 'learning_rate' in log_df.columns:\n",
        "        axes[1, 1].plot(log_df['epoch'], log_df['learning_rate'])\n",
        "        axes[1, 1].set_xlabel('Epoch')\n",
        "        axes[1, 1].set_ylabel('Learning Rate')\n",
        "        axes[1, 1].set_title('Learning Rate Schedule')\n",
        "    else:\n",
        "        axes[1, 1].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    if save_path:\n",
        "        plt.savefig(save_path)\n",
        "        plt.close()\n",
        "    else:\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "def visualize_molecular_properties(results_df, save_dir=None):\n",
        "    \"\"\"Visualize distributions of molecular properties.\"\"\"\n",
        "    # Filter to valid molecules only\n",
        "    valid_df = results_df[results_df['gen_valid'] == 1].copy()\n",
        "\n",
        "    if len(valid_df) == 0:\n",
        "        print(\"No valid molecules to visualize properties\")\n",
        "        return\n",
        "\n",
        "    # Create a figure with subplots\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "\n",
        "    # Plot molecular weight distribution\n",
        "    sns.histplot(valid_df['mw'], kde=True, ax=axes[0, 0])\n",
        "    axes[0, 0].set_title('Molecular Weight Distribution')\n",
        "    axes[0, 0].set_xlabel('Molecular Weight (Da)')\n",
        "    axes[0, 0].axvline(500, color='red', linestyle='--')  # Lipinski threshold\n",
        "\n",
        "    # Plot logP distribution\n",
        "    sns.histplot(valid_df['logp'], kde=True, ax=axes[0, 1])\n",
        "    axes[0, 1].set_title('LogP Distribution')\n",
        "    axes[0, 1].set_xlabel('LogP')\n",
        "    axes[0, 1].axvline(5, color='red', linestyle='--')  # Lipinski threshold\n",
        "\n",
        "    # Plot QED distribution\n",
        "    sns.histplot(valid_df['qed'], kde=True, ax=axes[0, 2])\n",
        "    axes[0, 2].set_title('QED Distribution')\n",
        "    axes[0, 2].set_xlabel('QED (Drug-likeness)')\n",
        "\n",
        "    # Plot H-bond acceptors\n",
        "    sns.histplot(valid_df['hba'], kde=True, ax=axes[1, 0])\n",
        "    axes[1, 0].set_title('H-Bond Acceptors')\n",
        "    axes[1, 0].set_xlabel('Number of H-Bond Acceptors')\n",
        "    axes[1, 0].axvline(10, color='red', linestyle='--')  # Lipinski threshold\n",
        "\n",
        "    # Plot H-bond donors\n",
        "    sns.histplot(valid_df['hbd'], kde=True, ax=axes[1, 1])\n",
        "    axes[1, 1].set_title('H-Bond Donors')\n",
        "    axes[1, 1].set_xlabel('Number of H-Bond Donors')\n",
        "    axes[1, 1].axvline(5, color='red', linestyle='--')  # Lipinski threshold\n",
        "\n",
        "    # Plot Lipinski violations\n",
        "    sns.countplot(x='lipinski_violations', data=valid_df, ax=axes[1, 2])\n",
        "    axes[1, 2].set_title('Lipinski Rule Violations')\n",
        "    axes[1, 2].set_xlabel('Number of Violations')\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    if save_dir:\n",
        "        os.makedirs(save_dir, exist_ok=True)\n",
        "        plt.savefig(os.path.join(save_dir, 'molecular_properties.png'))\n",
        "        plt.close()\n",
        "    else:\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "def visualize_molecule_similarity(results_df, save_dir=None, num_examples=5):\n",
        "    \"\"\"Visualize ground truth and generated molecule pairs with similarity scores.\"\"\"\n",
        "    # Filter to valid molecules only\n",
        "    valid_pairs = results_df[(results_df['gt_valid'] == 1) & (results_df['gen_valid'] == 1)].copy()\n",
        "\n",
        "    if len(valid_pairs) == 0:\n",
        "        print(\"No valid molecule pairs to visualize\")\n",
        "        return\n",
        "\n",
        "    # Sort by similarity\n",
        "    valid_pairs = valid_pairs.sort_values('tanimoto_similarity', ascending=False)\n",
        "\n",
        "    # Get molecules for top matches\n",
        "    top_pairs = valid_pairs.head(num_examples)\n",
        "\n",
        "    # Create molecules\n",
        "    gt_mols = [Chem.MolFromSmiles(smiles) for smiles in top_pairs['ground_truth']]\n",
        "    gen_mols = [Chem.MolFromSmiles(smiles) for smiles in top_pairs['generated']]\n",
        "\n",
        "    # Create figure to display pairs\n",
        "    fig, axes = plt.subplots(num_examples, 2, figsize=(10, 4 * num_examples))\n",
        "\n",
        "    for i in range(num_examples):\n",
        "        if i < len(top_pairs):\n",
        "            similarity = top_pairs.iloc[i]['tanimoto_similarity']\n",
        "\n",
        "            if num_examples == 1:\n",
        "                ax1, ax2 = axes\n",
        "            else:\n",
        "                ax1, ax2 = axes[i]\n",
        "\n",
        "            # Draw molecules\n",
        "            img1 = Draw.MolToImage(gt_mols[i], size=(300, 300))\n",
        "            img2 = Draw.MolToImage(gen_mols[i], size=(300, 300))\n",
        "\n",
        "            ax1.imshow(img1)\n",
        "            ax1.set_title(f\"Ground Truth\")\n",
        "            ax1.axis('off')\n",
        "\n",
        "            ax2.imshow(img2)\n",
        "            ax2.set_title(f\"Generated (Similarity: {similarity:.2f})\")\n",
        "            ax2.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    if save_dir:\n",
        "        os.makedirs(save_dir, exist_ok=True)\n",
        "        plt.savefig(os.path.join(save_dir, 'molecule_similarity.png'))\n",
        "        plt.close()\n",
        "    else:\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "def visualize_fingerprint_space(results_df, save_dir=None, max_molecules=100):\n",
        "    \"\"\"Visualize the distribution of ground truth and generated molecules in fingerprint space.\"\"\"\n",
        "    # Filter to valid molecules only\n",
        "    valid_pairs = results_df[(results_df['gt_valid'] == 1) & (results_df['gen_valid'] == 1)].copy()\n",
        "\n",
        "    if len(valid_pairs) == 0:\n",
        "        print(\"No valid molecule pairs to visualize\")\n",
        "        return\n",
        "\n",
        "    # Limit the number of molecules to avoid overcrowding\n",
        "    if len(valid_pairs) > max_molecules:\n",
        "        valid_pairs = valid_pairs.sample(max_molecules, random_state=42)\n",
        "\n",
        "    # Generate fingerprints\n",
        "    from rdkit import Chem\n",
        "    from rdkit.Chem import AllChem\n",
        "\n",
        "    gt_smiles = valid_pairs['ground_truth'].tolist()\n",
        "    gen_smiles = valid_pairs['generated'].tolist()\n",
        "\n",
        "    gt_fps = []\n",
        "    gen_fps = []\n",
        "\n",
        "    for gt, gen in zip(gt_smiles, gen_smiles):\n",
        "        gt_mol = Chem.MolFromSmiles(gt)\n",
        "        gen_mol = Chem.MolFromSmiles(gen)\n",
        "\n",
        "        gt_fp = AllChem.GetMorganFingerprintAsBitVect(gt_mol, 2, nBits=2048)\n",
        "        gen_fp = AllChem.GetMorganFingerprintAsBitVect(gen_mol, 2, nBits=2048)\n",
        "\n",
        "        # Convert fingerprints to numpy arrays\n",
        "        gt_array = np.zeros((1,))\n",
        "        gen_array = np.zeros((1,))\n",
        "        AllChem.DataStructs.ConvertToNumpyArray(gt_fp, gt_array)\n",
        "        AllChem.DataStructs.ConvertToNumpyArray(gen_fp, gen_array)\n",
        "\n",
        "        gt_fps.append(gt_array)\n",
        "        gen_fps.append(gen_array)\n",
        "\n",
        "    # Combine fingerprints for dimensionality reduction\n",
        "    all_fps = np.vstack(gt_fps + gen_fps)\n",
        "\n",
        "    # Use PCA for initial dimensionality reduction to avoid t-SNE on high-dim data\n",
        "    pca = PCA(n_components=50)\n",
        "    all_fps_pca = pca.fit_transform(all_fps)\n",
        "\n",
        "    # Apply t-SNE for visualization\n",
        "    tsne = TSNE(n_components=2, random_state=42)\n",
        "    all_fps_tsne = tsne.fit_transform(all_fps_pca)\n",
        "\n",
        "    # Split back to ground truth and generated\n",
        "    n = len(gt_fps)\n",
        "    gt_tsne = all_fps_tsne[:n]\n",
        "    gen_tsne = all_fps_tsne[n:]\n",
        "\n",
        "    # Plot\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    plt.scatter(gt_tsne[:, 0], gt_tsne[:, 1], label='Ground Truth', alpha=0.7)\n",
        "    plt.scatter(gen_tsne[:, 0], gen_tsne[:, 1], label='Generated', alpha=0.7)\n",
        "\n",
        "    # Connect corresponding molecules with lines\n",
        "    for i in range(len(gt_tsne)):\n",
        "        plt.plot([gt_tsne[i, 0], gen_tsne[i, 0]], [gt_tsne[i, 1], gen_tsne[i, 1]], 'k-', alpha=0.2)\n",
        "\n",
        "    plt.legend()\n",
        "    plt.title('t-SNE Visualization of Molecular Fingerprints')\n",
        "\n",
        "    if save_dir:\n",
        "        os.makedirs(save_dir, exist_ok=True)\n",
        "        plt.savefig(os.path.join(save_dir, 'fingerprint_space.png'))\n",
        "        plt.close()\n",
        "    else:\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xOjyl70YeO0J",
        "outputId": "af93ee34-14f5-4765-8e3a-8f9400d74524"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting model_freezing.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile model_freezing.py\n",
        "\n",
        "\n",
        "from scgpt_smiles_model import (\n",
        "    GatedCrossAttentionBlock\n",
        ")\n",
        "def freeze_model_layers(model, freeze_all=True, unfreeze_layers=None):\n",
        "    \"\"\"\n",
        "    Freeze or unfreeze specific layers of the model\n",
        "\n",
        "    Args:\n",
        "        model: The neural network model\n",
        "        freeze_all: Whether to freeze all parameters initially\n",
        "        unfreeze_layers: List of layer names to unfreeze\n",
        "    \"\"\"\n",
        "    # First freeze or unfreeze all parameters\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = not freeze_all\n",
        "\n",
        "    if unfreeze_layers:\n",
        "        # Unfreeze specific layers\n",
        "        for name, param in model.named_parameters():\n",
        "            if any(layer_name in name for layer_name in unfreeze_layers):\n",
        "                param.requires_grad = True\n",
        "\n",
        "def print_trainable_parameters(model):\n",
        "    \"\"\"\n",
        "    Print the number and percentage of trainable parameters\n",
        "    \"\"\"\n",
        "    trainable_params = 0\n",
        "    all_params = 0\n",
        "\n",
        "    for name, param in model.named_parameters():\n",
        "        all_params += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "\n",
        "    print(f\"Trainable parameters: {trainable_params:,} ({trainable_params / all_params:.1%})\")\n",
        "    return trainable_params, all_params\n",
        "\n",
        "# def gradual_unfreezing_schedule(model, epoch, total_epochs):\n",
        "#     \"\"\"\n",
        "#     Implements a gradual unfreezing schedule based on current epoch\n",
        "\n",
        "#     Args:\n",
        "#         model: The neural network model\n",
        "#         epoch: Current epoch\n",
        "#         total_epochs: Total number of epochs\n",
        "#     \"\"\"\n",
        "#     # Start by freezing everything\n",
        "#     for param in model.parameters():\n",
        "#         param.requires_grad = False\n",
        "\n",
        "#     # Calculate how many GPT layers should be unfrozen\n",
        "#     num_gpt_layers = len(model.smilesgpt_model.transformer.h)\n",
        "\n",
        "#     # ORIGINAL GRADUAL UNFREEZING CODE (commented out)\n",
        "#     '''\n",
        "#     # Determine how many layers to unfreeze based on current epoch\n",
        "#     if epoch < total_epochs // 4:\n",
        "#         # First quarter: Only unfreeze the last GPT layer\n",
        "#         layers_to_unfreeze = 1\n",
        "#     elif epoch < total_epochs // 2:\n",
        "#         # Second quarter: Unfreeze the last 25% of GPT layers\n",
        "#         layers_to_unfreeze = max(1, num_gpt_layers // 4)\n",
        "#     elif epoch < 3 * total_epochs // 4:\n",
        "#         # Third quarter: Unfreeze the last 50% of GPT layers\n",
        "#         layers_to_unfreeze = max(1, num_gpt_layers // 2)\n",
        "#     else:\n",
        "#         # Last quarter: Unfreeze all layers\n",
        "#         layers_to_unfreeze = num_gpt_layers\n",
        "\n",
        "#     # Unfreeze the last N GPT layers\n",
        "#     for i in range(num_gpt_layers - layers_to_unfreeze, num_gpt_layers):\n",
        "#         for param in model.smilesgpt_model.transformer.h[i].parameters():\n",
        "#             param.requires_grad = True\n",
        "#     '''\n",
        "\n",
        "#     # NEW CODE: ONLY UNFREEZE LAST TWO LAYERS\n",
        "#     # Unfreeze only the last two transformer layers regardless of epoch\n",
        "#     for i in range(num_gpt_layers - 2, num_gpt_layers):\n",
        "#         for param in model.smilesgpt_model.transformer.h[i].parameters():\n",
        "#             param.requires_grad = True\n",
        "\n",
        "#     # Always unfreeze the projection, perceiver, and cross-attention blocks\n",
        "#     for param in model.projection.parameters():\n",
        "#         param.requires_grad = True\n",
        "\n",
        "#     for param in model.cell_perceiver.parameters():\n",
        "#         param.requires_grad = True\n",
        "\n",
        "#     # Unfreeze all cross-attention blocks\n",
        "#     for name, module in model.named_modules():\n",
        "#         if isinstance(module, GatedCrossAttentionBlock):\n",
        "#             for param in module.parameters():\n",
        "#                 param.requires_grad = True\n",
        "\n",
        "#     # Unfreeze LM head\n",
        "#     for param in model.smilesgpt_model.lm_head.parameters():\n",
        "#         param.requires_grad = True\n",
        "\n",
        "#     # Unfreeze final cross-attention if it exists\n",
        "#     if hasattr(model, 'final_cross_attn'):\n",
        "#         for param in model.final_cross_attn.parameters():\n",
        "#             param.requires_grad = True\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def gradual_unfreezing_schedule(model, epoch, total_epochs):\n",
        "    # Start by freezing everything\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    # Get number of GPT layers\n",
        "    num_gpt_layers = len(model.smilesgpt_model.transformer.h)\n",
        "\n",
        "    # Unfreeze ONLY the last two transformer layers\n",
        "    last_two_indices = [num_gpt_layers - 2, num_gpt_layers - 1]\n",
        "    for i in last_two_indices:\n",
        "        for param in model.smilesgpt_model.transformer.h[i].parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "    # Unfreeze critical components\n",
        "    for param in model.projection.parameters():\n",
        "        param.requires_grad = True\n",
        "\n",
        "    for param in model.cell_perceiver.parameters():\n",
        "        param.requires_grad = True\n",
        "\n",
        "    # Unfreeze all cross-attention blocks\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, GatedCrossAttentionBlock):\n",
        "            for param in module.parameters():\n",
        "                param.requires_grad = True\n",
        "\n",
        "    # Unfreeze LM head\n",
        "    for param in model.smilesgpt_model.lm_head.parameters():\n",
        "        param.requires_grad = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "soozG7EHrFvE",
        "outputId": "a2cba887-e275-4541-c272-e9f897945424"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting scgpt_smiles_main.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile scgpt_smiles_main.py\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import argparse\n",
        "import os\n",
        "import pandas as pd\n",
        "import scanpy as sc\n",
        "from torch.utils.data import DataLoader\n",
        "import sys\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "# Make sure the paths are available\n",
        "sys.path.append(os.path.dirname(os.path.abspath(__file__)))\n",
        "\n",
        "from scgpt_smiles_model import (\n",
        "    SCGPTSmilesGPT, SCGPTSmilesPair, CellSmilesDataset,\n",
        "    collate_batch, apply_masking, MoleculeMetrics\n",
        ")\n",
        "\n",
        "from model_freezing import (\n",
        "    freeze_model_layers, print_trainable_parameters,\n",
        "    gradual_unfreezing_schedule\n",
        ")\n",
        "\n",
        "from improved_training import (\n",
        "    train_scgpt_smiles_gpt_improved,\n",
        "    generate_and_evaluate_improved, validate_model, print_model_structure_and_trainable_status\n",
        ")\n",
        "\n",
        "from evaluation_visualization import (\n",
        "    visualize_training_metrics, visualize_molecular_properties,\n",
        "    visualize_molecule_similarity, visualize_fingerprint_space\n",
        ")\n",
        "\n",
        "from molgpt_loader import load_mol_gpt_model\n",
        "\n",
        "from scgpt_smiles_model import (\n",
        "    GatedCrossAttentionBlock\n",
        ")\n",
        "\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser(description=\"Train and evaluate scGPT to SMILES model with improvements\")\n",
        "    parser.add_argument(\"--processed_data_dir\", type=str, default=\"/content/drive/MyDrive/Colab Notebooks/esm cell state/scgpt_workdir\",\n",
        "                        help=\"Directory with processed data files\")\n",
        "    parser.add_argument(\"--batch_size\", type=int, default=16, help=\"Batch size for training\")\n",
        "    parser.add_argument(\"--max_length\", type=int, default=512, help=\"Maximum sequence length\")\n",
        "    parser.add_argument(\"--num_epochs\", type=int, default=20, help=\"Number of training epochs\")\n",
        "    parser.add_argument(\"--learning_rate\", type=float, default=5e-5, help=\"Learning rate\")\n",
        "    parser.add_argument(\"--l2_reg\", type=float, default=1e-5, help=\"L2 regularization\")\n",
        "    parser.add_argument(\"--cross_attn_every\", type=int, default=3, help=\"Cross attention frequency\")\n",
        "    parser.add_argument(\"--perceiver_depth\", type=int, default=2, help=\"Perceiver depth\")\n",
        "    parser.add_argument(\"--seed\", type=int, default=42, help=\"Random seed\")\n",
        "    parser.add_argument(\"--checkpoints_dir\", type=str, default=\"./checkpoints\", help=\"Checkpoints directory\")\n",
        "    parser.add_argument(\"--eval_only\", action=\"store_true\", help=\"Run evaluation only\")\n",
        "    parser.add_argument(\"--checkpoint_path\", type=str, help=\"Path to checkpoint for evaluation\")\n",
        "    parser.add_argument(\"--embedding_dim\", type=int, default=512, help=\"Dimension of scGPT embeddings\")\n",
        "\n",
        "    # New arguments for improvements\n",
        "    parser.add_argument(\"--use_mlm\", action=\"store_true\", help=\"Use Masked Language Modeling\")\n",
        "    parser.add_argument(\"--mlm_probability\", type=float, default=0.15, help=\"Probability for MLM masking\")\n",
        "    parser.add_argument(\"--use_gradual_unfreezing\", action=\"store_true\", help=\"Use gradual unfreezing strategy\")\n",
        "    parser.add_argument(\"--vis_dir\", type=str, default=\"./visualizations\", help=\"Directory for visualizations\")\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    torch.manual_seed(args.seed)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Load molGPT tokenizer\n",
        "    _, tokenizer = load_mol_gpt_model()\n",
        "\n",
        "    # Create the model\n",
        "    model = SCGPTSmilesGPT(\n",
        "        max_len=args.max_length,\n",
        "        cross_attn_every=args.cross_attn_every,\n",
        "        perceiver_depth=args.perceiver_depth,\n",
        "        working_dir=args.processed_data_dir,\n",
        "        embedding_dim=args.embedding_dim\n",
        "    ).to(device)\n",
        "\n",
        "    print(\"Initial model structure:\")\n",
        "    print_model_structure_and_trainable_status(model)\n",
        "\n",
        "    # Create directories\n",
        "    os.makedirs(args.checkpoints_dir, exist_ok=True)\n",
        "    os.makedirs(args.vis_dir, exist_ok=True)\n",
        "\n",
        "    # File paths for loading processed data\n",
        "    train_pairs_file = os.path.join(args.processed_data_dir, \"train_pairs.pkl\")\n",
        "    val_pairs_file = os.path.join(args.processed_data_dir, \"val_pairs.pkl\")\n",
        "    test_pairs_file = os.path.join(args.processed_data_dir, \"test_pairs.pkl\")\n",
        "\n",
        "    print(\"Loading pre-processed data...\")\n",
        "    with open(train_pairs_file, 'rb') as f:\n",
        "        train_pairs = pickle.load(f)\n",
        "    with open(val_pairs_file, 'rb') as f:\n",
        "        val_pairs = pickle.load(f)\n",
        "    with open(test_pairs_file, 'rb') as f:\n",
        "        test_pairs = pickle.load(f)\n",
        "\n",
        "    print(f\"Loaded {len(train_pairs)} training, {len(val_pairs)} validation, and {len(test_pairs)} test pairs\")\n",
        "\n",
        "    # Create datasets and dataloaders\n",
        "    train_dataset = CellSmilesDataset(train_pairs, tokenizer, args.max_length)\n",
        "    val_dataset = CellSmilesDataset(val_pairs, tokenizer, args.max_length)\n",
        "    test_dataset = CellSmilesDataset(test_pairs, tokenizer, args.max_length)\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=args.batch_size,\n",
        "        shuffle=True,\n",
        "        collate_fn=collate_batch,\n",
        "        num_workers=2\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=args.batch_size,\n",
        "        shuffle=False,\n",
        "        collate_fn=collate_batch,\n",
        "        num_workers=2\n",
        "    )\n",
        "\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=args.batch_size,\n",
        "        shuffle=False,\n",
        "        collate_fn=collate_batch,\n",
        "        num_workers=2\n",
        "    )\n",
        "\n",
        "    # For evaluation only mode\n",
        "    if args.eval_only and args.checkpoint_path:\n",
        "        print(f\"Loading checkpoint from {args.checkpoint_path}\")\n",
        "        model.load_state_dict(torch.load(args.checkpoint_path, map_location=device))\n",
        "\n",
        "        print(\"Generating and evaluating SMILES...\")\n",
        "        results, aggregate_metrics = generate_and_evaluate_improved(model, test_pairs, device)\n",
        "\n",
        "        # Convert results to DataFrame\n",
        "        results_df = pd.DataFrame(results)\n",
        "        results_path = os.path.join(args.checkpoints_dir, \"evaluation_results.csv\")\n",
        "        results_df.to_csv(results_path, index=False)\n",
        "        print(f\"Evaluation results saved to {results_path}\")\n",
        "\n",
        "        # Generate visualizations\n",
        "        visualize_molecular_properties(results_df, save_dir=args.vis_dir)\n",
        "        visualize_molecule_similarity(results_df, save_dir=args.vis_dir)\n",
        "        visualize_fingerprint_space(results_df, save_dir=args.vis_dir)\n",
        "\n",
        "        # Save metrics to a JSON file\n",
        "        import json\n",
        "        with open(os.path.join(args.checkpoints_dir, \"metrics.json\"), 'w') as f:\n",
        "            json.dump(aggregate_metrics, f, indent=4)\n",
        "\n",
        "    else:\n",
        "        print(f\"Training for {args.num_epochs} epochs\")\n",
        "        print(f\"Using MLM: {args.use_mlm}, MLM Probability: {args.mlm_probability}\")\n",
        "        print(f\"Using Gradual Unfreezing: {args.use_gradual_unfreezing}\")\n",
        "\n",
        "        log_df = train_scgpt_smiles_gpt_improved(\n",
        "            model=model,\n",
        "            train_loader=train_loader,\n",
        "            val_loader=val_loader,\n",
        "            num_epochs=args.num_epochs,\n",
        "            device=device,\n",
        "            l2_reg=args.l2_reg,\n",
        "            learning_rate=args.learning_rate,\n",
        "            checkpoints_dir=args.checkpoints_dir,\n",
        "            use_mlm=args.use_mlm,\n",
        "            use_gradual_unfreezing=args.use_gradual_unfreezing,\n",
        "            mlm_probability=args.mlm_probability\n",
        "        )\n",
        "\n",
        "        # Visualize training progress\n",
        "        visualize_training_metrics(log_df, save_path=os.path.join(args.vis_dir, \"training_curves.png\"))\n",
        "\n",
        "        # Find the best checkpoint\n",
        "        best_epoch = log_df.iloc[log_df['val_loss'].argmin()]['epoch']\n",
        "        best_checkpoint = os.path.join(args.checkpoints_dir, f\"scgptsmilesgpt_epoch_{int(best_epoch)}.pth\")\n",
        "\n",
        "        print(f\"Loading best checkpoint from epoch {int(best_epoch)}\")\n",
        "        model.load_state_dict(torch.load(best_checkpoint, map_location=device))\n",
        "\n",
        "        # # Final evaluation\n",
        "        # print(\"Performing final evaluation...\")\n",
        "        # results, aggregate_metrics = generate_and_evaluate_improved(model, test_pairs, device)\n",
        "\n",
        "        # # Convert results to DataFrame\n",
        "        # results_df = pd.DataFrame(results)\n",
        "        # results_path = os.path.join(args.checkpoints_dir, \"evaluation_results.csv\")\n",
        "        # results_df.to_csv(results_path, index=False)\n",
        "\n",
        "        # # Generate visualizations\n",
        "        # visualize_molecular_properties(results_df, save_dir=args.vis_dir)\n",
        "        # visualize_molecule_similarity(results_df, save_dir=args.vis_dir)\n",
        "        # visualize_fingerprint_space(results_df, save_dir=args.vis_dir)\n",
        "\n",
        "        # # Save metrics to a JSON file\n",
        "        # import json\n",
        "        # with open(os.path.join(args.checkpoints_dir, \"metrics.json\"), 'w') as f:\n",
        "        #     json.dump(aggregate_metrics, f, indent=4)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDaGLER5cd2_"
      },
      "source": [
        "## run lines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yQJo7ZuQC6_o",
        "outputId": "3659a6f1-0d22-40d6-cab1-a02ec7ac76b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "PgxDB_Iucm42"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "# os.chdir(\"/content/drive/MyDrive/Colab Notebooks/esm cell state/\")\n",
        "os.chdir(\"/content/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oMA4zRYMcvix",
        "outputId": "4eb72b91-0edf-4417-a718-7e40376487f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "drive  sample_data\n"
          ]
        }
      ],
      "source": [
        "!ls \"/content/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "9LougWwxft4l",
        "outputId": "79da988f-a70e-4bc2-8341-9ef6ea54f3a1"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"pd\",\n  \"rows\": 93,\n  \"fields\": [\n    {\n      \"column\": \"drug\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 93,\n        \"samples\": [\n          \"ETC-206\",\n          \"TAK-901\",\n          \"DTP3\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"SMILES\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 91,\n        \"samples\": [\n          \"CC1=C(C=C(C=C1)C(=O)NC)N2C(=CC(=C(C2=O)Br)OCC3=C(C=C(C=C3)F)F)C\",\n          \"CCS(=O)(=O)C1=CC=CC(=C1)C2=CC(=C(C3=C2C4=C(N3)N=CC(=C4)C)C)C(=O)NC5CCN(CC5)C\",\n          \"CCCCCCCC/C=C\\\\CCCCCCCC(=O)O\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-4e5ab4ec-65e4-46b5-8f36-369943316fb1\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>drug</th>\n",
              "      <th>SMILES</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Dabrafenib</td>\n",
              "      <td>CC(C)(C)C1=NC(=C(S1)C2=NC(=NC=C2)N)C3=C(C(=CC=...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Encorafenib</td>\n",
              "      <td>C[C@@H](CNC1=NC=CC(=N1)C2=CN(N=C2C3=C(C(=CC(=C...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Paclitaxel</td>\n",
              "      <td>CC1=C2[C@H](C(=O)[C@@]3([C@H](C[C@@H]4[C@]([C@...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9-ING-41</td>\n",
              "      <td>CN1C=C(C2=CC3=C(C=C21)OCO3)C4=C(C(=O)NC4=O)C5=...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Sapanisertib</td>\n",
              "      <td>CC(C)N1C2=NC=NC(=C2C(=N1)C3=CC4=C(C=C3)OC(=N4)N)N</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>88</th>\n",
              "      <td>Torkinib</td>\n",
              "      <td>CC(C)N1C2=NC=NC(=C2C(=N1)C3=CC4=C(N3)C=CC(=C4)O)N</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>89</th>\n",
              "      <td>AZD2858</td>\n",
              "      <td>CN1CCN(CC1)S(=O)(=O)C2=CC=C(C=C2)C3=CN=C(C(=N3...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>90</th>\n",
              "      <td>Bortezomib</td>\n",
              "      <td>B([C@H](CC(C)C)NC(=O)[C@H](CC1=CC=CC=C1)NC(=O)...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>91</th>\n",
              "      <td>LY-2584702 (tosylate salt)</td>\n",
              "      <td>CC1=CC=C(C=C1)S(=O)(=O)O.CN1C=C(N=C1C2CCN(CC2)...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>92</th>\n",
              "      <td>Lonafarnib</td>\n",
              "      <td>C1CN(CCC1CC(=O)N2CCC(CC2)[C@@H]3C4=C(CCC5=C3N=...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>93 rows × 2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4e5ab4ec-65e4-46b5-8f36-369943316fb1')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-4e5ab4ec-65e4-46b5-8f36-369943316fb1 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-4e5ab4ec-65e4-46b5-8f36-369943316fb1');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-9df6b120-53b9-4d1e-9e46-a6fd5cba7dab\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-9df6b120-53b9-4d1e-9e46-a6fd5cba7dab')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-9df6b120-53b9-4d1e-9e46-a6fd5cba7dab button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                          drug  \\\n",
              "0                   Dabrafenib   \n",
              "1                  Encorafenib   \n",
              "2                   Paclitaxel   \n",
              "3                     9-ING-41   \n",
              "4                 Sapanisertib   \n",
              "..                         ...   \n",
              "88                    Torkinib   \n",
              "89                     AZD2858   \n",
              "90                  Bortezomib   \n",
              "91  LY-2584702 (tosylate salt)   \n",
              "92                  Lonafarnib   \n",
              "\n",
              "                                               SMILES  \n",
              "0   CC(C)(C)C1=NC(=C(S1)C2=NC(=NC=C2)N)C3=C(C(=CC=...  \n",
              "1   C[C@@H](CNC1=NC=CC(=N1)C2=CN(N=C2C3=C(C(=CC(=C...  \n",
              "2   CC1=C2[C@H](C(=O)[C@@]3([C@H](C[C@@H]4[C@]([C@...  \n",
              "3   CN1C=C(C2=CC3=C(C=C21)OCO3)C4=C(C(=O)NC4=O)C5=...  \n",
              "4   CC(C)N1C2=NC=NC(=C2C(=N1)C3=CC4=C(C=C3)OC(=N4)N)N  \n",
              "..                                                ...  \n",
              "88  CC(C)N1C2=NC=NC(=C2C(=N1)C3=CC4=C(N3)C=CC(=C4)O)N  \n",
              "89  CN1CCN(CC1)S(=O)(=O)C2=CC=C(C=C2)C3=CN=C(C(=N3...  \n",
              "90  B([C@H](CC(C)C)NC(=O)[C@H](CC1=CC=CC=C1)NC(=O)...  \n",
              "91  CC1=CC=C(C=C1)S(=O)(=O)O.CN1C=C(N=C1C2CCN(CC2)...  \n",
              "92  C1CN(CCC1CC(=O)N2CCC(CC2)[C@@H]3C4=C(CCC5=C3N=...  \n",
              "\n",
              "[93 rows x 2 columns]"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/esm cell state/smiles_df.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j-l2LgLTcW8T",
        "outputId": "3c7b75cc-2dff-4c95-c41d-0b547f9678f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-03-19 01:15:31.986067: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1742346932.008114   32800 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1742346932.014846   32800 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "Using device: cuda\n",
            "Adding cross-attention after transformer layer 11/12\n",
            "Adding cross-attention after transformer layer 12/12\n",
            "Adding cross-attention before LM head\n",
            "Initial model structure:\n",
            "\n",
            "=== MODEL STRUCTURE AND TRAINABLE STATUS ===\n",
            "\n",
            "--- GPT Transformer Layers ---\n",
            "Total GPT transformer layers: 12\n",
            "GPT Layer 0: Trainable = True\n",
            "GPT Layer 1: Trainable = True\n",
            "GPT Layer 2: Trainable = True\n",
            "GPT Layer 3: Trainable = True\n",
            "GPT Layer 4: Trainable = True\n",
            "GPT Layer 5: Trainable = True\n",
            "GPT Layer 6: Trainable = True\n",
            "GPT Layer 7: Trainable = True\n",
            "GPT Layer 8: Trainable = True\n",
            "GPT Layer 9: Trainable = True\n",
            "GPT Layer 10: Trainable = True\n",
            "GPT Layer 11: Trainable = True\n",
            "\n",
            "--- Cross-Attention Blocks ---\n",
            "Cross-Attention Block 1: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.0\n",
            "Cross-Attention Block 2: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.1\n",
            "Cross-Attention Block 3: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.2\n",
            "Cross-Attention Block 4: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.3\n",
            "Cross-Attention Block 5: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.4\n",
            "Cross-Attention Block 6: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.5\n",
            "Cross-Attention Block 7: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.6\n",
            "Cross-Attention Block 8: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.7\n",
            "Cross-Attention Block 9: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.8\n",
            "Cross-Attention Block 10: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.9\n",
            "Cross-Attention Block 11: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.10\n",
            "Cross-Attention Block 12: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.11\n",
            "Cross-Attention Block 13: Trainable = True\n",
            "  Location: After layer index 11\n",
            "  Full path: layers.11\n",
            "Cross-Attention Block 14: Trainable = True\n",
            "  Location: After layer index 12\n",
            "  Full path: layers.12\n",
            "Cross-Attention Block 15: Trainable = True\n",
            "  Location: Before LM head\n",
            "  Full path: final_cross_attn\n",
            "\n",
            "--- LM Head ---\n",
            "LM Head: Trainable = True\n",
            "\n",
            "Total Parameters: 216,360,222\n",
            "Trainable Parameters: 216,360,222 (100.00%)\n",
            "\n",
            "--- Unfreezing Target Details ---\n",
            "Total GPT layers found: 12\n",
            "Layers targeted for unfreezing: 10 and 11\n",
            "Loading pre-processed data...\n",
            "Loaded 20619 training, 2955 validation, and 4244 test pairs\n",
            "Training for 30 epochs\n",
            "Using MLM: True, MLM Probability: 0.15\n",
            "Using Gradual Unfreezing: True\n",
            "Initial trainable parameters:\n",
            "Trainable parameters: 94,884,122 (43.9%)\n",
            "\n",
            "Epoch 1/30 - Trainable parameters:\n",
            "\n",
            "=== MODEL STRUCTURE AND TRAINABLE STATUS ===\n",
            "\n",
            "--- GPT Transformer Layers ---\n",
            "Total GPT transformer layers: 12\n",
            "GPT Layer 0: Trainable = False\n",
            "GPT Layer 1: Trainable = False\n",
            "GPT Layer 2: Trainable = False\n",
            "GPT Layer 3: Trainable = False\n",
            "GPT Layer 4: Trainable = False\n",
            "GPT Layer 5: Trainable = False\n",
            "GPT Layer 6: Trainable = False\n",
            "GPT Layer 7: Trainable = False\n",
            "GPT Layer 8: Trainable = False\n",
            "GPT Layer 9: Trainable = False\n",
            "GPT Layer 10: Trainable = True\n",
            "GPT Layer 11: Trainable = True\n",
            "\n",
            "--- Cross-Attention Blocks ---\n",
            "Cross-Attention Block 1: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.0\n",
            "Cross-Attention Block 2: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.1\n",
            "Cross-Attention Block 3: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.2\n",
            "Cross-Attention Block 4: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.3\n",
            "Cross-Attention Block 5: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.4\n",
            "Cross-Attention Block 6: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.5\n",
            "Cross-Attention Block 7: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.6\n",
            "Cross-Attention Block 8: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.7\n",
            "Cross-Attention Block 9: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.8\n",
            "Cross-Attention Block 10: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.9\n",
            "Cross-Attention Block 11: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.10\n",
            "Cross-Attention Block 12: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.11\n",
            "Cross-Attention Block 13: Trainable = True\n",
            "  Location: After layer index 11\n",
            "  Full path: layers.11\n",
            "Cross-Attention Block 14: Trainable = True\n",
            "  Location: After layer index 12\n",
            "  Full path: layers.12\n",
            "Cross-Attention Block 15: Trainable = True\n",
            "  Location: Before LM head\n",
            "  Full path: final_cross_attn\n",
            "\n",
            "--- LM Head ---\n",
            "LM Head: Trainable = True\n",
            "\n",
            "Total Parameters: 216,360,222\n",
            "Trainable Parameters: 144,693,534 (66.88%)\n",
            "\n",
            "--- Unfreezing Target Details ---\n",
            "Total GPT layers found: 12\n",
            "Layers targeted for unfreezing: 10 and 11\n",
            "Epoch 1/30: 100% 20619/20619 [53:48<00:00,  6.39it/s]\n",
            "\n",
            "Epoch 1/30, Avg Loss: 3.9910, Perplexity: 54.1100\n",
            "Token Accuracy: 8.03%\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Validation Loss: 8.0155, Perplexity: 3027.5058, Accuracy: 8.81%\n",
            "Checkpoint saved at ./checkpoints/scgptsmilesgpt_epoch_1.pth (Validation loss improved)\n",
            "\n",
            "Epoch 2/30 - Trainable parameters:\n",
            "\n",
            "=== MODEL STRUCTURE AND TRAINABLE STATUS ===\n",
            "\n",
            "--- GPT Transformer Layers ---\n",
            "Total GPT transformer layers: 12\n",
            "GPT Layer 0: Trainable = False\n",
            "GPT Layer 1: Trainable = False\n",
            "GPT Layer 2: Trainable = False\n",
            "GPT Layer 3: Trainable = False\n",
            "GPT Layer 4: Trainable = False\n",
            "GPT Layer 5: Trainable = False\n",
            "GPT Layer 6: Trainable = False\n",
            "GPT Layer 7: Trainable = False\n",
            "GPT Layer 8: Trainable = False\n",
            "GPT Layer 9: Trainable = False\n",
            "GPT Layer 10: Trainable = True\n",
            "GPT Layer 11: Trainable = True\n",
            "\n",
            "--- Cross-Attention Blocks ---\n",
            "Cross-Attention Block 1: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.0\n",
            "Cross-Attention Block 2: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.1\n",
            "Cross-Attention Block 3: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.2\n",
            "Cross-Attention Block 4: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.3\n",
            "Cross-Attention Block 5: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.4\n",
            "Cross-Attention Block 6: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.5\n",
            "Cross-Attention Block 7: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.6\n",
            "Cross-Attention Block 8: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.7\n",
            "Cross-Attention Block 9: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.8\n",
            "Cross-Attention Block 10: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.9\n",
            "Cross-Attention Block 11: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.10\n",
            "Cross-Attention Block 12: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.11\n",
            "Cross-Attention Block 13: Trainable = True\n",
            "  Location: After layer index 11\n",
            "  Full path: layers.11\n",
            "Cross-Attention Block 14: Trainable = True\n",
            "  Location: After layer index 12\n",
            "  Full path: layers.12\n",
            "Cross-Attention Block 15: Trainable = True\n",
            "  Location: Before LM head\n",
            "  Full path: final_cross_attn\n",
            "\n",
            "--- LM Head ---\n",
            "LM Head: Trainable = True\n",
            "\n",
            "Total Parameters: 216,360,222\n",
            "Trainable Parameters: 144,693,534 (66.88%)\n",
            "\n",
            "--- Unfreezing Target Details ---\n",
            "Total GPT layers found: 12\n",
            "Layers targeted for unfreezing: 10 and 11\n",
            "Epoch 2/30:   0% 0/20619 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Epoch 2/30: 100% 20619/20619 [53:49<00:00,  6.38it/s]\n",
            "\n",
            "Epoch 2/30, Avg Loss: 3.2761, Perplexity: 26.4734\n",
            "Token Accuracy: 13.85%\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Validation Loss: 10.2229, Perplexity: 27526.1780, Accuracy: 9.71%\n",
            "\n",
            "Epoch 3/30 - Trainable parameters:\n",
            "\n",
            "=== MODEL STRUCTURE AND TRAINABLE STATUS ===\n",
            "\n",
            "--- GPT Transformer Layers ---\n",
            "Total GPT transformer layers: 12\n",
            "GPT Layer 0: Trainable = False\n",
            "GPT Layer 1: Trainable = False\n",
            "GPT Layer 2: Trainable = False\n",
            "GPT Layer 3: Trainable = False\n",
            "GPT Layer 4: Trainable = False\n",
            "GPT Layer 5: Trainable = False\n",
            "GPT Layer 6: Trainable = False\n",
            "GPT Layer 7: Trainable = False\n",
            "GPT Layer 8: Trainable = False\n",
            "GPT Layer 9: Trainable = False\n",
            "GPT Layer 10: Trainable = True\n",
            "GPT Layer 11: Trainable = True\n",
            "\n",
            "--- Cross-Attention Blocks ---\n",
            "Cross-Attention Block 1: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.0\n",
            "Cross-Attention Block 2: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.1\n",
            "Cross-Attention Block 3: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.2\n",
            "Cross-Attention Block 4: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.3\n",
            "Cross-Attention Block 5: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.4\n",
            "Cross-Attention Block 6: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.5\n",
            "Cross-Attention Block 7: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.6\n",
            "Cross-Attention Block 8: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.7\n",
            "Cross-Attention Block 9: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.8\n",
            "Cross-Attention Block 10: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.9\n",
            "Cross-Attention Block 11: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.10\n",
            "Cross-Attention Block 12: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.11\n",
            "Cross-Attention Block 13: Trainable = True\n",
            "  Location: After layer index 11\n",
            "  Full path: layers.11\n",
            "Cross-Attention Block 14: Trainable = True\n",
            "  Location: After layer index 12\n",
            "  Full path: layers.12\n",
            "Cross-Attention Block 15: Trainable = True\n",
            "  Location: Before LM head\n",
            "  Full path: final_cross_attn\n",
            "\n",
            "--- LM Head ---\n",
            "LM Head: Trainable = True\n",
            "\n",
            "Total Parameters: 216,360,222\n",
            "Trainable Parameters: 144,693,534 (66.88%)\n",
            "\n",
            "--- Unfreezing Target Details ---\n",
            "Total GPT layers found: 12\n",
            "Layers targeted for unfreezing: 10 and 11\n",
            "Epoch 3/30:   0% 0/20619 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Epoch 3/30: 100% 20619/20619 [53:44<00:00,  6.40it/s]\n",
            "\n",
            "Epoch 3/30, Avg Loss: 2.8481, Perplexity: 17.2548\n",
            "Token Accuracy: 21.27%\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Validation Loss: 10.8786, Perplexity: 53027.9086, Accuracy: 11.48%\n",
            "\n",
            "Epoch 4/30 - Trainable parameters:\n",
            "\n",
            "=== MODEL STRUCTURE AND TRAINABLE STATUS ===\n",
            "\n",
            "--- GPT Transformer Layers ---\n",
            "Total GPT transformer layers: 12\n",
            "GPT Layer 0: Trainable = False\n",
            "GPT Layer 1: Trainable = False\n",
            "GPT Layer 2: Trainable = False\n",
            "GPT Layer 3: Trainable = False\n",
            "GPT Layer 4: Trainable = False\n",
            "GPT Layer 5: Trainable = False\n",
            "GPT Layer 6: Trainable = False\n",
            "GPT Layer 7: Trainable = False\n",
            "GPT Layer 8: Trainable = False\n",
            "GPT Layer 9: Trainable = False\n",
            "GPT Layer 10: Trainable = True\n",
            "GPT Layer 11: Trainable = True\n",
            "\n",
            "--- Cross-Attention Blocks ---\n",
            "Cross-Attention Block 1: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.0\n",
            "Cross-Attention Block 2: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.1\n",
            "Cross-Attention Block 3: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.2\n",
            "Cross-Attention Block 4: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.3\n",
            "Cross-Attention Block 5: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.4\n",
            "Cross-Attention Block 6: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.5\n",
            "Cross-Attention Block 7: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.6\n",
            "Cross-Attention Block 8: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.7\n",
            "Cross-Attention Block 9: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.8\n",
            "Cross-Attention Block 10: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.9\n",
            "Cross-Attention Block 11: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.10\n",
            "Cross-Attention Block 12: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.11\n",
            "Cross-Attention Block 13: Trainable = True\n",
            "  Location: After layer index 11\n",
            "  Full path: layers.11\n",
            "Cross-Attention Block 14: Trainable = True\n",
            "  Location: After layer index 12\n",
            "  Full path: layers.12\n",
            "Cross-Attention Block 15: Trainable = True\n",
            "  Location: Before LM head\n",
            "  Full path: final_cross_attn\n",
            "\n",
            "--- LM Head ---\n",
            "LM Head: Trainable = True\n",
            "\n",
            "Total Parameters: 216,360,222\n",
            "Trainable Parameters: 144,693,534 (66.88%)\n",
            "\n",
            "--- Unfreezing Target Details ---\n",
            "Total GPT layers found: 12\n",
            "Layers targeted for unfreezing: 10 and 11\n",
            "Epoch 4/30:   0% 0/20619 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Epoch 4/30: 100% 20619/20619 [53:59<00:00,  6.36it/s]\n",
            "\n",
            "Epoch 4/30, Avg Loss: 2.5314, Perplexity: 12.5709\n",
            "Token Accuracy: 30.32%\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Validation Loss: 10.6951, Perplexity: 44140.9171, Accuracy: 13.08%\n",
            "\n",
            "Epoch 5/30 - Trainable parameters:\n",
            "\n",
            "=== MODEL STRUCTURE AND TRAINABLE STATUS ===\n",
            "\n",
            "--- GPT Transformer Layers ---\n",
            "Total GPT transformer layers: 12\n",
            "GPT Layer 0: Trainable = False\n",
            "GPT Layer 1: Trainable = False\n",
            "GPT Layer 2: Trainable = False\n",
            "GPT Layer 3: Trainable = False\n",
            "GPT Layer 4: Trainable = False\n",
            "GPT Layer 5: Trainable = False\n",
            "GPT Layer 6: Trainable = False\n",
            "GPT Layer 7: Trainable = False\n",
            "GPT Layer 8: Trainable = False\n",
            "GPT Layer 9: Trainable = False\n",
            "GPT Layer 10: Trainable = True\n",
            "GPT Layer 11: Trainable = True\n",
            "\n",
            "--- Cross-Attention Blocks ---\n",
            "Cross-Attention Block 1: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.0\n",
            "Cross-Attention Block 2: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.1\n",
            "Cross-Attention Block 3: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.2\n",
            "Cross-Attention Block 4: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.3\n",
            "Cross-Attention Block 5: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.4\n",
            "Cross-Attention Block 6: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.5\n",
            "Cross-Attention Block 7: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.6\n",
            "Cross-Attention Block 8: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.7\n",
            "Cross-Attention Block 9: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.8\n",
            "Cross-Attention Block 10: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.9\n",
            "Cross-Attention Block 11: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.10\n",
            "Cross-Attention Block 12: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.11\n",
            "Cross-Attention Block 13: Trainable = True\n",
            "  Location: After layer index 11\n",
            "  Full path: layers.11\n",
            "Cross-Attention Block 14: Trainable = True\n",
            "  Location: After layer index 12\n",
            "  Full path: layers.12\n",
            "Cross-Attention Block 15: Trainable = True\n",
            "  Location: Before LM head\n",
            "  Full path: final_cross_attn\n",
            "\n",
            "--- LM Head ---\n",
            "LM Head: Trainable = True\n",
            "\n",
            "Total Parameters: 216,360,222\n",
            "Trainable Parameters: 144,693,534 (66.88%)\n",
            "\n",
            "--- Unfreezing Target Details ---\n",
            "Total GPT layers found: 12\n",
            "Layers targeted for unfreezing: 10 and 11\n",
            "Epoch 5/30:   0% 0/20619 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Epoch 5/30: 100% 20619/20619 [54:07<00:00,  6.35it/s]\n",
            "\n",
            "Epoch 5/30, Avg Loss: 2.3611, Perplexity: 10.6027\n",
            "Token Accuracy: 34.91%\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Validation Loss: 10.8447, Perplexity: 51261.9261, Accuracy: 15.91%\n",
            "\n",
            "Epoch 6/30 - Trainable parameters:\n",
            "\n",
            "=== MODEL STRUCTURE AND TRAINABLE STATUS ===\n",
            "\n",
            "--- GPT Transformer Layers ---\n",
            "Total GPT transformer layers: 12\n",
            "GPT Layer 0: Trainable = False\n",
            "GPT Layer 1: Trainable = False\n",
            "GPT Layer 2: Trainable = False\n",
            "GPT Layer 3: Trainable = False\n",
            "GPT Layer 4: Trainable = False\n",
            "GPT Layer 5: Trainable = False\n",
            "GPT Layer 6: Trainable = False\n",
            "GPT Layer 7: Trainable = False\n",
            "GPT Layer 8: Trainable = False\n",
            "GPT Layer 9: Trainable = False\n",
            "GPT Layer 10: Trainable = True\n",
            "GPT Layer 11: Trainable = True\n",
            "\n",
            "--- Cross-Attention Blocks ---\n",
            "Cross-Attention Block 1: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.0\n",
            "Cross-Attention Block 2: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.1\n",
            "Cross-Attention Block 3: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.2\n",
            "Cross-Attention Block 4: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.3\n",
            "Cross-Attention Block 5: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.4\n",
            "Cross-Attention Block 6: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.5\n",
            "Cross-Attention Block 7: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.6\n",
            "Cross-Attention Block 8: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.7\n",
            "Cross-Attention Block 9: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.8\n",
            "Cross-Attention Block 10: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.9\n",
            "Cross-Attention Block 11: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.10\n",
            "Cross-Attention Block 12: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.11\n",
            "Cross-Attention Block 13: Trainable = True\n",
            "  Location: After layer index 11\n",
            "  Full path: layers.11\n",
            "Cross-Attention Block 14: Trainable = True\n",
            "  Location: After layer index 12\n",
            "  Full path: layers.12\n",
            "Cross-Attention Block 15: Trainable = True\n",
            "  Location: Before LM head\n",
            "  Full path: final_cross_attn\n",
            "\n",
            "--- LM Head ---\n",
            "LM Head: Trainable = True\n",
            "\n",
            "Total Parameters: 216,360,222\n",
            "Trainable Parameters: 144,693,534 (66.88%)\n",
            "\n",
            "--- Unfreezing Target Details ---\n",
            "Total GPT layers found: 12\n",
            "Layers targeted for unfreezing: 10 and 11\n",
            "Epoch 6/30:   0% 0/20619 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Epoch 6/30: 100% 20619/20619 [53:51<00:00,  6.38it/s]\n",
            "\n",
            "Epoch 6/30, Avg Loss: 2.2233, Perplexity: 9.2378\n",
            "Token Accuracy: 39.34%\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Validation Loss: 11.6514, Perplexity: 114851.0606, Accuracy: 17.86%\n",
            "\n",
            "Epoch 7/30 - Trainable parameters:\n",
            "\n",
            "=== MODEL STRUCTURE AND TRAINABLE STATUS ===\n",
            "\n",
            "--- GPT Transformer Layers ---\n",
            "Total GPT transformer layers: 12\n",
            "GPT Layer 0: Trainable = False\n",
            "GPT Layer 1: Trainable = False\n",
            "GPT Layer 2: Trainable = False\n",
            "GPT Layer 3: Trainable = False\n",
            "GPT Layer 4: Trainable = False\n",
            "GPT Layer 5: Trainable = False\n",
            "GPT Layer 6: Trainable = False\n",
            "GPT Layer 7: Trainable = False\n",
            "GPT Layer 8: Trainable = False\n",
            "GPT Layer 9: Trainable = False\n",
            "GPT Layer 10: Trainable = True\n",
            "GPT Layer 11: Trainable = True\n",
            "\n",
            "--- Cross-Attention Blocks ---\n",
            "Cross-Attention Block 1: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.0\n",
            "Cross-Attention Block 2: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.1\n",
            "Cross-Attention Block 3: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.2\n",
            "Cross-Attention Block 4: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.3\n",
            "Cross-Attention Block 5: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.4\n",
            "Cross-Attention Block 6: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.5\n",
            "Cross-Attention Block 7: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.6\n",
            "Cross-Attention Block 8: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.7\n",
            "Cross-Attention Block 9: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.8\n",
            "Cross-Attention Block 10: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.9\n",
            "Cross-Attention Block 11: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.10\n",
            "Cross-Attention Block 12: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.11\n",
            "Cross-Attention Block 13: Trainable = True\n",
            "  Location: After layer index 11\n",
            "  Full path: layers.11\n",
            "Cross-Attention Block 14: Trainable = True\n",
            "  Location: After layer index 12\n",
            "  Full path: layers.12\n",
            "Cross-Attention Block 15: Trainable = True\n",
            "  Location: Before LM head\n",
            "  Full path: final_cross_attn\n",
            "\n",
            "--- LM Head ---\n",
            "LM Head: Trainable = True\n",
            "\n",
            "Total Parameters: 216,360,222\n",
            "Trainable Parameters: 144,693,534 (66.88%)\n",
            "\n",
            "--- Unfreezing Target Details ---\n",
            "Total GPT layers found: 12\n",
            "Layers targeted for unfreezing: 10 and 11\n",
            "Epoch 7/30:   0% 0/20619 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Epoch 7/30: 100% 20619/20619 [54:02<00:00,  6.36it/s]\n",
            "\n",
            "Epoch 7/30, Avg Loss: 2.1653, Perplexity: 8.7172\n",
            "Token Accuracy: 42.37%\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Validation Loss: 12.1340, Perplexity: 186088.7074, Accuracy: 17.93%\n",
            "\n",
            "Epoch 8/30 - Trainable parameters:\n",
            "\n",
            "=== MODEL STRUCTURE AND TRAINABLE STATUS ===\n",
            "\n",
            "--- GPT Transformer Layers ---\n",
            "Total GPT transformer layers: 12\n",
            "GPT Layer 0: Trainable = False\n",
            "GPT Layer 1: Trainable = False\n",
            "GPT Layer 2: Trainable = False\n",
            "GPT Layer 3: Trainable = False\n",
            "GPT Layer 4: Trainable = False\n",
            "GPT Layer 5: Trainable = False\n",
            "GPT Layer 6: Trainable = False\n",
            "GPT Layer 7: Trainable = False\n",
            "GPT Layer 8: Trainable = False\n",
            "GPT Layer 9: Trainable = False\n",
            "GPT Layer 10: Trainable = True\n",
            "GPT Layer 11: Trainable = True\n",
            "\n",
            "--- Cross-Attention Blocks ---\n",
            "Cross-Attention Block 1: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.0\n",
            "Cross-Attention Block 2: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.1\n",
            "Cross-Attention Block 3: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.2\n",
            "Cross-Attention Block 4: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.3\n",
            "Cross-Attention Block 5: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.4\n",
            "Cross-Attention Block 6: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.5\n",
            "Cross-Attention Block 7: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.6\n",
            "Cross-Attention Block 8: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.7\n",
            "Cross-Attention Block 9: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.8\n",
            "Cross-Attention Block 10: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.9\n",
            "Cross-Attention Block 11: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.10\n",
            "Cross-Attention Block 12: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.11\n",
            "Cross-Attention Block 13: Trainable = True\n",
            "  Location: After layer index 11\n",
            "  Full path: layers.11\n",
            "Cross-Attention Block 14: Trainable = True\n",
            "  Location: After layer index 12\n",
            "  Full path: layers.12\n",
            "Cross-Attention Block 15: Trainable = True\n",
            "  Location: Before LM head\n",
            "  Full path: final_cross_attn\n",
            "\n",
            "--- LM Head ---\n",
            "LM Head: Trainable = True\n",
            "\n",
            "Total Parameters: 216,360,222\n",
            "Trainable Parameters: 144,693,534 (66.88%)\n",
            "\n",
            "--- Unfreezing Target Details ---\n",
            "Total GPT layers found: 12\n",
            "Layers targeted for unfreezing: 10 and 11\n",
            "Epoch 8/30:   0% 0/20619 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Epoch 8/30: 100% 20619/20619 [53:46<00:00,  6.39it/s]\n",
            "\n",
            "Epoch 8/30, Avg Loss: 2.0791, Perplexity: 7.9975\n",
            "Token Accuracy: 45.25%\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Validation Loss: 11.3315, Perplexity: 83409.7246, Accuracy: 16.78%\n",
            "\n",
            "Epoch 9/30 - Trainable parameters:\n",
            "\n",
            "=== MODEL STRUCTURE AND TRAINABLE STATUS ===\n",
            "\n",
            "--- GPT Transformer Layers ---\n",
            "Total GPT transformer layers: 12\n",
            "GPT Layer 0: Trainable = False\n",
            "GPT Layer 1: Trainable = False\n",
            "GPT Layer 2: Trainable = False\n",
            "GPT Layer 3: Trainable = False\n",
            "GPT Layer 4: Trainable = False\n",
            "GPT Layer 5: Trainable = False\n",
            "GPT Layer 6: Trainable = False\n",
            "GPT Layer 7: Trainable = False\n",
            "GPT Layer 8: Trainable = False\n",
            "GPT Layer 9: Trainable = False\n",
            "GPT Layer 10: Trainable = True\n",
            "GPT Layer 11: Trainable = True\n",
            "\n",
            "--- Cross-Attention Blocks ---\n",
            "Cross-Attention Block 1: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.0\n",
            "Cross-Attention Block 2: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.1\n",
            "Cross-Attention Block 3: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.2\n",
            "Cross-Attention Block 4: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.3\n",
            "Cross-Attention Block 5: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.4\n",
            "Cross-Attention Block 6: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.5\n",
            "Cross-Attention Block 7: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.6\n",
            "Cross-Attention Block 8: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.7\n",
            "Cross-Attention Block 9: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.8\n",
            "Cross-Attention Block 10: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.9\n",
            "Cross-Attention Block 11: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.10\n",
            "Cross-Attention Block 12: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.11\n",
            "Cross-Attention Block 13: Trainable = True\n",
            "  Location: After layer index 11\n",
            "  Full path: layers.11\n",
            "Cross-Attention Block 14: Trainable = True\n",
            "  Location: After layer index 12\n",
            "  Full path: layers.12\n",
            "Cross-Attention Block 15: Trainable = True\n",
            "  Location: Before LM head\n",
            "  Full path: final_cross_attn\n",
            "\n",
            "--- LM Head ---\n",
            "LM Head: Trainable = True\n",
            "\n",
            "Total Parameters: 216,360,222\n",
            "Trainable Parameters: 144,693,534 (66.88%)\n",
            "\n",
            "--- Unfreezing Target Details ---\n",
            "Total GPT layers found: 12\n",
            "Layers targeted for unfreezing: 10 and 11\n",
            "Epoch 9/30:   0% 0/20619 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Epoch 9/30: 100% 20619/20619 [53:53<00:00,  6.38it/s]\n",
            "\n",
            "Epoch 9/30, Avg Loss: 2.0319, Perplexity: 7.6285\n",
            "Token Accuracy: 47.58%\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Validation Loss: 11.7122, Perplexity: 122054.9132, Accuracy: 18.86%\n",
            "\n",
            "Epoch 10/30 - Trainable parameters:\n",
            "\n",
            "=== MODEL STRUCTURE AND TRAINABLE STATUS ===\n",
            "\n",
            "--- GPT Transformer Layers ---\n",
            "Total GPT transformer layers: 12\n",
            "GPT Layer 0: Trainable = False\n",
            "GPT Layer 1: Trainable = False\n",
            "GPT Layer 2: Trainable = False\n",
            "GPT Layer 3: Trainable = False\n",
            "GPT Layer 4: Trainable = False\n",
            "GPT Layer 5: Trainable = False\n",
            "GPT Layer 6: Trainable = False\n",
            "GPT Layer 7: Trainable = False\n",
            "GPT Layer 8: Trainable = False\n",
            "GPT Layer 9: Trainable = False\n",
            "GPT Layer 10: Trainable = True\n",
            "GPT Layer 11: Trainable = True\n",
            "\n",
            "--- Cross-Attention Blocks ---\n",
            "Cross-Attention Block 1: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.0\n",
            "Cross-Attention Block 2: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.1\n",
            "Cross-Attention Block 3: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.2\n",
            "Cross-Attention Block 4: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.3\n",
            "Cross-Attention Block 5: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.4\n",
            "Cross-Attention Block 6: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.5\n",
            "Cross-Attention Block 7: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.6\n",
            "Cross-Attention Block 8: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.7\n",
            "Cross-Attention Block 9: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.8\n",
            "Cross-Attention Block 10: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.9\n",
            "Cross-Attention Block 11: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.10\n",
            "Cross-Attention Block 12: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.11\n",
            "Cross-Attention Block 13: Trainable = True\n",
            "  Location: After layer index 11\n",
            "  Full path: layers.11\n",
            "Cross-Attention Block 14: Trainable = True\n",
            "  Location: After layer index 12\n",
            "  Full path: layers.12\n",
            "Cross-Attention Block 15: Trainable = True\n",
            "  Location: Before LM head\n",
            "  Full path: final_cross_attn\n",
            "\n",
            "--- LM Head ---\n",
            "LM Head: Trainable = True\n",
            "\n",
            "Total Parameters: 216,360,222\n",
            "Trainable Parameters: 144,693,534 (66.88%)\n",
            "\n",
            "--- Unfreezing Target Details ---\n",
            "Total GPT layers found: 12\n",
            "Layers targeted for unfreezing: 10 and 11\n",
            "Epoch 10/30:   0% 0/20619 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Epoch 10/30: 100% 20619/20619 [53:55<00:00,  6.37it/s]\n",
            "\n",
            "Epoch 10/30, Avg Loss: 1.9548, Perplexity: 7.0628\n",
            "Token Accuracy: 50.48%\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Validation Loss: 11.8998, Perplexity: 147230.0830, Accuracy: 18.89%\n",
            "\n",
            "Epoch 11/30 - Trainable parameters:\n",
            "\n",
            "=== MODEL STRUCTURE AND TRAINABLE STATUS ===\n",
            "\n",
            "--- GPT Transformer Layers ---\n",
            "Total GPT transformer layers: 12\n",
            "GPT Layer 0: Trainable = False\n",
            "GPT Layer 1: Trainable = False\n",
            "GPT Layer 2: Trainable = False\n",
            "GPT Layer 3: Trainable = False\n",
            "GPT Layer 4: Trainable = False\n",
            "GPT Layer 5: Trainable = False\n",
            "GPT Layer 6: Trainable = False\n",
            "GPT Layer 7: Trainable = False\n",
            "GPT Layer 8: Trainable = False\n",
            "GPT Layer 9: Trainable = False\n",
            "GPT Layer 10: Trainable = True\n",
            "GPT Layer 11: Trainable = True\n",
            "\n",
            "--- Cross-Attention Blocks ---\n",
            "Cross-Attention Block 1: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.0\n",
            "Cross-Attention Block 2: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.1\n",
            "Cross-Attention Block 3: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.2\n",
            "Cross-Attention Block 4: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.3\n",
            "Cross-Attention Block 5: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.4\n",
            "Cross-Attention Block 6: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.5\n",
            "Cross-Attention Block 7: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.6\n",
            "Cross-Attention Block 8: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.7\n",
            "Cross-Attention Block 9: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.8\n",
            "Cross-Attention Block 10: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.9\n",
            "Cross-Attention Block 11: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.10\n",
            "Cross-Attention Block 12: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.11\n",
            "Cross-Attention Block 13: Trainable = True\n",
            "  Location: After layer index 11\n",
            "  Full path: layers.11\n",
            "Cross-Attention Block 14: Trainable = True\n",
            "  Location: After layer index 12\n",
            "  Full path: layers.12\n",
            "Cross-Attention Block 15: Trainable = True\n",
            "  Location: Before LM head\n",
            "  Full path: final_cross_attn\n",
            "\n",
            "--- LM Head ---\n",
            "LM Head: Trainable = True\n",
            "\n",
            "Total Parameters: 216,360,222\n",
            "Trainable Parameters: 144,693,534 (66.88%)\n",
            "\n",
            "--- Unfreezing Target Details ---\n",
            "Total GPT layers found: 12\n",
            "Layers targeted for unfreezing: 10 and 11\n",
            "Epoch 11/30:   0% 0/20619 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Epoch 11/30: 100% 20619/20619 [53:57<00:00,  6.37it/s]\n",
            "\n",
            "Epoch 11/30, Avg Loss: 1.8913, Perplexity: 6.6283\n",
            "Token Accuracy: 52.83%\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Validation Loss: 12.6290, Perplexity: 305299.5412, Accuracy: 21.50%\n",
            "\n",
            "Epoch 12/30 - Trainable parameters:\n",
            "\n",
            "=== MODEL STRUCTURE AND TRAINABLE STATUS ===\n",
            "\n",
            "--- GPT Transformer Layers ---\n",
            "Total GPT transformer layers: 12\n",
            "GPT Layer 0: Trainable = False\n",
            "GPT Layer 1: Trainable = False\n",
            "GPT Layer 2: Trainable = False\n",
            "GPT Layer 3: Trainable = False\n",
            "GPT Layer 4: Trainable = False\n",
            "GPT Layer 5: Trainable = False\n",
            "GPT Layer 6: Trainable = False\n",
            "GPT Layer 7: Trainable = False\n",
            "GPT Layer 8: Trainable = False\n",
            "GPT Layer 9: Trainable = False\n",
            "GPT Layer 10: Trainable = True\n",
            "GPT Layer 11: Trainable = True\n",
            "\n",
            "--- Cross-Attention Blocks ---\n",
            "Cross-Attention Block 1: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.0\n",
            "Cross-Attention Block 2: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.1\n",
            "Cross-Attention Block 3: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.2\n",
            "Cross-Attention Block 4: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.3\n",
            "Cross-Attention Block 5: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.4\n",
            "Cross-Attention Block 6: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.5\n",
            "Cross-Attention Block 7: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.6\n",
            "Cross-Attention Block 8: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.7\n",
            "Cross-Attention Block 9: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.8\n",
            "Cross-Attention Block 10: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.9\n",
            "Cross-Attention Block 11: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.10\n",
            "Cross-Attention Block 12: Trainable = True\n",
            "  Location: Unknown location\n",
            "  Full path: cross_attn.11\n",
            "Cross-Attention Block 13: Trainable = True\n",
            "  Location: After layer index 11\n",
            "  Full path: layers.11\n",
            "Cross-Attention Block 14: Trainable = True\n",
            "  Location: After layer index 12\n",
            "  Full path: layers.12\n",
            "Cross-Attention Block 15: Trainable = True\n",
            "  Location: Before LM head\n",
            "  Full path: final_cross_attn\n",
            "\n",
            "--- LM Head ---\n",
            "LM Head: Trainable = True\n",
            "\n",
            "Total Parameters: 216,360,222\n",
            "Trainable Parameters: 144,693,534 (66.88%)\n",
            "\n",
            "--- Unfreezing Target Details ---\n",
            "Total GPT layers found: 12\n",
            "Layers targeted for unfreezing: 10 and 11\n",
            "Epoch 12/30:   0% 0/20619 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Epoch 12/30:  64% 13146/20619 [34:22<19:26,  6.41it/s]"
          ]
        }
      ],
      "source": [
        "# !python scgpt_smiles_main.py --adata_dir \"/content/drive/MyDrive/Colab Notebooks/esm cell state/difference counts/\" \\\n",
        "# --smiles_path \"/content/drive/MyDrive/Colab Notebooks/esm cell state/smiles_df.csv\" \\\n",
        "# --working_dir \"/content/drive/MyDrive/Colab Notebooks/esm cell state/scgpt_workdir\" \\\n",
        "# --batch_size 1 --max_length 256 --num_epochs 10 --learning_rate 1e-5 \\\n",
        "# --checkpoints_dir \"/content/drive/MyDrive/Colab Notebooks/esm cell state/checkpoints\"\n",
        "\n",
        "\n",
        "!python scgpt_smiles_main.py \\\n",
        "  --processed_data_dir \"/content/drive/MyDrive/Colab Notebooks/esm cell state/scgpt_workdir_fixed_1\" \\\n",
        "  --use_mlm \\\n",
        "  --use_gradual_unfreezing \\\n",
        "  --num_epochs 30 \\\n",
        "  --learning_rate 3e-5 \\\n",
        "  --batch_size 1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## note to self --> cross attn is every 3 but only last two is unfrozen, so like the flamingo for plastics, we should just add cross attn for the end 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lqE4pDfM0XhV",
        "outputId": "83b8cf3d-2d5a-4487-e8bf-a928485e0688"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "processed_pairs.pkl  test_pairs.pkl  train_pairs.pkl  val_pairs.pkl\n"
          ]
        }
      ],
      "source": [
        "!ls  \"/content/drive/MyDrive/Colab Notebooks/esm cell state/scgpt_workdir\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebQTGkVlIMRS"
      },
      "source": [
        "# arc dataset..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWmzm43Oh9KN"
      },
      "source": [
        "#### eda"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rMvkpxs60ruj",
        "outputId": "b0c19851-6fab-4e68-8306-1179bc91c302"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total items: 27982\n",
            "Sample items: [<scgpt_smiles_model.SCGPTSmilesPair object at 0x7dc333318690>, <scgpt_smiles_model.SCGPTSmilesPair object at 0x7dc33338be50>, <scgpt_smiles_model.SCGPTSmilesPair object at 0x7dc3333aa8d0>, <scgpt_smiles_model.SCGPTSmilesPair object at 0x7dc3333aae50>, <scgpt_smiles_model.SCGPTSmilesPair object at 0x7dc33331a350>]\n",
            "Data types in sample: {0: <class 'scgpt_smiles_model.SCGPTSmilesPair'>, 1: <class 'scgpt_smiles_model.SCGPTSmilesPair'>, 2: <class 'scgpt_smiles_model.SCGPTSmilesPair'>, 3: <class 'scgpt_smiles_model.SCGPTSmilesPair'>, 4: <class 'scgpt_smiles_model.SCGPTSmilesPair'>}\n",
            "Unique types: {<class 'scgpt_smiles_model.SCGPTSmilesPair'>}\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "\n",
        "file_path = \"/content/drive/MyDrive/Colab Notebooks/esm cell state/scgpt_workdir/processed_pairs.pkl\"\n",
        "\n",
        "with open(file_path, \"rb\") as file:\n",
        "    data = pickle.load(file)\n",
        "\n",
        "# Display the structure of the data\n",
        "type(data), len(data) if hasattr(data, '__len__') else None\n",
        "\n",
        "sample_size = 5 if len(data) > 5 else len(data)\n",
        "sample_items = data[:sample_size]\n",
        "\n",
        "data_types = {i: type(item) for i, item in enumerate(sample_items)}\n",
        "\n",
        "unique_types = set(type(item) for item in data)\n",
        "\n",
        "print(f\"Total items: {len(data)}\")\n",
        "print(f\"Sample items: {sample_items}\")\n",
        "print(f\"Data types in sample: {data_types}\")\n",
        "print(f\"Unique types: {unique_types}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55xwCmznkKHK",
        "outputId": "dadc5112-d5a7-4b99-e538-06a98af226d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data type: <class 'list'>, Length: 27982\n",
            "Processed 0 items...\n",
            "Processed 1000 items...\n",
            "Processed 2000 items...\n",
            "Processed 3000 items...\n",
            "Processed 4000 items...\n",
            "Processed 5000 items...\n",
            "Processed 6000 items...\n",
            "Processed 7000 items...\n",
            "Processed 8000 items...\n",
            "Processed 9000 items...\n",
            "\n",
            "Total SMILES with uppercase 'M': 0\n",
            "Total SMILES with lowercase 'm': 0\n",
            "SMILES containing 'M' or 'm': []\n",
            "\n",
            "=== SMILES Character Statistics ===\n",
            "Unique characters found: #()+-./123456789=@BCFHINOPS[\\]lrt\n",
            "Sample SMILES strings: ['CCN1CCN(CC1)C2=CC=C(C=C2)NC3=CC(=NC=N3)N(C)C(=O)NC4=C(C(=CC(=C4Cl)OC)OC)Cl', 'CCN1CCN(CC1)C2=CC=C(C=C2)NC3=CC(=NC=N3)N(C)C(=O)NC4=C(C(=CC(=C4Cl)OC)OC)Cl', 'CCN1CCN(CC1)C2=CC=C(C=C2)NC3=CC(=NC=N3)N(C)C(=O)NC4=C(C(=CC(=C4Cl)OC)OC)Cl', 'CCN1CCN(CC1)C2=CC=C(C=C2)NC3=CC(=NC=N3)N(C)C(=O)NC4=C(C(=CC(=C4Cl)OC)OC)Cl', 'CCN1CCN(CC1)C2=CC=C(C=C2)NC3=CC(=NC=N3)N(C)C(=O)NC4=C(C(=CC(=C4Cl)OC)OC)Cl']\n",
            "\n",
            "CONCLUSION: Neither 'M' nor 'm' found in any SMILES strings. It is safe to use 'M' as a mask token.\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "from collections import Counter\n",
        "\n",
        "file_path = \"/content/drive/MyDrive/Colab Notebooks/esm cell state/scgpt_workdir/processed_pairs.pkl\"\n",
        "\n",
        "with open(file_path, \"rb\") as file:\n",
        "    data = pickle.load(file)\n",
        "\n",
        "print(f\"Data type: {type(data)}, Length: {len(data) if hasattr(data, '__len__') else None}\")\n",
        "\n",
        "# Function to identify SMILES strings\n",
        "def is_likely_smiles(s):\n",
        "    # This is a basic check. You might want to make it more sophisticated.\n",
        "    return isinstance(s, str) and any(char in s for char in 'CNO[]()=#')\n",
        "\n",
        "# Initialize counters and lists\n",
        "m_upper_count = 0\n",
        "m_lower_count = 0\n",
        "m_containing_items = []\n",
        "all_chars = Counter()\n",
        "sample_smiles = []\n",
        "\n",
        "# Process the data\n",
        "for i, item in enumerate(data):\n",
        "    if hasattr(item, '__dict__'):\n",
        "        for key, value in item.__dict__.items():\n",
        "            if is_likely_smiles(value):\n",
        "                # Count 'M' and 'm'\n",
        "                if 'M' in value:\n",
        "                    m_upper_count += 1\n",
        "                    if len(m_containing_items) < 10:\n",
        "                        m_containing_items.append((i, 'M', value))\n",
        "                if 'm' in value:\n",
        "                    m_lower_count += 1\n",
        "                    if len(m_containing_items) < 10:\n",
        "                        m_containing_items.append((i, 'm', value))\n",
        "\n",
        "                # Collect character statistics\n",
        "                all_chars.update(value)\n",
        "\n",
        "                # Collect sample SMILES\n",
        "                if len(sample_smiles) < 5:\n",
        "                    sample_smiles.append(value)\n",
        "\n",
        "    if i < 10000 and (i % 1000 == 0):\n",
        "        print(f\"Processed {i} items...\")\n",
        "\n",
        "# Print results\n",
        "print(f\"\\nTotal SMILES with uppercase 'M': {m_upper_count}\")\n",
        "print(f\"Total SMILES with lowercase 'm': {m_lower_count}\")\n",
        "print(f\"SMILES containing 'M' or 'm': {m_containing_items}\")\n",
        "\n",
        "print(\"\\n=== SMILES Character Statistics ===\")\n",
        "print(f\"Unique characters found: {''.join(sorted(all_chars.keys()))}\")\n",
        "print(f\"Sample SMILES strings: {sample_smiles}\")\n",
        "\n",
        "if m_upper_count == 0 and m_lower_count == 0:\n",
        "    print(\"\\nCONCLUSION: Neither 'M' nor 'm' found in any SMILES strings. It is safe to use 'M' as a mask token.\")\n",
        "else:\n",
        "    print(f\"\\nCONCLUSION: Found {m_upper_count} instances of 'M' and {m_lower_count} instances of 'm' in SMILES strings. Consider using a different mask token.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ptxaarO21q4L",
        "outputId": "8e8ce64c-a61f-4e5f-96fa-3b9066854a18"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Attributes: ['cell_embedding', 'smiles_string']\n",
            "Attribute Values: {'cell_embedding': tensor([ 6.6132e-03, -3.0450e-03, -1.3491e-03,  1.6596e-03,  4.5485e-03,\n",
            "         1.0073e-03,  1.4558e-03, -1.5186e-04,  3.6667e-04,  3.9284e-03,\n",
            "         2.1864e-03,  7.1017e-03,  2.8821e-03, -3.1636e-03, -4.0230e-04,\n",
            "         1.6058e-03,  1.5317e-03,  8.1104e-03, -1.1847e-03,  7.2079e-04,\n",
            "         2.9614e-03,  5.9930e-03, -6.8728e-04, -1.3140e-03,  3.7801e-03,\n",
            "         1.1000e-03,  1.1637e-03,  5.2239e-03,  6.1034e-03, -1.3406e-03,\n",
            "        -5.2022e-03, -6.4475e-03, -2.2136e-03,  4.5650e-04, -4.4613e-03,\n",
            "         1.0329e-03,  3.2738e-03, -2.3091e-03, -4.4869e-03,  7.6485e-04,\n",
            "         5.5050e-04,  1.4897e-03,  1.0159e-03, -5.5807e-03, -1.3702e-03,\n",
            "        -3.7904e-03, -3.7659e-04, -1.1254e-03,  3.9252e-03, -3.4051e-03,\n",
            "        -2.1338e-04,  2.0985e-03,  1.6477e-03,  6.3730e-04,  3.1382e-03,\n",
            "        -4.1852e-03, -6.0325e-03, -7.0772e-04,  2.3096e-03,  4.0186e-03,\n",
            "        -1.1126e-03, -5.3812e-03, -8.6283e-03, -1.8076e-03, -2.8122e-03,\n",
            "        -5.0362e-03,  5.4089e-03,  1.3787e-03,  2.9060e-03,  4.0115e-03,\n",
            "         1.0703e-02, -4.9542e-03,  3.8216e-03,  2.6341e-03,  1.3922e-03,\n",
            "        -2.3201e-03,  4.2515e-04,  1.8340e-03,  1.0196e-03,  7.0689e-04,\n",
            "         3.3690e-03,  5.2929e-03, -1.9572e-03,  9.6072e-04, -3.9507e-03,\n",
            "         5.8395e-03,  9.7247e-03, -5.6053e-03, -5.5821e-04, -4.5847e-03,\n",
            "         7.4558e-03,  2.6530e-03,  1.4177e-03, -5.0276e-03, -5.1499e-04,\n",
            "         6.8194e-03,  1.0879e-03,  4.3596e-03, -2.0795e-03, -6.1256e-03,\n",
            "         5.9879e-03, -1.3346e-03, -2.3047e-03, -1.2110e-03, -4.2279e-03,\n",
            "        -2.3176e-03, -4.6699e-03, -7.8009e-04, -2.4957e-03, -5.1940e-03,\n",
            "         2.7743e-03,  3.5545e-04, -1.2902e-03,  6.8213e-04, -3.2598e-03,\n",
            "         8.3372e-04, -2.4140e-06,  1.5147e-03, -2.6544e-03, -5.2206e-03,\n",
            "        -2.8822e-03,  1.0592e-03, -3.7672e-03,  1.6662e-03, -9.5275e-04,\n",
            "        -6.2517e-03,  3.9847e-04,  6.5912e-04, -3.3852e-04, -1.7199e-03,\n",
            "         1.0451e-04,  3.5972e-03,  2.7304e-04, -4.5445e-03, -1.5128e-03,\n",
            "        -1.4491e-03, -1.9059e-03, -5.0559e-03, -3.7541e-03, -4.6586e-03,\n",
            "        -3.5013e-03,  8.7048e-04,  6.1830e-03, -2.5675e-03, -6.2397e-03,\n",
            "         3.4606e-03,  2.1657e-03, -4.4193e-04,  1.5994e-03, -4.7702e-03,\n",
            "         6.8110e-03, -4.0433e-03,  7.5236e-04,  3.6985e-03, -5.3941e-03,\n",
            "        -2.8332e-04,  1.9784e-03, -2.7931e-03,  3.8884e-03,  1.9623e-03,\n",
            "        -4.0846e-03, -3.8263e-03,  4.3853e-03, -5.3851e-03,  6.2220e-03,\n",
            "        -4.5969e-03, -9.3054e-04,  4.0854e-03, -3.1508e-03, -3.4175e-04,\n",
            "         1.4557e-03,  4.1397e-03, -2.0317e-03,  1.9691e-03, -4.3036e-03,\n",
            "        -2.0989e-03,  1.2540e-03, -1.5753e-03, -4.0669e-03,  3.1210e-04,\n",
            "        -3.2530e-03,  7.0759e-03,  7.5770e-04, -5.4853e-03, -5.1784e-03,\n",
            "        -1.6535e-03,  5.4850e-03,  6.7476e-04, -1.4531e-03, -4.7137e-03,\n",
            "        -9.2319e-04,  6.9378e-04, -1.4331e-03,  7.1302e-03,  3.9544e-03,\n",
            "         2.2957e-03,  2.6093e-03,  3.7949e-03,  1.1750e-03, -8.2780e-04,\n",
            "         1.6960e-03,  2.9954e-03,  8.5658e-04,  1.6600e-03,  8.6334e-03,\n",
            "         8.7176e-03,  3.9690e-05,  8.9747e-04,  3.9399e-03,  4.7367e-03,\n",
            "        -1.6229e-03, -1.2558e-05,  1.8471e-03, -1.0086e-03, -2.8800e-03,\n",
            "        -4.8480e-03, -3.4601e-03, -7.1251e-03,  3.0884e-03, -2.3467e-03,\n",
            "        -2.1147e-03,  1.7368e-03,  6.6250e-04, -4.0256e-04,  2.7559e-03,\n",
            "         2.2995e-03,  5.5169e-03,  3.4161e-03,  1.7621e-03, -2.5424e-03,\n",
            "         2.5583e-03,  5.1675e-03, -1.3615e-03,  1.7428e-03,  2.9883e-03,\n",
            "         7.0462e-03,  6.9332e-03,  1.7984e-03,  2.3043e-03, -3.4478e-03,\n",
            "         1.1223e-03, -1.1267e-03,  1.0403e-03,  5.8686e-03, -4.6231e-03,\n",
            "        -1.0553e-03, -2.6148e-03, -3.6039e-03, -3.0164e-03,  1.9726e-03,\n",
            "         2.4660e-03, -9.2006e-04,  4.5539e-04, -3.6664e-03, -2.6114e-03,\n",
            "         4.4883e-03,  5.5747e-03, -3.7350e-03, -1.1236e-03, -3.0761e-03,\n",
            "         5.9496e-03, -1.6657e-04,  3.2146e-03,  6.4615e-05, -2.5191e-03,\n",
            "         3.5431e-03, -2.1960e-03,  1.3005e-03,  1.9293e-03, -4.9541e-03,\n",
            "        -3.4118e-03,  6.2811e-03,  2.9293e-03,  2.8955e-03,  6.3490e-03,\n",
            "         1.5244e-03, -3.7766e-04, -3.5996e-03, -7.4357e-03, -4.1560e-03,\n",
            "         7.7030e-04,  4.0343e-03,  1.6196e-02, -6.7509e-03,  3.3891e-03,\n",
            "        -4.1297e-03,  2.6960e-03, -2.2104e-03, -2.7885e-03,  4.6450e-03,\n",
            "        -1.3989e-03, -7.8649e-03,  3.1034e-03, -4.0097e-05,  4.3510e-04,\n",
            "         8.0875e-03,  2.4923e-03,  1.8751e-03,  3.6365e-03,  4.7351e-03,\n",
            "         4.1498e-04, -3.8484e-03, -1.4481e-03,  4.9445e-03, -6.3954e-04,\n",
            "         3.7313e-03,  2.2826e-03,  4.3220e-04,  2.8130e-03,  5.1644e-03,\n",
            "         6.0156e-03, -7.7892e-03, -2.3879e-03,  5.4080e-03,  4.7834e-04,\n",
            "         2.9147e-03, -3.4721e-03, -7.6779e-03,  2.5594e-03,  5.6182e-03,\n",
            "        -4.7246e-03, -1.0802e-02, -1.9554e-04, -9.0394e-04, -5.9177e-03,\n",
            "         1.2452e-03,  6.0928e-04, -1.5613e-03,  2.7766e-03, -6.5453e-04,\n",
            "        -2.4042e-03, -4.3306e-03, -2.4836e-03,  2.7845e-03, -1.2989e-03,\n",
            "        -2.3442e-04,  1.3717e-03, -7.7684e-03,  5.6617e-03, -6.3688e-03,\n",
            "         6.0576e-03,  5.5121e-04, -4.7752e-04, -5.7886e-03, -1.9472e-03,\n",
            "         8.2482e-03, -2.6399e-03,  3.8872e-03, -7.1080e-03, -8.9594e-03,\n",
            "         4.6282e-03,  2.9934e-03, -1.4800e-03, -2.5744e-03, -2.2583e-03,\n",
            "        -5.6347e-03,  3.7591e-03,  5.8949e-03,  2.1787e-03, -3.6952e-03,\n",
            "        -5.3298e-03, -3.3747e-03, -7.6268e-04, -3.7796e-03, -6.9127e-04,\n",
            "         1.4889e-03,  1.0086e-02,  3.4816e-03,  2.6289e-03,  3.7910e-03,\n",
            "         2.9540e-03, -7.0204e-03, -6.5140e-03,  7.0429e-03, -4.5599e-03,\n",
            "         4.9947e-03, -9.4999e-04,  1.3578e-04, -3.3499e-04,  2.4971e-04,\n",
            "         4.5533e-03, -7.2091e-03, -3.4338e-03, -4.7118e-04,  8.5962e-04,\n",
            "         3.7076e-03,  5.5746e-03, -6.0436e-03,  2.8925e-03,  3.8328e-03,\n",
            "        -7.9626e-03, -5.3929e-04, -1.1195e-03,  6.2971e-04, -1.8863e-03,\n",
            "         1.0046e-02, -8.4580e-03, -1.7730e-03,  3.5440e-03, -2.8088e-03,\n",
            "        -2.0558e-04, -4.1204e-03, -1.5614e-04,  3.5834e-04,  7.3326e-03,\n",
            "        -5.9776e-03, -1.2122e-04, -3.3360e-03, -8.1705e-04, -1.9088e-03,\n",
            "         7.0057e-03,  2.8900e-03, -3.3431e-03, -1.4396e-03, -5.6456e-03,\n",
            "         8.8362e-03,  7.6360e-04, -3.2044e-04,  4.8401e-04,  2.5557e-03,\n",
            "         1.6530e-03,  1.0447e-03,  3.7568e-03,  7.1362e-04, -6.7667e-03,\n",
            "         5.9175e-03,  5.2647e-03, -6.1873e-04, -5.0895e-03,  1.3926e-03,\n",
            "         3.8287e-04,  7.0547e-03, -8.3157e-04,  2.8848e-03,  1.7600e-03,\n",
            "         1.8788e-04, -3.5927e-03,  7.0674e-04,  4.3755e-04,  7.6454e-05,\n",
            "        -1.6674e-03,  3.1040e-04,  4.9066e-03, -1.9075e-03, -7.2530e-04,\n",
            "         4.3633e-03, -1.0485e-03, -1.1298e-02, -3.9611e-03, -6.9841e-03,\n",
            "        -6.9168e-03, -8.8791e-04, -1.9805e-03,  2.5591e-03,  9.2908e-05,\n",
            "        -6.5807e-05, -4.3402e-04, -1.5251e-03, -8.3369e-04,  6.0368e-03,\n",
            "        -1.2683e-03, -4.1666e-03, -4.7099e-03,  1.1366e-03, -2.5807e-03,\n",
            "         7.3798e-04,  2.3801e-03, -1.0830e-03, -4.6008e-03, -2.9167e-04,\n",
            "        -2.6641e-03, -3.8481e-03,  7.9703e-04, -1.7008e-03,  2.0378e-04,\n",
            "         6.1093e-03, -2.6562e-04, -1.9550e-03, -1.3012e-03, -2.0428e-03,\n",
            "         1.5668e-03, -3.9510e-03,  6.0342e-04, -6.4198e-03, -5.6316e-03,\n",
            "        -1.9833e-04,  5.0288e-03, -1.5593e-03,  2.2367e-03,  1.0292e-03,\n",
            "        -3.7784e-03, -1.6456e-03, -2.8459e-03, -4.0614e-03,  8.3338e-04,\n",
            "         3.5182e-03,  9.9768e-04, -2.4240e-03, -5.6529e-03,  4.1020e-03,\n",
            "        -4.9344e-04, -3.8687e-03, -6.1148e-04, -3.1760e-03, -1.0497e-02,\n",
            "        -4.8603e-03,  1.7486e-03, -5.5650e-04,  5.6433e-04,  1.5017e-03,\n",
            "         6.9230e-04,  1.7145e-05]), 'smiles_string': 'CCN1CCN(CC1)C2=CC=C(C=C2)NC3=CC(=NC=N3)N(C)C(=O)NC4=C(C(=CC(=C4Cl)OC)OC)Cl'}\n"
          ]
        }
      ],
      "source": [
        "sample_item = data[0]\n",
        "\n",
        "attributes = dir(sample_item)\n",
        "filtered_attributes = [attr for attr in attributes if not attr.startswith(\"__\")]\n",
        "\n",
        "attribute_values = {attr: getattr(sample_item, attr) for attr in filtered_attributes if hasattr(sample_item, attr)}\n",
        "\n",
        "print(f\"Attributes: {filtered_attributes}\")\n",
        "print(f\"Attribute Values: {attribute_values}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jZGq3Ewt14Mj",
        "outputId": "53fae9af-c6b6-4c6d-f41f-971bea85649e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total pairs: 27982\n",
            "Unique embedding shapes: {torch.Size([512])}\n",
            "Sample entries: [{'smiles': 'CCN1CCN(CC1)C2=CC=C(C=C2)NC3=CC(=NC=N3)N(C)C(=O)NC4=C(C(=CC(=C4Cl)OC)OC)Cl', 'embedding_shape': torch.Size([512])}, {'smiles': 'CCN1CCN(CC1)C2=CC=C(C=C2)NC3=CC(=NC=N3)N(C)C(=O)NC4=C(C(=CC(=C4Cl)OC)OC)Cl', 'embedding_shape': torch.Size([512])}, {'smiles': 'CCN1CCN(CC1)C2=CC=C(C=C2)NC3=CC(=NC=N3)N(C)C(=O)NC4=C(C(=CC(=C4Cl)OC)OC)Cl', 'embedding_shape': torch.Size([512])}, {'smiles': 'CCN1CCN(CC1)C2=CC=C(C=C2)NC3=CC(=NC=N3)N(C)C(=O)NC4=C(C(=CC(=C4Cl)OC)OC)Cl', 'embedding_shape': torch.Size([512])}, {'smiles': 'CCN1CCN(CC1)C2=CC=C(C=C2)NC3=CC(=NC=N3)N(C)C(=O)NC4=C(C(=CC(=C4Cl)OC)OC)Cl', 'embedding_shape': torch.Size([512])}]\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "import anndata\n",
        "import scanpy as sc\n",
        "import os\n",
        "import numpy as np\n",
        "from typing import Dict, List, Optional, Tuple, Union\n",
        "import math\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from molgpt_loader import load_mol_gpt_model\n",
        "# Extracting first 5 samples for quick inspection\n",
        "sample_size = 5\n",
        "samples = [\n",
        "    {\n",
        "        \"smiles\": pair.smiles_string,\n",
        "        \"embedding_shape\": pair.cell_embedding.shape if isinstance(pair.cell_embedding, torch.Tensor) else None\n",
        "    }\n",
        "    for pair in data[:sample_size]\n",
        "]\n",
        "\n",
        "# Get summary statistics of tensor embeddings\n",
        "embedding_shapes = [pair.cell_embedding.shape for pair in data if isinstance(pair.cell_embedding, torch.Tensor)]\n",
        "unique_shapes = set(embedding_shapes)\n",
        "\n",
        "print(f\"Total pairs: {len(data)}\")\n",
        "print(f\"Unique embedding shapes: {unique_shapes}\")\n",
        "print(f\"Sample entries: {samples}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DE3ZDVAn_n9Q",
        "outputId": "8145e263-cbfa-4477-b9e7-1fa6b7e6ff5b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of training samples: 13628\n",
            "Number of validation samples: 4037\n",
            "\n",
            "=== Training SMILES Analysis ===\n",
            "Average length: 86.70 ± 39.65\n",
            "Min length: 50\n",
            "Max length: 196\n",
            "Valid molecules: 13628/13628 (100.00%)\n",
            "Most common characters: [('C', 381359), ('=', 146842), ('(', 116946), (')', 116946), ('N', 65186), ('O', 60501), ('@', 43443), ('[', 30250), (']', 30250), ('1', 27256)]\n",
            "\n",
            "=== Validation SMILES Analysis ===\n",
            "Average length: 57.70 ± 9.25\n",
            "Min length: 49\n",
            "Max length: 75\n",
            "Valid molecules: 4037/4037 (100.00%)\n",
            "Most common characters: [('C', 73662), ('=', 33266), ('(', 21434), (')', 21434), ('N', 16882), ('O', 11007), ('1', 8074), ('2', 8074), ('3', 7080), ('4', 6168)]\n",
            "\n",
            "Overlap between training and validation sets: 0 molecules\n",
            "Percentage of validation set in training: 0.00%\n",
            "\n",
            "Cosine similarity between average training and validation embeddings: 0.9747\n",
            "\n",
            "Training embeddings: shape=(13628, 512), mean=-0.0000, std=0.0042\n",
            "Validation embeddings: shape=(4037, 512), mean=-0.0000, std=0.0042\n",
            "\n",
            "=== Example SMILES from Training Set ===\n",
            "CN(C1CCS(=O)(=O)CC1)C(=O)CNC(=O)/C=C/C2=CC(=CC=C2)Cl\n",
            "CN(C1CCS(=O)(=O)CC1)C(=O)CNC(=O)/C=C/C2=CC(=CC=C2)Cl\n",
            "CN(C1CCS(=O)(=O)CC1)C(=O)CNC(=O)/C=C/C2=CC(=CC=C2)Cl\n",
            "CN(C1CCS(=O)(=O)CC1)C(=O)CNC(=O)/C=C/C2=CC(=CC=C2)Cl\n",
            "CN(C1CCS(=O)(=O)CC1)C(=O)CNC(=O)/C=C/C2=CC(=CC=C2)Cl\n",
            "\n",
            "=== Example SMILES from Validation Set ===\n",
            "C1CN(CCC1(C(=O)N[C@@H](CCO)C2=CC=C(C=C2)Cl)N)C3=NC=NC4=C3C=CN4\n",
            "C1CN(CCC1(C(=O)N[C@@H](CCO)C2=CC=C(C=C2)Cl)N)C3=NC=NC4=C3C=CN4\n",
            "C1CN(CCC1(C(=O)N[C@@H](CCO)C2=CC=C(C=C2)Cl)N)C3=NC=NC4=C3C=CN4\n",
            "C1CN(CCC1(C(=O)N[C@@H](CCO)C2=CC=C(C=C2)Cl)N)C3=NC=NC4=C3C=CN4\n",
            "C1CN(CCC1(C(=O)N[C@@H](CCO)C2=CC=C(C=C2)Cl)N)C3=NC=NC4=C3C=CN4\n",
            "\n",
            "=== Summary ===\n",
            "      Dataset  Samples  Avg Length  Valid Molecules %\n",
            "0    Training    13628   86.703625              100.0\n",
            "1  Validation     4037   57.695071              100.0\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "from rdkit import Chem\n",
        "import seaborn as sns\n",
        "\n",
        "# Load the data\n",
        "train_data_path = \"/content/drive/MyDrive/Colab Notebooks/esm cell state/scgpt_workdir/train_pairs.pkl\"\n",
        "val_data_path = \"/content/drive/MyDrive/Colab Notebooks/esm cell state/scgpt_workdir/val_pairs.pkl\"\n",
        "\n",
        "with open(train_data_path, 'rb') as f:\n",
        "    train_pairs = pickle.load(f)\n",
        "\n",
        "with open(val_data_path, 'rb') as f:\n",
        "    val_pairs = pickle.load(f)\n",
        "\n",
        "print(f\"Number of training samples: {len(train_pairs)}\")\n",
        "print(f\"Number of validation samples: {len(val_pairs)}\")\n",
        "\n",
        "# Extract SMILES strings\n",
        "train_smiles = [pair.smiles_string for pair in train_pairs]\n",
        "val_smiles = [pair.smiles_string for pair in val_pairs]\n",
        "\n",
        "# Extract cell embeddings\n",
        "train_embeddings = [pair.cell_embedding.numpy() for pair in train_pairs]\n",
        "val_embeddings = [pair.cell_embedding.numpy() for pair in val_pairs]\n",
        "\n",
        "# Basic statistics about SMILES strings\n",
        "def analyze_smiles(smiles_list, name):\n",
        "    lengths = [len(s) for s in smiles_list]\n",
        "    valid_mols = [Chem.MolFromSmiles(s) is not None for s in smiles_list]\n",
        "\n",
        "    print(f\"\\n=== {name} SMILES Analysis ===\")\n",
        "    print(f\"Average length: {np.mean(lengths):.2f} ± {np.std(lengths):.2f}\")\n",
        "    print(f\"Min length: {min(lengths)}\")\n",
        "    print(f\"Max length: {max(lengths)}\")\n",
        "    print(f\"Valid molecules: {sum(valid_mols)}/{len(smiles_list)} ({sum(valid_mols)/len(smiles_list)*100:.2f}%)\")\n",
        "\n",
        "    # Most common characters\n",
        "    all_chars = ''.join(smiles_list)\n",
        "    char_counts = Counter(all_chars)\n",
        "    print(f\"Most common characters: {char_counts.most_common(10)}\")\n",
        "\n",
        "    return lengths, valid_mols\n",
        "\n",
        "train_lengths, train_valid = analyze_smiles(train_smiles, \"Training\")\n",
        "val_lengths, val_valid = analyze_smiles(val_smiles, \"Validation\")\n",
        "\n",
        "# Visualize SMILES length distribution\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.hist(train_lengths, bins=30, alpha=0.5, label='Training')\n",
        "plt.hist(val_lengths, bins=30, alpha=0.5, label='Validation')\n",
        "plt.xlabel('SMILES Length')\n",
        "plt.ylabel('Count')\n",
        "plt.title('SMILES Length Distribution')\n",
        "plt.legend()\n",
        "plt.savefig('smiles_length_distribution.png')\n",
        "plt.close()\n",
        "\n",
        "# Check for overlap between training and validation sets\n",
        "train_set = set(train_smiles)\n",
        "val_set = set(val_smiles)\n",
        "overlap = train_set.intersection(val_set)\n",
        "print(f\"\\nOverlap between training and validation sets: {len(overlap)} molecules\")\n",
        "print(f\"Percentage of validation set in training: {len(overlap)/len(val_set)*100:.2f}%\")\n",
        "\n",
        "# Analyze embedding similarity\n",
        "def compute_embedding_stats(train_emb, val_emb):\n",
        "    train_emb_array = np.vstack(train_emb)\n",
        "    val_emb_array = np.vstack(val_emb)\n",
        "\n",
        "    # Compute mean and std for each set\n",
        "    train_mean = np.mean(train_emb_array, axis=0)\n",
        "    val_mean = np.mean(val_emb_array, axis=0)\n",
        "\n",
        "    # Compute cosine similarity between means\n",
        "    cosine_sim = np.dot(train_mean, val_mean) / (np.linalg.norm(train_mean) * np.linalg.norm(val_mean))\n",
        "\n",
        "    # Compute PCA to visualize\n",
        "    from sklearn.decomposition import PCA\n",
        "    combined = np.vstack([train_emb_array[:1000], val_emb_array[:1000]])  # Limit to 1000 samples each for visualization\n",
        "    pca = PCA(n_components=2)\n",
        "    reduced = pca.fit_transform(combined)\n",
        "\n",
        "    # Plot PCA\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    plt.scatter(reduced[:1000, 0], reduced[:1000, 1], alpha=0.5, label='Training')\n",
        "    plt.scatter(reduced[1000:, 0], reduced[1000:, 1], alpha=0.5, label='Validation')\n",
        "    plt.xlabel('PC1')\n",
        "    plt.ylabel('PC2')\n",
        "    plt.title('PCA of Cell Embeddings')\n",
        "    plt.legend()\n",
        "    plt.savefig('embedding_pca.png')\n",
        "    plt.close()\n",
        "\n",
        "    return cosine_sim\n",
        "\n",
        "emb_similarity = compute_embedding_stats(train_embeddings, val_embeddings)\n",
        "print(f\"\\nCosine similarity between average training and validation embeddings: {emb_similarity:.4f}\")\n",
        "\n",
        "# Check for embedding differences between train and val\n",
        "train_emb_array = np.vstack(train_embeddings)\n",
        "val_emb_array = np.vstack(val_embeddings)\n",
        "\n",
        "print(f\"\\nTraining embeddings: shape={train_emb_array.shape}, mean={np.mean(train_emb_array):.4f}, std={np.std(train_emb_array):.4f}\")\n",
        "print(f\"Validation embeddings: shape={val_emb_array.shape}, mean={np.mean(val_emb_array):.4f}, std={np.std(val_emb_array):.4f}\")\n",
        "\n",
        "# Save some examples for inspection\n",
        "print(\"\\n=== Example SMILES from Training Set ===\")\n",
        "for i in range(5):\n",
        "    print(train_smiles[i])\n",
        "\n",
        "print(\"\\n=== Example SMILES from Validation Set ===\")\n",
        "for i in range(5):\n",
        "    print(val_smiles[i])\n",
        "\n",
        "# Create summary dataframe\n",
        "summary = {\n",
        "    'Dataset': ['Training', 'Validation'],\n",
        "    'Samples': [len(train_smiles), len(val_smiles)],\n",
        "    'Avg Length': [np.mean(train_lengths), np.mean(val_lengths)],\n",
        "    'Valid Molecules %': [sum(train_valid)/len(train_valid)*100, sum(val_valid)/len(val_valid)*100]\n",
        "}\n",
        "summary_df = pd.DataFrame(summary)\n",
        "print(\"\\n=== Summary ===\")\n",
        "print(summary_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7AaVafohlm-"
      },
      "source": [
        "#### split 1 evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vAgiKFZVDnSp",
        "outputId": "090706e4-9c38-4f1e-a9bb-e126371f0820"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train: 13628, Val: 4037, Test: 10153\n",
            "Train embeddings: (13628, 512), Val: (4037, 512), Test: (10153, 512)\n",
            "\n",
            "--- SMILES Distribution Analysis ---\n",
            "Train MW: 547.41 ± 171.55\n",
            "Val MW: 371.88 ± 74.01\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n",
            "[16:20:52] DEPRECATION WARNING: please use MorganGenerator\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train internal similarity: 0.1371\n",
            "Val internal similarity: 0.2258\n",
            "Train-Val similarity: 0.1222\n",
            "Top 5 substructure differences (MACCS keys):\n",
            "Key 126: Train 0.55, Val 0.05, Diff 0.50\n",
            "Key 111: Train 0.71, Val 0.26, Diff 0.46\n",
            "Key 112: Train 0.63, Val 0.17, Diff 0.45\n",
            "Key 147: Train 0.69, Val 0.26, Diff 0.43\n",
            "Key 141: Train 0.57, Val 0.16, Diff 0.41\n",
            "\n",
            "--- SMILES Augmentation & Resampling Strategies ---\n",
            "1. Canonicalization - standardizes SMILES representation\n",
            "2. SMILES randomization - generates multiple valid SMILES for the same molecule\n",
            "3. Fragmentation - creates smaller molecules from large ones\n",
            "\n",
            "4. Molecular weight stratification:\n",
            "MW stratification bin counts:\n",
            "Bin 1: Train 2792, Val 2059\n",
            "Bin 2: Train 6751, Val 1978\n",
            "Bin 3: Train 1440, Val 0\n",
            "Bin 4: Train 875, Val 0\n",
            "Bin 5: Train 1770, Val 0\n",
            "\n",
            "5. Fingerprint clustering:\n",
            "Fingerprint clustering results:\n",
            "Cluster 1: Train 370, Val 0\n",
            "Cluster 2: Train 5240, Val 847\n",
            "Cluster 3: Train 781, Val 0\n",
            "Cluster 4: Train 1564, Val 0\n",
            "Cluster 5: Train 5673, Val 3190\n",
            "\n",
            "Additional strategies:\n",
            "6. Scaffold-based splitting - ensures diverse chemical scaffolds in all sets\n",
            "7. Property-based stratification - LogP, HBA, HBD, etc.\n",
            "8. Augmentation through enumeration of stereoisomers\n",
            "9. Substructure-based enrichment to balance representation\n",
            "10. Time-split for temporal datasets or assay-based grouping\n",
            "\n",
            "--- Cell Embedding Analysis ---\n",
            "Train embeddings: mean=-0.000003, std=0.004244\n",
            "Val embeddings: mean=-0.000002, std=0.004185\n",
            "Embedding dimensionality: 512\n",
            "Train internal cosine similarity: 0.2326 ± 0.2402\n",
            "Val internal cosine similarity: 0.2107 ± 0.2987\n",
            "Train-Val cosine similarity: 0.2273 ± 0.2626\n",
            "Dimensions needed for 95% variance: 96\n",
            "Dimensions needed for 99% variance: 250\n",
            "\n",
            "--- Cell Embedding Enhancement Strategies ---\n",
            "1. PCA dimensionality reduction - focus on major variance components\n",
            "Reduced from 512 to 96 dimensions\n",
            "\n",
            "2. UMAP projection - preserves local neighborhood structure\n",
            "(UMAP visualization would be generated)\n",
            "\n",
            "3. Clustering embeddings - group similar cell states\n",
            "Cluster sizes: [195 178 242 181 204]\n",
            "Silhouette score: 0.0658\n",
            "\n",
            "4. Standardization - normalize feature scales\n",
            "Before: mean=-0.0000, std=0.0037\n",
            "After: mean=-0.0000, std=1.0000\n",
            "\n",
            "Additional strategies:\n",
            "5. Noise injection - add small Gaussian noise for regularization\n",
            "6. Feature selection - keep only the most informative dimensions\n",
            "7. Adversarial examples - generate boundary cases\n",
            "8. Interpolation between similar embeddings\n",
            "9. Ensemble of projections - combine multiple dimensionality reductions\n",
            "10. Mixture model fitting - model cell states as mixtures\n",
            "\n",
            "--- Recommended Data Stratification Approaches ---\n",
            "\n",
            "Based on the analysis, here are the most promising stratification approaches:\n",
            "\n",
            "1. Combined Fingerprint & Embedding clustering:\n",
            "   - Cluster molecules by fingerprint similarity\n",
            "   - Within each cluster, ensure proportional distribution of cell embeddings\n",
            "   - Ensures chemical diversity and balanced cell state distribution\n",
            "\n",
            "2. Multi-property stratification:\n",
            "   - Stratify by molecular weight, logP, and fingerprint clusters\n",
            "   - Ensures balanced representation of different molecular classes\n",
            "\n",
            "3. SMILES augmentation with cluster balancing:\n",
            "   - Generate multiple SMILES representations for underrepresented clusters\n",
            "   - Ensures more balanced training\n",
            "\n",
            "4. Size-matched molecular pairs:\n",
            "   - Ensure similar molecules appear in both training and validation\n",
            "   - Improves model generalization to similar chemical space\n",
            "\n",
            "5. Scaffold-based splitting with embedding alignment:\n",
            "   - Split by scaffold but ensure similar cell embedding distributions\n",
            "\n",
            "Recommended Implementation Plan:\n",
            "1. Canonicalize all SMILES\n",
            "2. Cluster by molecular fingerprints (5-10 clusters)\n",
            "3. Within each cluster, apply stratified sampling based on embedding PCA\n",
            "4. Augment training data with randomized SMILES for diversity\n",
            "5. Validate the split by measuring intra/inter similarity metrics\n",
            "\n",
            "This approach should dramatically improve the distributional similarity\n",
            "between training and validation sets while maintaining independence.\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import AllChem, DataStructs\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans, DBSCAN\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import umap\n",
        "import random\n",
        "from collections import Counter\n",
        "\n",
        "# Load data\n",
        "train_data_path = \"/content/drive/MyDrive/Colab Notebooks/esm cell state/scgpt_workdir/train_pairs.pkl\"\n",
        "val_data_path = \"/content/drive/MyDrive/Colab Notebooks/esm cell state/scgpt_workdir/val_pairs.pkl\"\n",
        "test_data_path = \"/content/drive/MyDrive/Colab Notebooks/esm cell state/scgpt_workdir/test_pairs.pkl\"\n",
        "\n",
        "with open(train_data_path, 'rb') as f:\n",
        "    train_pairs = pickle.load(f)\n",
        "\n",
        "with open(val_data_path, 'rb') as f:\n",
        "    val_pairs = pickle.load(f)\n",
        "\n",
        "with open(test_data_path, 'rb') as f:\n",
        "    test_pairs = pickle.load(f)\n",
        "\n",
        "print(f\"Train: {len(train_pairs)}, Val: {len(val_pairs)}, Test: {len(test_pairs)}\")\n",
        "\n",
        "# Extract SMILES and embeddings\n",
        "train_smiles = [p.smiles_string for p in train_pairs]\n",
        "val_smiles = [p.smiles_string for p in val_pairs]\n",
        "test_smiles = [p.smiles_string for p in test_pairs]\n",
        "\n",
        "train_embs = np.vstack([p.cell_embedding.numpy() for p in train_pairs])\n",
        "val_embs = np.vstack([p.cell_embedding.numpy() for p in val_pairs])\n",
        "test_embs = np.vstack([p.cell_embedding.numpy() for p in test_pairs])\n",
        "\n",
        "# Basic statistics\n",
        "print(f\"Train embeddings: {train_embs.shape}, Val: {val_embs.shape}, Test: {test_embs.shape}\")\n",
        "\n",
        "# ----- PART 1: SMILES ANALYSIS & STRATEGIES -----\n",
        "\n",
        "def analyze_smiles_distribution(train_s, val_s, test_s):\n",
        "    print(\"\\n--- SMILES Distribution Analysis ---\")\n",
        "\n",
        "    # 1. Molecular weight distribution\n",
        "    train_mols = [Chem.MolFromSmiles(s) for s in train_s]\n",
        "    val_mols = [Chem.MolFromSmiles(s) for s in val_s]\n",
        "\n",
        "    train_mw = [Chem.Descriptors.MolWt(m) for m in train_mols if m is not None]\n",
        "    val_mw = [Chem.Descriptors.MolWt(m) for m in val_mols if m is not None]\n",
        "\n",
        "    print(f\"Train MW: {np.mean(train_mw):.2f} ± {np.std(train_mw):.2f}\")\n",
        "    print(f\"Val MW: {np.mean(val_mw):.2f} ± {np.std(val_mw):.2f}\")\n",
        "\n",
        "    # 2. Fingerprint similarity within and between sets\n",
        "    train_fps = [AllChem.GetMorganFingerprintAsBitVect(m, 2, 1024) for m in train_mols if m is not None]\n",
        "    val_fps = [AllChem.GetMorganFingerprintAsBitVect(m, 2, 1024) for m in val_mols if m is not None]\n",
        "\n",
        "    # Sample 100 random pairs for each set for efficiency\n",
        "    def sample_similarity(fps, n=100):\n",
        "        if len(fps) < 2:\n",
        "            return []\n",
        "        pairs = [(i, j) for i in range(len(fps)) for j in range(i+1, len(fps))]\n",
        "        sample_pairs = random.sample(pairs, min(n, len(pairs)))\n",
        "        similarities = []\n",
        "        for i, j in sample_pairs:\n",
        "            sim = DataStructs.TanimotoSimilarity(fps[i], fps[j])\n",
        "            similarities.append(sim)\n",
        "        return similarities\n",
        "\n",
        "    train_internal_sim = sample_similarity(train_fps)\n",
        "    val_internal_sim = sample_similarity(val_fps)\n",
        "\n",
        "    # Cross-similarity between train and val\n",
        "    cross_sim = []\n",
        "    if train_fps and val_fps:\n",
        "        for _ in range(100):\n",
        "            i = random.randint(0, len(train_fps)-1)\n",
        "            j = random.randint(0, len(val_fps)-1)\n",
        "            sim = DataStructs.TanimotoSimilarity(train_fps[i], val_fps[j])\n",
        "            cross_sim.append(sim)\n",
        "\n",
        "    print(f\"Train internal similarity: {np.mean(train_internal_sim):.4f}\")\n",
        "    print(f\"Val internal similarity: {np.mean(val_internal_sim):.4f}\")\n",
        "    print(f\"Train-Val similarity: {np.mean(cross_sim):.4f}\")\n",
        "\n",
        "    # 3. Most common substructures\n",
        "    from rdkit.Chem import MACCSkeys\n",
        "    train_maccs = [MACCSkeys.GenMACCSKeys(m) for m in train_mols if m is not None]\n",
        "    val_maccs = [MACCSkeys.GenMACCSKeys(m) for m in val_mols if m is not None]\n",
        "\n",
        "    # Convert to numpy arrays for analysis\n",
        "    train_maccs_array = np.zeros((len(train_maccs), 167), dtype=np.int8)\n",
        "    for i, fp in enumerate(train_maccs):\n",
        "        arr = np.zeros((0,), dtype=np.int8)\n",
        "        DataStructs.ConvertToNumpyArray(fp, arr)\n",
        "        train_maccs_array[i] = arr\n",
        "\n",
        "    val_maccs_array = np.zeros((len(val_maccs), 167), dtype=np.int8)\n",
        "    for i, fp in enumerate(val_maccs):\n",
        "        arr = np.zeros((0,), dtype=np.int8)\n",
        "        DataStructs.ConvertToNumpyArray(fp, arr)\n",
        "        val_maccs_array[i] = arr\n",
        "\n",
        "    # Substructure frequency difference\n",
        "    train_mean = np.mean(train_maccs_array, axis=0)\n",
        "    val_mean = np.mean(val_maccs_array, axis=0)\n",
        "\n",
        "    # Feature differences\n",
        "    diff = np.abs(train_mean - val_mean)\n",
        "    top_diff_idx = np.argsort(diff)[-5:]\n",
        "\n",
        "    print(\"Top 5 substructure differences (MACCS keys):\")\n",
        "    for idx in top_diff_idx[::-1]:\n",
        "        print(f\"Key {idx}: Train {train_mean[idx]:.2f}, Val {val_mean[idx]:.2f}, Diff {diff[idx]:.2f}\")\n",
        "\n",
        "    return train_mols, val_mols, train_fps, val_fps\n",
        "\n",
        "train_mols, val_mols, train_fps, val_fps = analyze_smiles_distribution(train_smiles, val_smiles, test_smiles)\n",
        "\n",
        "# ----- SMILES AUGMENTATION STRATEGIES -----\n",
        "\n",
        "def generate_smiles_augmentation_strategies():\n",
        "    print(\"\\n--- SMILES Augmentation & Resampling Strategies ---\")\n",
        "\n",
        "    # Strategy 1: SMILES canonicalization\n",
        "    def canonicalize_smiles(smiles_list):\n",
        "        mols = [Chem.MolFromSmiles(s) for s in smiles_list]\n",
        "        return [Chem.MolToSmiles(m, isomericSmiles=True) for m in mols if m is not None]\n",
        "\n",
        "    # Strategy 2: SMILES randomization\n",
        "    def randomize_smiles(smiles_list, n_per_mol=3):\n",
        "        result = []\n",
        "        for s in smiles_list:\n",
        "            mol = Chem.MolFromSmiles(s)\n",
        "            if mol is not None:\n",
        "                for _ in range(n_per_mol):\n",
        "                    random_smiles = Chem.MolToSmiles(mol, doRandom=True, isomericSmiles=True)\n",
        "                    result.append(random_smiles)\n",
        "        return result\n",
        "\n",
        "    # Strategy 3: Fragment the molecules\n",
        "    def fragment_molecules(smiles_list, n_frags=2):\n",
        "        result = []\n",
        "        for s in smiles_list:\n",
        "            mol = Chem.MolFromSmiles(s)\n",
        "            if mol is not None and mol.GetNumAtoms() > 10:\n",
        "                frags = Chem.FragmentOnBonds(mol, [random.randint(0, mol.GetNumBonds()-1) for _ in range(n_frags)])\n",
        "                frag_smiles = Chem.MolToSmiles(frags, isomericSmiles=True)\n",
        "                result.append(frag_smiles)\n",
        "        return result\n",
        "\n",
        "    # Strategy 4: Molecular weight stratification\n",
        "    def stratify_by_mw(train_mols, val_mols):\n",
        "        train_mw = [(i, Chem.Descriptors.MolWt(m)) for i, m in enumerate(train_mols) if m is not None]\n",
        "        val_mw = [(i, Chem.Descriptors.MolWt(m)) for i, m in enumerate(val_mols) if m is not None]\n",
        "\n",
        "        # Create MW bins\n",
        "        all_mw = [mw for _, mw in train_mw + val_mw]\n",
        "        min_mw, max_mw = min(all_mw), max(all_mw)\n",
        "        bins = np.linspace(min_mw, max_mw, 6)\n",
        "\n",
        "        # Bin the molecules\n",
        "        train_bins = [[] for _ in range(5)]\n",
        "        val_bins = [[] for _ in range(5)]\n",
        "\n",
        "        for idx, mw in train_mw:\n",
        "            bin_idx = min(4, int((mw - min_mw) / (max_mw - min_mw) * 5))\n",
        "            train_bins[bin_idx].append(idx)\n",
        "\n",
        "        for idx, mw in val_mw:\n",
        "            bin_idx = min(4, int((mw - min_mw) / (max_mw - min_mw) * 5))\n",
        "            val_bins[bin_idx].append(idx)\n",
        "\n",
        "        print(\"MW stratification bin counts:\")\n",
        "        for i in range(5):\n",
        "            print(f\"Bin {i+1}: Train {len(train_bins[i])}, Val {len(val_bins[i])}\")\n",
        "\n",
        "        return train_bins, val_bins\n",
        "\n",
        "    # Strategy 5: Fingerprint clustering\n",
        "    def cluster_by_fingerprint(train_fps, val_fps, n_clusters=10):\n",
        "        if not train_fps or not val_fps:\n",
        "            return [], []\n",
        "\n",
        "        # Convert to numpy arrays\n",
        "        train_fp_array = np.zeros((len(train_fps), 1024))\n",
        "        for i, fp in enumerate(train_fps):\n",
        "            arr = np.zeros((0,))\n",
        "            DataStructs.ConvertToNumpyArray(fp, arr)\n",
        "            train_fp_array[i] = arr\n",
        "\n",
        "        val_fp_array = np.zeros((len(val_fps), 1024))\n",
        "        for i, fp in enumerate(val_fps):\n",
        "            arr = np.zeros((0,))\n",
        "            DataStructs.ConvertToNumpyArray(fp, arr)\n",
        "            val_fp_array[i] = arr\n",
        "\n",
        "        # Cluster the fingerprints\n",
        "        kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "        train_clusters = kmeans.fit_predict(train_fp_array)\n",
        "        val_clusters = kmeans.predict(val_fp_array)\n",
        "\n",
        "        # Group indices by cluster\n",
        "        train_cluster_indices = [[] for _ in range(n_clusters)]\n",
        "        val_cluster_indices = [[] for _ in range(n_clusters)]\n",
        "\n",
        "        for i, cluster in enumerate(train_clusters):\n",
        "            train_cluster_indices[cluster].append(i)\n",
        "\n",
        "        for i, cluster in enumerate(val_clusters):\n",
        "            val_cluster_indices[cluster].append(i)\n",
        "\n",
        "        print(\"Fingerprint clustering results:\")\n",
        "        for i in range(n_clusters):\n",
        "            print(f\"Cluster {i+1}: Train {len(train_cluster_indices[i])}, Val {len(val_cluster_indices[i])}\")\n",
        "\n",
        "        return train_cluster_indices, val_cluster_indices\n",
        "\n",
        "    # Execute the strategies with small samples for demonstration\n",
        "    sample_train = train_smiles[:100]\n",
        "    sample_val = val_smiles[:100]\n",
        "\n",
        "    print(\"1. Canonicalization - standardizes SMILES representation\")\n",
        "    print(\"2. SMILES randomization - generates multiple valid SMILES for the same molecule\")\n",
        "    print(\"3. Fragmentation - creates smaller molecules from large ones\")\n",
        "\n",
        "    # Execute more complex strategies\n",
        "    print(\"\\n4. Molecular weight stratification:\")\n",
        "    train_bins, val_bins = stratify_by_mw(train_mols, val_mols)\n",
        "\n",
        "    print(\"\\n5. Fingerprint clustering:\")\n",
        "    train_cluster_indices, val_cluster_indices = cluster_by_fingerprint(train_fps, val_fps)\n",
        "\n",
        "    # More strategies to mention\n",
        "    print(\"\\nAdditional strategies:\")\n",
        "    print(\"6. Scaffold-based splitting - ensures diverse chemical scaffolds in all sets\")\n",
        "    print(\"7. Property-based stratification - LogP, HBA, HBD, etc.\")\n",
        "    print(\"8. Augmentation through enumeration of stereoisomers\")\n",
        "    print(\"9. Substructure-based enrichment to balance representation\")\n",
        "    print(\"10. Time-split for temporal datasets or assay-based grouping\")\n",
        "\n",
        "generate_smiles_augmentation_strategies()\n",
        "\n",
        "# ----- PART 2: CELL EMBEDDING ANALYSIS & STRATEGIES -----\n",
        "\n",
        "def analyze_cell_embeddings(train_embs, val_embs, test_embs):\n",
        "    print(\"\\n--- Cell Embedding Analysis ---\")\n",
        "\n",
        "    # 1. Basic statistics\n",
        "    print(f\"Train embeddings: mean={np.mean(train_embs):.6f}, std={np.std(train_embs):.6f}\")\n",
        "    print(f\"Val embeddings: mean={np.mean(val_embs):.6f}, std={np.std(val_embs):.6f}\")\n",
        "\n",
        "    # 2. Dimensionality\n",
        "    print(f\"Embedding dimensionality: {train_embs.shape[1]}\")\n",
        "\n",
        "    # 3. Cosine similarity distribution\n",
        "    def cosine_sim(a, b):\n",
        "        return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
        "\n",
        "    # Sample pairs for efficiency\n",
        "    def sample_emb_pairs(embs, n=1000):\n",
        "        if len(embs) < 2:\n",
        "            return []\n",
        "        pairs = [(i, j) for i in range(len(embs)) for j in range(i+1, len(embs))]\n",
        "        sample_pairs = random.sample(pairs, min(n, len(pairs)))\n",
        "        return sample_pairs\n",
        "\n",
        "    train_pairs = sample_emb_pairs(train_embs)\n",
        "    val_pairs = sample_emb_pairs(val_embs)\n",
        "\n",
        "    train_internal_sim = [cosine_sim(train_embs[i], train_embs[j]) for i, j in train_pairs]\n",
        "    val_internal_sim = [cosine_sim(val_embs[i], val_embs[j]) for i, j in val_pairs]\n",
        "\n",
        "    # Cross-similarity\n",
        "    cross_sim = []\n",
        "    for _ in range(1000):\n",
        "        i = random.randint(0, len(train_embs)-1)\n",
        "        j = random.randint(0, len(val_embs)-1)\n",
        "        sim = cosine_sim(train_embs[i], val_embs[j])\n",
        "        cross_sim.append(sim)\n",
        "\n",
        "    print(f\"Train internal cosine similarity: {np.mean(train_internal_sim):.4f} ± {np.std(train_internal_sim):.4f}\")\n",
        "    print(f\"Val internal cosine similarity: {np.mean(val_internal_sim):.4f} ± {np.std(val_internal_sim):.4f}\")\n",
        "    print(f\"Train-Val cosine similarity: {np.mean(cross_sim):.4f} ± {np.std(cross_sim):.4f}\")\n",
        "\n",
        "    # 4. PCA to check variance explained\n",
        "    pca = PCA()\n",
        "    pca.fit(np.vstack([train_embs[:1000], val_embs[:1000]]))\n",
        "\n",
        "    # Cumulative explained variance\n",
        "    cum_var = np.cumsum(pca.explained_variance_ratio_)\n",
        "    dim_95 = np.argmax(cum_var >= 0.95) + 1\n",
        "    dim_99 = np.argmax(cum_var >= 0.99) + 1\n",
        "\n",
        "    print(f\"Dimensions needed for 95% variance: {dim_95}\")\n",
        "    print(f\"Dimensions needed for 99% variance: {dim_99}\")\n",
        "\n",
        "    return dim_95, dim_99\n",
        "\n",
        "dim_95, dim_99 = analyze_cell_embeddings(train_embs, val_embs, test_embs)\n",
        "\n",
        "# ----- CELL EMBEDDING AUGMENTATION STRATEGIES -----\n",
        "\n",
        "def generate_cell_embedding_strategies(train_embs, val_embs, dim_95):\n",
        "    print(\"\\n--- Cell Embedding Enhancement Strategies ---\")\n",
        "\n",
        "    # Strategy 1: PCA dimensionality reduction\n",
        "    def pca_reduction(train_embs, val_embs, n_components):\n",
        "        pca = PCA(n_components=n_components)\n",
        "        train_pca = pca.fit_transform(train_embs)\n",
        "        val_pca = pca.transform(val_embs)\n",
        "        return train_pca, val_pca\n",
        "\n",
        "    # Strategy 2: UMAP for better neighborhood preservation\n",
        "    def umap_projection(train_embs, val_embs, n_components=2):\n",
        "        reducer = umap.UMAP(n_components=n_components)\n",
        "        train_umap = reducer.fit_transform(train_embs[:1000])  # Sample for speed\n",
        "        return train_umap\n",
        "\n",
        "    # Strategy 3: Clustering embeddings\n",
        "    def cluster_embeddings(train_embs, n_clusters=10):\n",
        "        kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "        clusters = kmeans.fit_predict(train_embs)\n",
        "        cluster_sizes = np.bincount(clusters)\n",
        "        print(f\"Cluster sizes: {cluster_sizes}\")\n",
        "        silhouette = silhouette_score(train_embs, clusters)\n",
        "        print(f\"Silhouette score: {silhouette:.4f}\")\n",
        "        return clusters\n",
        "\n",
        "    # Strategy 4: Standardization\n",
        "    def standardize_embeddings(train_embs, val_embs):\n",
        "        scaler = StandardScaler()\n",
        "        train_std = scaler.fit_transform(train_embs)\n",
        "        val_std = scaler.transform(val_embs)\n",
        "        return train_std, val_std\n",
        "\n",
        "    # Strategy 5: Noise addition for regularization\n",
        "    def add_noise(embeddings, noise_level=0.01):\n",
        "        noise = np.random.normal(0, noise_level, embeddings.shape)\n",
        "        return embeddings + noise\n",
        "\n",
        "    # Execute strategies with samples for demonstration\n",
        "    print(\"1. PCA dimensionality reduction - focus on major variance components\")\n",
        "    train_pca, val_pca = pca_reduction(train_embs[:1000], val_embs[:1000], dim_95)\n",
        "    print(f\"Reduced from {train_embs.shape[1]} to {train_pca.shape[1]} dimensions\")\n",
        "\n",
        "    print(\"\\n2. UMAP projection - preserves local neighborhood structure\")\n",
        "    # train_umap = umap_projection(train_embs, val_embs)\n",
        "    print(\"(UMAP visualization would be generated)\")\n",
        "\n",
        "    print(\"\\n3. Clustering embeddings - group similar cell states\")\n",
        "    clusters = cluster_embeddings(train_embs[:1000])\n",
        "\n",
        "    print(\"\\n4. Standardization - normalize feature scales\")\n",
        "    train_std, val_std = standardize_embeddings(train_embs[:1000], val_embs[:1000])\n",
        "    print(f\"Before: mean={np.mean(train_embs[:1000]):.4f}, std={np.std(train_embs[:1000]):.4f}\")\n",
        "    print(f\"After: mean={np.mean(train_std):.4f}, std={np.std(train_std):.4f}\")\n",
        "\n",
        "    print(\"\\nAdditional strategies:\")\n",
        "    print(\"5. Noise injection - add small Gaussian noise for regularization\")\n",
        "    print(\"6. Feature selection - keep only the most informative dimensions\")\n",
        "    print(\"7. Adversarial examples - generate boundary cases\")\n",
        "    print(\"8. Interpolation between similar embeddings\")\n",
        "    print(\"9. Ensemble of projections - combine multiple dimensionality reductions\")\n",
        "    print(\"10. Mixture model fitting - model cell states as mixtures\")\n",
        "\n",
        "generate_cell_embedding_strategies(train_embs, val_embs, dim_95)\n",
        "\n",
        "# ----- IMPROVED DATA STRATIFICATION -----\n",
        "\n",
        "def stratified_split_recommendations():\n",
        "    print(\"\\n--- Recommended Data Stratification Approaches ---\")\n",
        "\n",
        "    print(\"\\nBased on the analysis, here are the most promising stratification approaches:\")\n",
        "\n",
        "    print(\"\\n1. Combined Fingerprint & Embedding clustering:\")\n",
        "    print(\"   - Cluster molecules by fingerprint similarity\")\n",
        "    print(\"   - Within each cluster, ensure proportional distribution of cell embeddings\")\n",
        "    print(\"   - Ensures chemical diversity and balanced cell state distribution\")\n",
        "\n",
        "    print(\"\\n2. Multi-property stratification:\")\n",
        "    print(\"   - Stratify by molecular weight, logP, and fingerprint clusters\")\n",
        "    print(\"   - Ensures balanced representation of different molecular classes\")\n",
        "\n",
        "    print(\"\\n3. SMILES augmentation with cluster balancing:\")\n",
        "    print(\"   - Generate multiple SMILES representations for underrepresented clusters\")\n",
        "    print(\"   - Ensures more balanced training\")\n",
        "\n",
        "    print(\"\\n4. Size-matched molecular pairs:\")\n",
        "    print(\"   - Ensure similar molecules appear in both training and validation\")\n",
        "    print(\"   - Improves model generalization to similar chemical space\")\n",
        "\n",
        "    print(\"\\n5. Scaffold-based splitting with embedding alignment:\")\n",
        "    print(\"   - Split by scaffold but ensure similar cell embedding distributions\")\n",
        "\n",
        "    print(\"\\nRecommended Implementation Plan:\")\n",
        "    print(\"1. Canonicalize all SMILES\")\n",
        "    print(\"2. Cluster by molecular fingerprints (5-10 clusters)\")\n",
        "    print(\"3. Within each cluster, apply stratified sampling based on embedding PCA\")\n",
        "    print(\"4. Augment training data with randomized SMILES for diversity\")\n",
        "    print(\"5. Validate the split by measuring intra/inter similarity metrics\")\n",
        "\n",
        "    print(\"\\nThis approach should dramatically improve the distributional similarity\")\n",
        "    print(\"between training and validation sets while maintaining independence.\")\n",
        "\n",
        "stratified_split_recommendations()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDEb9rkThpeY"
      },
      "source": [
        "#### split logic take #2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "HmohLjgWuPPV",
        "outputId": "90057086-756c-4ca9-8195-941d42343ca9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total pairs: 27982\n",
            "Train exclusive clusters: {np.int32(0), np.int32(10), np.int32(11), np.int32(14)}\n",
            "Val exclusive clusters: {np.int32(2), np.int32(6)}\n",
            "Test exclusive clusters: {np.int32(9), np.int32(12)}\n",
            "Shared clusters: {np.int32(1), np.int32(3), np.int32(4), np.int32(5), np.int32(7), np.int32(8), np.int32(13)}\n",
            "Initial split ratios - Train: 0.741, Val: 0.106, Test: 0.153\n",
            "Final split: Train 20619, Val 2955, Test 4244\n",
            "\n",
            "Per-Cluster Distribution (Count and Percentage of Total Samples in Each Split):\n",
            "Total Train Samples: 20619, Total Val Samples: 2955, Total Test Samples: 4244\n",
            "Cluster 0: Total 1116, Train 1116 (100.0%), Val 0 (0.0%), Test 0 (0.0%)\n",
            "Cluster 1: Total 533, Train 373 (70.0%), Val 76 (14.3%), Test 84 (15.8%)\n",
            "Cluster 2: Total 1103, Train 0 (0.0%), Val 1103 (100.0%), Test 0 (0.0%)\n",
            "Cluster 3: Total 782, Train 549 (70.2%), Val 127 (16.2%), Test 106 (13.6%)\n",
            "Cluster 4: Total 828, Train 572 (69.1%), Val 116 (14.0%), Test 140 (16.9%)\n",
            "Cluster 5: Total 979, Train 689 (70.4%), Val 131 (13.4%), Test 159 (16.2%)\n",
            "Cluster 6: Total 1048, Train 0 (0.0%), Val 1048 (100.0%), Test 0 (0.0%)\n",
            "Cluster 7: Total 637, Train 454 (71.3%), Val 96 (15.1%), Test 87 (13.7%)\n",
            "Cluster 8: Total 781, Train 545 (69.8%), Val 127 (16.3%), Test 109 (14.0%)\n",
            "Cluster 9: Total 2187, Train 0 (0.0%), Val 0 (0.0%), Test 2187 (100.0%)\n",
            "Cluster 10: Total 9146, Train 9146 (100.0%), Val 0 (0.0%), Test 0 (0.0%)\n",
            "Cluster 11: Total 5481, Train 5481 (100.0%), Val 0 (0.0%), Test 0 (0.0%)\n",
            "Cluster 12: Total 1251, Train 0 (0.0%), Val 0 (0.0%), Test 1251 (100.0%)\n",
            "Cluster 13: Total 824, Train 572 (69.4%), Val 131 (15.9%), Test 121 (14.7%)\n",
            "Cluster 14: Total 1122, Train 1122 (100.0%), Val 0 (0.0%), Test 0 (0.0%)\n",
            "\n",
            "Total clusters: 15\n",
            "Clusters in Train: 11 (73.3%)\n",
            "Clusters in Val: 9 (60.0%)\n",
            "Clusters in Test: 9 (60.0%)\n",
            "Overlapping clusters (Train & Val): 7 (53.8%)\n",
            "Overlapping clusters (Train & Test): 7 (53.8%)\n",
            "Overlapping clusters (Val & Test): 7 (63.6%)\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "a must be 1-dimensional",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-c7226c7ddec1>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[0mtrain_pairs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_pairs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_pairs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_clusters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcluster_sizes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimprove_dataset_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_clusters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_splits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_pairs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_pairs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_pairs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_clusters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcluster_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_clusters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nMetrics Summary:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-c7226c7ddec1>\u001b[0m in \u001b[0;36mevaluate_splits\u001b[0;34m(train_pairs, val_pairs, test_pairs, all_clusters, cluster_sizes, n_clusters)\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;34m\"test_mw_mean\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtest_mw\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m         \u001b[0;34m\"test_mw_std\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtest_mw\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m         \u001b[0;34m\"train_tanimoto_sim\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcompute_tanimoto_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_fps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m         \u001b[0;34m\"val_tanimoto_sim\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcompute_tanimoto_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_fps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0;34m\"test_tanimoto_sim\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcompute_tanimoto_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_fps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-c7226c7ddec1>\u001b[0m in \u001b[0;36mcompute_tanimoto_similarity\u001b[0;34m(fps_list, sample_size)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfps_list\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0msample_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mfps_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfps_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mfps_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfps_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mnumpy/random/mtrand.pyx\u001b[0m in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: a must be 1-dimensional"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1400x800 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from rdkit import Chem\n",
        "from rdkit.Chem import Descriptors\n",
        "from rdkit.Chem import rdFingerprintGenerator\n",
        "import numpy as np\n",
        "from rdkit.DataStructs import ConvertToNumpyArray\n",
        "from sklearn.cluster import KMeans\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "def extract_monomer(smiles):\n",
        "    parts = smiles.split('.')\n",
        "    monomer = [p for p in parts if '[*]' not in p]\n",
        "    return monomer[0] if monomer else smiles\n",
        "\n",
        "def smiles_to_morgan_fp(smiles, radius=3, nBits=512):\n",
        "    monomer = extract_monomer(smiles)\n",
        "    mol = Chem.MolFromSmiles(monomer)\n",
        "    if mol is None:\n",
        "        return None\n",
        "    fp_gen = rdFingerprintGenerator.GetMorganGenerator(radius=radius, fpSize=nBits)\n",
        "    return fp_gen.GetFingerprint(mol)\n",
        "\n",
        "def compute_molecular_weight(smiles):\n",
        "    monomer = extract_monomer(smiles)\n",
        "    mol = Chem.MolFromSmiles(monomer)\n",
        "    if mol is None:\n",
        "        return None\n",
        "    return Descriptors.MolWt(mol)\n",
        "\n",
        "def compute_mw_stats(smiles_list):\n",
        "    weights = [compute_molecular_weight(s) for s in smiles_list]\n",
        "    weights = [w for w in weights if w is not None]\n",
        "    return np.mean(weights) if weights else 0, np.std(weights) if weights else 0\n",
        "\n",
        "def compute_tanimoto_similarity(fps_list, sample_size=1000):\n",
        "    if not fps_list or len(fps_list) < 2:\n",
        "        return 0\n",
        "\n",
        "    if len(fps_list) > sample_size:\n",
        "        fps_sample = np.random.choice(fps_list, sample_size, replace=False)\n",
        "    else:\n",
        "        fps_sample = fps_list\n",
        "\n",
        "    similarities = []\n",
        "    for i in range(len(fps_sample)):\n",
        "        for j in range(i+1, len(fps_sample)):\n",
        "            sim = Chem.DataStructs.TanimotoSimilarity(fps_sample[i], fps_sample[j])\n",
        "            similarities.append(sim)\n",
        "\n",
        "    return np.mean(similarities) if similarities else 0\n",
        "\n",
        "def improve_dataset_split(data_path, output_dir, n_clusters=15):\n",
        "    with open(data_path, 'rb') as f:\n",
        "        all_pairs = pickle.load(f)\n",
        "    print(f\"Total pairs: {len(all_pairs)}\")\n",
        "\n",
        "    valid_indices, fps = [], []\n",
        "    for i, pair in enumerate(all_pairs):\n",
        "        smiles = pair.smiles_string if hasattr(pair, 'smiles_string') else None\n",
        "        if isinstance(smiles, str):\n",
        "            fp = smiles_to_morgan_fp(smiles)\n",
        "            if fp is not None:\n",
        "                fps.append(fp)\n",
        "                valid_indices.append(i)\n",
        "\n",
        "    valid_pairs = [all_pairs[i] for i in valid_indices]\n",
        "    fp_array = np.zeros((len(fps), 512))\n",
        "    for i, fp in enumerate(fps):\n",
        "        ConvertToNumpyArray(fp, fp_array[i])\n",
        "\n",
        "    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
        "    all_clusters = kmeans.fit_predict(fp_array)\n",
        "\n",
        "    cluster_sizes = {}\n",
        "    for c in all_clusters:\n",
        "        if c not in cluster_sizes:\n",
        "            cluster_sizes[c] = 0\n",
        "        cluster_sizes[c] += 1\n",
        "\n",
        "    sorted_clusters = sorted([(k, v) for k, v in cluster_sizes.items()], key=lambda x: x[1], reverse=True)\n",
        "    cluster_size_order = [k for k, v in sorted_clusters]\n",
        "\n",
        "    big_clusters = cluster_size_order[:4]\n",
        "    mid_clusters = cluster_size_order[4:8]\n",
        "    small_clusters = cluster_size_order[8:]\n",
        "\n",
        "    train_exclusive_clusters = set(big_clusters[:2] + mid_clusters[:2])\n",
        "    val_exclusive_clusters = set(mid_clusters[2:4])\n",
        "    test_exclusive_clusters = set(big_clusters[2:4])\n",
        "    shared_clusters = set(small_clusters)\n",
        "\n",
        "    print(f\"Train exclusive clusters: {train_exclusive_clusters}\")\n",
        "    print(f\"Val exclusive clusters: {val_exclusive_clusters}\")\n",
        "    print(f\"Test exclusive clusters: {test_exclusive_clusters}\")\n",
        "    print(f\"Shared clusters: {shared_clusters}\")\n",
        "\n",
        "    cluster_to_indices = {i: [] for i in range(n_clusters)}\n",
        "    for i, cluster in enumerate(all_clusters):\n",
        "        cluster_to_indices[cluster].append(i)  # Store position in valid_pairs list, not original index\n",
        "\n",
        "    train_indices = set()\n",
        "    for cluster in train_exclusive_clusters:\n",
        "        train_indices.update(cluster_to_indices[cluster])\n",
        "\n",
        "    val_indices = set()\n",
        "    for cluster in val_exclusive_clusters:\n",
        "        val_indices.update(cluster_to_indices[cluster])\n",
        "\n",
        "    test_indices = set()\n",
        "    for cluster in test_exclusive_clusters:\n",
        "        test_indices.update(cluster_to_indices[cluster])\n",
        "\n",
        "    shared_indices = []\n",
        "    for cluster in shared_clusters:\n",
        "        shared_indices.extend(cluster_to_indices[cluster])\n",
        "\n",
        "    np.random.shuffle(shared_indices)\n",
        "    total_shared = len(shared_indices)\n",
        "    train_share = int(0.7 * total_shared)\n",
        "    val_share = int(0.15 * total_shared)\n",
        "    test_share = total_shared - train_share - val_share\n",
        "\n",
        "    train_indices.update(shared_indices[:train_share])\n",
        "    val_indices.update(shared_indices[train_share:train_share + val_share])\n",
        "    test_indices.update(shared_indices[train_share + val_share:])\n",
        "\n",
        "    total_valid = len(valid_pairs)\n",
        "    desired_train_ratio = 0.7\n",
        "    desired_val_ratio = 0.15\n",
        "    desired_test_ratio = 0.15\n",
        "\n",
        "    train_ratio = len(train_indices) / total_valid\n",
        "    val_ratio = len(val_indices) / total_valid\n",
        "    test_ratio = len(test_indices) / total_valid\n",
        "\n",
        "    print(f\"Initial split ratios - Train: {train_ratio:.3f}, Val: {val_ratio:.3f}, Test: {test_ratio:.3f}\")\n",
        "\n",
        "    # Make sure we're only using valid indices (within the range of valid_pairs)\n",
        "    train_indices = {i for i in train_indices if i < len(valid_pairs)}\n",
        "    val_indices = {i for i in val_indices if i < len(valid_pairs)}\n",
        "    test_indices = {i for i in test_indices if i < len(valid_pairs)}\n",
        "\n",
        "    # Recalculate ratios\n",
        "    train_ratio = len(train_indices) / total_valid\n",
        "    val_ratio = len(val_indices) / total_valid\n",
        "    test_ratio = len(test_indices) / total_valid\n",
        "\n",
        "    if train_ratio < desired_train_ratio:\n",
        "        remaining = list(set(range(len(valid_pairs))) - train_indices - val_indices - test_indices)\n",
        "        needed = int(desired_train_ratio * total_valid) - len(train_indices)\n",
        "        if needed > 0 and remaining:\n",
        "            train_indices.update(remaining[:needed])\n",
        "\n",
        "    if val_ratio < desired_val_ratio:\n",
        "        remaining = list(set(range(len(valid_pairs))) - train_indices - val_indices - test_indices)\n",
        "        needed = int(desired_val_ratio * total_valid) - len(val_indices)\n",
        "        if needed > 0 and remaining:\n",
        "            val_indices.update(remaining[:needed])\n",
        "\n",
        "    remaining = list(set(range(len(valid_pairs))) - train_indices - val_indices - test_indices)\n",
        "    if remaining:\n",
        "        test_indices.update(remaining)\n",
        "\n",
        "    train_pairs = [(valid_pairs[i], i) for i in train_indices]\n",
        "    val_pairs = [(valid_pairs[i], i) for i in val_indices]\n",
        "    test_pairs = [(valid_pairs[i], i) for i in test_indices]\n",
        "\n",
        "    print(f\"Final split: Train {len(train_pairs)}, Val {len(val_pairs)}, Test {len(test_pairs)}\")\n",
        "\n",
        "    with open(f\"{output_dir}/train_pairs.pkl\", 'wb') as f:\n",
        "        pickle.dump([p[0] for p in train_pairs], f)\n",
        "    with open(f\"{output_dir}/val_pairs.pkl\", 'wb') as f:\n",
        "        pickle.dump([p[0] for p in val_pairs], f)\n",
        "    with open(f\"{output_dir}/test_pairs.pkl\", 'wb') as f:\n",
        "        pickle.dump([p[0] for p in test_pairs], f)\n",
        "    with open(f\"{output_dir}/all_clusters.pkl\", 'wb') as f:\n",
        "        pickle.dump(all_clusters, f)\n",
        "\n",
        "    return train_pairs, val_pairs, test_pairs, all_clusters, cluster_sizes\n",
        "\n",
        "def evaluate_splits(train_pairs, val_pairs, test_pairs, all_clusters, cluster_sizes, n_clusters=15):\n",
        "    train_smiles = [pair[0].smiles_string for pair in train_pairs if hasattr(pair[0], 'smiles_string')]\n",
        "    val_smiles = [pair[0].smiles_string for pair in val_pairs if hasattr(pair[0], 'smiles_string')]\n",
        "    test_smiles = [pair[0].smiles_string for pair in test_pairs if hasattr(pair[0], 'smiles_string')]\n",
        "\n",
        "    train_fps = [smiles_to_morgan_fp(s) for s in train_smiles if isinstance(s, str)]\n",
        "    train_fps = [fp for fp in train_fps if fp is not None]\n",
        "\n",
        "    val_fps = [smiles_to_morgan_fp(s) for s in val_smiles if isinstance(s, str)]\n",
        "    val_fps = [fp for fp in val_fps if fp is not None]\n",
        "\n",
        "    test_fps = [smiles_to_morgan_fp(s) for s in test_smiles if isinstance(s, str)]\n",
        "    test_fps = [fp for fp in test_fps if fp is not None]\n",
        "\n",
        "    train_clusters = np.array([all_clusters[pair[1]] for pair in train_pairs])\n",
        "    val_clusters = np.array([all_clusters[pair[1]] for pair in val_pairs])\n",
        "    test_clusters = np.array([all_clusters[pair[1]] for pair in test_pairs])\n",
        "\n",
        "    total_train = len(train_clusters) if train_clusters.size else 0\n",
        "    total_val = len(val_clusters) if val_clusters.size else 0\n",
        "    total_test = len(test_clusters) if test_clusters.size else 0\n",
        "\n",
        "    train_cluster_dist = {i: 0 for i in range(n_clusters)}\n",
        "    val_cluster_dist = {i: 0 for i in range(n_clusters)}\n",
        "    test_cluster_dist = {i: 0 for i in range(n_clusters)}\n",
        "\n",
        "    if train_clusters.size:\n",
        "        for cluster in train_clusters:\n",
        "            train_cluster_dist[cluster] += 1\n",
        "    if val_clusters.size:\n",
        "        for cluster in val_clusters:\n",
        "            val_cluster_dist[cluster] += 1\n",
        "    if test_clusters.size:\n",
        "        for cluster in test_clusters:\n",
        "            test_cluster_dist[cluster] += 1\n",
        "\n",
        "    print(\"\\nPer-Cluster Distribution (Count and Percentage of Total Samples in Each Split):\")\n",
        "    print(f\"Total Train Samples: {total_train}, Total Val Samples: {total_val}, Total Test Samples: {total_test}\")\n",
        "\n",
        "    train_pcts = []\n",
        "    val_pcts = []\n",
        "    test_pcts = []\n",
        "    cluster_labels = []\n",
        "\n",
        "    for cluster in range(n_clusters):\n",
        "        train_count = train_cluster_dist[cluster]\n",
        "        val_count = val_cluster_dist[cluster]\n",
        "        test_count = test_cluster_dist[cluster]\n",
        "        cluster_total = cluster_sizes.get(cluster, 0)\n",
        "\n",
        "        train_pct = (train_count / cluster_total * 100) if cluster_total > 0 else 0\n",
        "        val_pct = (val_count / cluster_total * 100) if cluster_total > 0 else 0\n",
        "        test_pct = (test_count / cluster_total * 100) if cluster_total > 0 else 0\n",
        "\n",
        "        train_pcts.append(train_pct)\n",
        "        val_pcts.append(val_pct)\n",
        "        test_pcts.append(test_pct)\n",
        "        cluster_labels.append(f\"Cluster {cluster}\")\n",
        "\n",
        "        print(f\"Cluster {cluster}: Total {cluster_total}, Train {train_count} ({train_pct:.1f}%), \"\n",
        "              f\"Val {val_count} ({val_pct:.1f}%), Test {test_count} ({test_pct:.1f}%)\")\n",
        "\n",
        "    train_set = set(train_clusters) if train_clusters.size else set()\n",
        "    val_set = set(val_clusters) if val_clusters.size else set()\n",
        "    test_set = set(test_clusters) if test_clusters.size else set()\n",
        "\n",
        "    train_cluster_count = len(train_set)\n",
        "    val_cluster_count = len(val_set)\n",
        "    test_cluster_count = len(test_set)\n",
        "\n",
        "    union_train_val = len(train_set | val_set) if (train_set or val_set) else 1\n",
        "    union_train_test = len(train_set | test_set) if (train_set or test_set) else 1\n",
        "    union_val_test = len(val_set | test_set) if (val_set or test_set) else 1\n",
        "\n",
        "    overlap_train_val = len(train_set & val_set)\n",
        "    overlap_train_test = len(train_set & test_set)\n",
        "    overlap_val_test = len(val_set & test_set)\n",
        "\n",
        "    print(f\"\\nTotal clusters: {n_clusters}\")\n",
        "    print(f\"Clusters in Train: {train_cluster_count} ({train_cluster_count/n_clusters*100:.1f}%)\")\n",
        "    print(f\"Clusters in Val: {val_cluster_count} ({val_cluster_count/n_clusters*100:.1f}%)\")\n",
        "    print(f\"Clusters in Test: {test_cluster_count} ({test_cluster_count/n_clusters*100:.1f}%)\")\n",
        "    print(f\"Overlapping clusters (Train & Val): {overlap_train_val} ({overlap_train_val/union_train_val*100:.1f}%)\")\n",
        "    print(f\"Overlapping clusters (Train & Test): {overlap_train_test} ({overlap_train_test/union_train_test*100:.1f}%)\")\n",
        "    print(f\"Overlapping clusters (Val & Test): {overlap_val_test} ({overlap_val_test/union_val_test*100:.1f}%)\")\n",
        "\n",
        "    # Plot cluster distribution\n",
        "    plt.figure(figsize=(14, 8))\n",
        "    width = 0.25\n",
        "    x = np.arange(len(cluster_labels))\n",
        "\n",
        "    plt.bar(x - width, train_pcts, width, label='Train')\n",
        "    plt.bar(x, val_pcts, width, label='Validation')\n",
        "    plt.bar(x + width, test_pcts, width, label='Test')\n",
        "\n",
        "    plt.xticks(x, cluster_labels, rotation=45)\n",
        "    plt.ylabel('Percentage of Cluster in Split')\n",
        "    plt.title('Distribution of Clusters Across Train/Val/Test Splits')\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"{output_dir}/cluster_distribution.png\")\n",
        "\n",
        "    # Calculate molecular weight distributions\n",
        "    train_mw = [compute_molecular_weight(s) for s in train_smiles if isinstance(s, str)]\n",
        "    train_mw = [mw for mw in train_mw if mw is not None]\n",
        "\n",
        "    val_mw = [compute_molecular_weight(s) for s in val_smiles if isinstance(s, str)]\n",
        "    val_mw = [mw for mw in val_mw if mw is not None]\n",
        "\n",
        "    test_mw = [compute_molecular_weight(s) for s in test_smiles if isinstance(s, str)]\n",
        "    test_mw = [mw for mw in test_mw if mw is not None]\n",
        "\n",
        "    # Plot molecular weight distributions\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.kdeplot(train_mw, label='Train', fill=True, alpha=0.3)\n",
        "    sns.kdeplot(val_mw, label='Validation', fill=True, alpha=0.3)\n",
        "    sns.kdeplot(test_mw, label='Test', fill=True, alpha=0.3)\n",
        "    plt.xlabel('Molecular Weight')\n",
        "    plt.ylabel('Density')\n",
        "    plt.title('Molecular Weight Distribution Across Splits')\n",
        "    plt.legend()\n",
        "    plt.savefig(f\"{output_dir}/molecular_weight_distribution.png\")\n",
        "\n",
        "    metrics = {\n",
        "        \"train_size\": total_train,\n",
        "        \"val_size\": total_val,\n",
        "        \"test_size\": total_test,\n",
        "        \"train_mw_mean\": np.mean(train_mw) if train_mw else 0,\n",
        "        \"train_mw_std\": np.std(train_mw) if train_mw else 0,\n",
        "        \"val_mw_mean\": np.mean(val_mw) if val_mw else 0,\n",
        "        \"val_mw_std\": np.std(val_mw) if val_mw else 0,\n",
        "        \"test_mw_mean\": np.mean(test_mw) if test_mw else 0,\n",
        "        \"test_mw_std\": np.std(test_mw) if test_mw else 0,\n",
        "        \"train_tanimoto_sim\": compute_tanimoto_similarity(train_fps),\n",
        "        \"val_tanimoto_sim\": compute_tanimoto_similarity(val_fps),\n",
        "        \"test_tanimoto_sim\": compute_tanimoto_similarity(test_fps),\n",
        "        \"train_val_cluster_overlap\": overlap_train_val / union_train_val if union_train_val else 0,\n",
        "        \"train_test_cluster_overlap\": overlap_train_test / union_train_test if union_train_test else 0,\n",
        "        \"val_test_cluster_overlap\": overlap_val_test / union_val_test if union_val_test else 0\n",
        "    }\n",
        "\n",
        "    return metrics\n",
        "\n",
        "data_path = \"/content/drive/MyDrive/Colab Notebooks/esm cell state/scgpt_workdir/processed_pairs.pkl\"\n",
        "output_dir = \"/content/drive/MyDrive/Colab Notebooks/esm cell state/scgpt_workdir_fixed_1\"\n",
        "\n",
        "train_pairs, val_pairs, test_pairs, all_clusters, cluster_sizes = improve_dataset_split(data_path, output_dir, n_clusters=15)\n",
        "metrics = evaluate_splits(train_pairs, val_pairs, test_pairs, all_clusters, cluster_sizes, n_clusters=15)\n",
        "\n",
        "print(\"\\nMetrics Summary:\")\n",
        "for key, value in metrics.items():\n",
        "    print(f\"{key}: {value:.4f}\" if isinstance(value, float) else f\"{key}: {value}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tWBc9Avxix9-",
        "outputId": "1514fa17-90cf-4762-ad34-ccf906e73e26"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ls: cannot access '/content/drive/MyDrive/Colab Notebooks/esm cell state/scgpt_workdir/processed_pairs.pkl': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!ls  \"/content/drive/MyDrive/Colab Notebooks/esm cell state/scgpt_workdir/processed_pairs.pkl\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H53EyxvIleEx",
        "outputId": "f3928f3e-a31e-472b-bc6c-dba25e2a5e4f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Representative SMILES for each cluster:\n",
            "\n",
            "Cluster 0:\n",
            "  Example 1: CC(C)OC1=NC=C(C=C1)C2=NNC3=C2C=C(C=C3)NC(=O)[C@@]4(CCN(C4)CC(=O)N5CCC(=CC5)C6=CC=C(C=C6)C7=NN(C=N7)C)SC\n",
            "  Example 2: C1CC(C1)(C2=CC=C(C=C2)C3=C(N4C(=N3)C=CC(=N4)C(=O)N)C5=CC=CC=C5)N\n",
            "\n",
            "Cluster 1:\n",
            "  Example 1: CCC1=C2CN3C(=CC4=C(C3=O)COC(=O)[C@@]4(CC)O)C2=NC5=C1C=C(C=C5)OC(=O)N6CCC(CC6)N7CCCCC7\n",
            "\n",
            "Cluster 2:\n",
            "  Example 1: CN(C)C/C=C/C(=O)NC1=C(C=C2C(=C1)C(=NC=N2)NC3=CC(=C(C=C3)F)Cl)O[C@H]4CCOC4\n",
            "  Example 2: CC1=C(C=CC(=C1)NC2=NC=NC3=C2C=C(C=C3)NC4=NC(CO4)(C)C)OC5=CC6=NC=NN6C=C5\n",
            "\n",
            "Cluster 3:\n",
            "  Example 1: C[C@@H](CNC1=NC=CC(=N1)C2=CN(N=C2C3=C(C(=CC(=C3)Cl)NS(=O)(=O)C)F)C(C)C)NC(=O)OC\n",
            "\n",
            "Cluster 4:\n",
            "  Example 1: CC(C)(C)C1=NC(=C(S1)C2=NC(=NC=C2)N)C3=C(C(=CC=C3)NS(=O)(=O)C4=C(C=CC=C4F)F)F\n",
            "\n",
            "Cluster 5:\n",
            "  Example 1: CCS(=O)(=O)C1=CC=CC(=C1)C2=CC(=C(C3=C2C4=C(N3)N=CC(=C4)C)C)C(=O)NC5CCN(CC5)C\n",
            "  Example 2: C1CNCCC1NC(=O)C2=C(C=NN2)NC(=O)C3=C(C=CC=C3Cl)Cl\n",
            "\n",
            "Cluster 6:\n",
            "  Example 1: C1COC2=C(O1)C=CC(=C2)N3C(=O)NN=C3SC4=NC=C(S4)[N+](=O)[O-]\n",
            "  Example 2: C1=CC2=NC=CC(=C2C=C1/C=C\\3/C(=O)NC(=O)S3)C4=CC=NC=C4\n",
            "\n",
            "Cluster 7:\n",
            "  Example 1: CCN1C2=C3C=C(C=C2)C4=CSC(=N4)C[C@@H](C(=O)N5CCC[C@H](N5)C(=O)OCC(CC3=C1C6=C(N=CC(=C6)N7CCN(CC7)C)[C@H](C)OC)(C)C)NC(=O)[C@H]8C[C@@H]8C\n",
            "\n",
            "Cluster 8:\n",
            "  Example 1: C[C@@H]1CC[C@H]2C[C@@H](/C(=C/C=C/C=C/[C@H](C[C@H](C(=O)[C@@H]([C@@H](/C(=C/[C@H](C(=O)C[C@H](OC(=O)[C@@H]3CCCCN3C(=O)C(=O)[C@@]1(O2)O)[C@H](C)C[C@@H]4CC[C@H]([C@@H](C4)OC)OCCO)C)/C)O)OC)C)C)/C)OC\n",
            "\n",
            "Cluster 9:\n",
            "  Example 1: CCN1CCN(CC1)CC2=C(C=C(C=C2)NC(=O)C3=CC(=C(C=C3)C)OC4=C5C=CNC5=NC=C4)C(F)(F)F\n",
            "  Example 2: CCCS(=O)(=O)NC1=C(C(=C(C=C1)F)C(=O)C2=CNC3=C2C=C(C=N3)C4=CC=C(C=C4)Cl)F\n",
            "\n",
            "Cluster 10:\n",
            "  Example 1: CCN1CCN(CC1)C2=CC=C(C=C2)NC3=CC(=NC=N3)N(C)C(=O)NC4=C(C(=CC(=C4Cl)OC)OC)Cl\n",
            "  Example 2: CC1=C(SC(=N1)NC(=O)N2CCC[C@H]2C(=O)N)C3=CC(=NC=C3)C(C)(C)C(F)(F)F\n",
            "\n",
            "Cluster 11:\n",
            "  Example 1: C[C@H]1COCCN1C2=NC(=NC3=C2C=CC(=N3)C4=CC(=C(C=C4)OC)CO)N5CCOC[C@@H]5C\n",
            "  Example 2: CN1C=C(C=N1)C2=NC=CC(=C2)OC3=C(C=C(C(=C3)F)NC(=O)C4(CC4)C(=O)NC5=CC=CC=C5)F\n",
            "\n",
            "Cluster 12:\n",
            "  Example 1: C1CCN(CC1)C(=O)N2CCN3C=C(C4=CC(=CC(=C43)C2)F)C5=C(C(=O)NC5=O)C6=CN=C7N6C=CC=C7\n",
            "  Example 2: CN1C=C(C2=CC=CC=C21)C3=C(C(=O)NC3=O)C4=CC(=CC=C4)N\n",
            "\n",
            "Cluster 13:\n",
            "  Example 1: CC1=C2C(=C(N(C1=O)C)NC3=C(C=C(C=C3)I)F)C(=O)N(C(=O)N2C4=CC=CC(=C4)NC(=O)C)C5CC5\n",
            "  Example 2: CN1C2=C(C(=C(C1=O)F)NC3=C(C=C(C=C3)I)F)C(=O)N(C=N2)C[C@H](CO)O\n",
            "\n",
            "Cluster 14:\n",
            "  Example 1: CC(C)N1C2=C(C=NC3=CC(=C(C=C32)C4=CN=C(C=C4)OCCCN5CCCCC5)F)N(C1=O)C\n",
            "  Example 2: C[C@@H]1C[C@H](C2=C1C(=NC=N2)N3CCN(CC3)C(=O)[C@H](CNC(C)C)C4=CC=C(C=C4)Cl)O\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "import random\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import Draw\n",
        "\n",
        "def extract_example_smiles_from_clusters(data_path, all_clusters_path, n_samples=3):\n",
        "    # Load the data\n",
        "    with open(data_path, 'rb') as f:\n",
        "        all_pairs = pickle.load(f)\n",
        "\n",
        "    # Load cluster assignments\n",
        "    with open(all_clusters_path, 'rb') as f:\n",
        "        all_clusters = pickle.load(f)\n",
        "\n",
        "    # Get valid indices (same as in your main code)\n",
        "    valid_indices = []\n",
        "    for i, pair in enumerate(all_pairs):\n",
        "        smiles = pair.smiles_string if hasattr(pair, 'smiles_string') else None\n",
        "        if isinstance(smiles, str):\n",
        "            try:\n",
        "                mol = Chem.MolFromSmiles(smiles)\n",
        "                if mol is not None:\n",
        "                    valid_indices.append(i)\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "    # Create a mapping of valid index positions to cluster assignments\n",
        "    valid_clusters = all_clusters[:len(valid_indices)]\n",
        "\n",
        "    # Group SMILES by cluster\n",
        "    cluster_to_smiles = {}\n",
        "    for i, cluster in enumerate(valid_clusters):\n",
        "        if cluster not in cluster_to_smiles:\n",
        "            cluster_to_smiles[cluster] = []\n",
        "\n",
        "        smiles = all_pairs[valid_indices[i]].smiles_string if hasattr(all_pairs[valid_indices[i]], 'smiles_string') else None\n",
        "        if smiles:\n",
        "            cluster_to_smiles[cluster].append(smiles)\n",
        "\n",
        "    # Extract unique representative SMILES for each cluster\n",
        "    cluster_representatives = {}\n",
        "    for cluster, smiles_list in cluster_to_smiles.items():\n",
        "        # Get a random sample of unique SMILES (up to n_samples)\n",
        "        unique_smiles = list(set(smiles_list))\n",
        "        if len(unique_smiles) > n_samples:\n",
        "            samples = random.sample(unique_smiles, n_samples)\n",
        "        else:\n",
        "            samples = unique_smiles\n",
        "\n",
        "        cluster_representatives[cluster] = samples\n",
        "\n",
        "    return cluster_representatives\n",
        "\n",
        "def print_cluster_smiles(cluster_representatives):\n",
        "    print(\"Representative SMILES for each cluster:\")\n",
        "    for cluster, smiles_list in sorted(cluster_representatives.items()):\n",
        "        print(f\"\\nCluster {cluster}:\")\n",
        "        for i, smiles in enumerate(smiles_list):\n",
        "            print(f\"  Example {i+1}: {smiles}\")\n",
        "\n",
        "# Specify your paths\n",
        "data_path = \"/content/drive/MyDrive/Colab Notebooks/esm cell state/scgpt_workdir/processed_pairs.pkl\"\n",
        "all_clusters_path = \"/content/drive/MyDrive/Colab Notebooks/esm cell state/scgpt_workdir_fixed_1/all_clusters.pkl\"\n",
        "\n",
        "# Extract and display representative SMILES from each cluster\n",
        "cluster_representatives = extract_example_smiles_from_clusters(data_path, all_clusters_path, n_samples=2)\n",
        "print_cluster_smiles(cluster_representatives)\n",
        "\n",
        "# Optional: visualize molecules from specific clusters\n",
        "def visualize_cluster_molecules(cluster_num, smiles_list, n_to_show=5):\n",
        "    if len(smiles_list) > n_to_show:\n",
        "        samples = random.sample(smiles_list, n_to_show)\n",
        "    else:\n",
        "        samples = smiles_list\n",
        "\n",
        "    mols = [Chem.MolFromSmiles(s) for s in samples]\n",
        "    valid_mols = [m for m in mols if m is not None]\n",
        "\n",
        "    if valid_mols:\n",
        "        img = Draw.MolsToGridImage(valid_mols, molsPerRow=3, subImgSize=(200, 200),\n",
        "                                  legends=[f\"Mol {i+1}\" for i in range(len(valid_mols))])\n",
        "        return img\n",
        "    return None\n",
        "\n",
        "# Example usage:\n",
        "# Uncomment to visualize molecules from a specific cluster\n",
        "# For cluster 0 (Train exclusive)\n",
        "# img = visualize_cluster_molecules(0, cluster_representatives[0])\n",
        "# display(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "z-L2BTZOuPRr",
        "outputId": "8a5a1755-6e96-46e4-c2e9-b341340b3f1c"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1500x500 with 3 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of genes with significant differences (p < 0.05):\n",
            "Train vs Val: 481 (93.9%)\n",
            "Train vs Test: 432 (84.4%)\n",
            "Val vs Test: 437 (85.4%)\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1500x500 with 3 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pickle\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from scipy.stats import ks_2samp\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "with open(\"/content/drive/MyDrive/Colab Notebooks/esm cell state/scgpt_workdir_fixed_1/train_pairs.pkl\", 'rb') as f:\n",
        "    train_pairs = pickle.load(f)\n",
        "with open(\"/content/drive/MyDrive/Colab Notebooks/esm cell state/scgpt_workdir_fixed_1/val_pairs.pkl\", 'rb') as f:\n",
        "    val_pairs = pickle.load(f)\n",
        "with open(\"/content/drive/MyDrive/Colab Notebooks/esm cell state/scgpt_workdir_fixed_1/test_pairs.pkl\", 'rb') as f:\n",
        "    test_pairs = pickle.load(f)\n",
        "\n",
        "train_embeddings = np.array([pair.cell_embedding for pair in train_pairs if hasattr(pair, 'cell_embedding')])\n",
        "val_embeddings = np.array([pair.cell_embedding for pair in val_pairs if hasattr(pair, 'cell_embedding')])\n",
        "test_embeddings = np.array([pair.cell_embedding for pair in test_pairs if hasattr(pair, 'cell_embedding')])\n",
        "\n",
        "train_cos_sim = cosine_similarity(train_embeddings)\n",
        "val_cos_sim = cosine_similarity(val_embeddings)\n",
        "test_cos_sim = cosine_similarity(test_embeddings)\n",
        "\n",
        "plt.figure(figsize=(15, 5))\n",
        "plt.subplot(1, 3, 1)\n",
        "sns.histplot(train_cos_sim[np.triu_indices(len(train_cos_sim), k=1)], bins=50, color='blue', label='Train')\n",
        "plt.title('Train Cosine Similarity')\n",
        "plt.subplot(1, 3, 2)\n",
        "sns.histplot(val_cos_sim[np.triu_indices(len(val_cos_sim), k=1)], bins=50, color='green', label='Val')\n",
        "plt.title('Val Cosine Similarity')\n",
        "plt.subplot(1, 3, 3)\n",
        "sns.histplot(test_cos_sim[np.triu_indices(len(test_cos_sim), k=1)], bins=50, color='red', label='Test')\n",
        "plt.title('Test Cosine Similarity')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "num_genes = train_embeddings.shape[1]\n",
        "ks_results = {'train_val': [], 'train_test': [], 'val_test': []}\n",
        "for gene_idx in range(num_genes):\n",
        "    train_gene = train_embeddings[:, gene_idx]\n",
        "    val_gene = val_embeddings[:, gene_idx]\n",
        "    test_gene = test_embeddings[:, gene_idx]\n",
        "\n",
        "    ks_stat, p_val = ks_2samp(train_gene, val_gene)\n",
        "    ks_results['train_val'].append((ks_stat, p_val))\n",
        "\n",
        "    ks_stat, p_val = ks_2samp(train_gene, test_gene)\n",
        "    ks_results['train_test'].append((ks_stat, p_val))\n",
        "\n",
        "    ks_stat, p_val = ks_2samp(val_gene, test_gene)\n",
        "    ks_results['val_test'].append((ks_stat, p_val))\n",
        "\n",
        "significant_genes = {\n",
        "    'train_val': sum(1 for _, p in ks_results['train_val'] if p < 0.05),\n",
        "    'train_test': sum(1 for _, p in ks_results['train_test'] if p < 0.05),\n",
        "    'val_test': sum(1 for _, p in ks_results['val_test'] if p < 0.05)\n",
        "}\n",
        "\n",
        "print(\"Number of genes with significant differences (p < 0.05):\")\n",
        "print(f\"Train vs Val: {significant_genes['train_val']} ({significant_genes['train_val']/num_genes*100:.1f}%)\")\n",
        "print(f\"Train vs Test: {significant_genes['train_test']} ({significant_genes['train_test']/num_genes*100:.1f}%)\")\n",
        "print(f\"Val vs Test: {significant_genes['val_test']} ({significant_genes['val_test']/num_genes*100:.1f}%)\")\n",
        "\n",
        "gene_idx = 0\n",
        "plt.figure(figsize=(15, 5))\n",
        "plt.subplot(1, 3, 1)\n",
        "sns.histplot(train_embeddings[:, gene_idx], bins=50, color='blue', label='Train', stat='density')\n",
        "sns.histplot(val_embeddings[:, gene_idx], bins=50, color='green', label='Val', stat='density', alpha=0.5)\n",
        "plt.title(f'Gene {gene_idx} Distribution (Train vs Val)')\n",
        "plt.legend()\n",
        "plt.subplot(1, 3, 2)\n",
        "sns.histplot(train_embeddings[:, gene_idx], bins=50, color='blue', label='Train', stat='density')\n",
        "sns.histplot(test_embeddings[:, gene_idx], bins=50, color='red', label='Test', stat='density', alpha=0.5)\n",
        "plt.title(f'Gene {gene_idx} Distribution (Train vs Test)')\n",
        "plt.legend()\n",
        "plt.subplot(1, 3, 3)\n",
        "sns.histplot(val_embeddings[:, gene_idx], bins=50, color='green', label='Val', stat='density')\n",
        "sns.histplot(test_embeddings[:, gene_idx], bins=50, color='red', label='Test', stat='density', alpha=0.5)\n",
        "plt.title(f'Gene {gene_idx} Distribution (Val vs Test)')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "non_zero_train = np.sum(train_embeddings != 0, axis=0) / len(train_embeddings)\n",
        "non_zero_val = np.sum(val_embeddings != 0, axis=0) / len(val_embeddings)\n",
        "non_zero_test = np.sum(test_embeddings != 0, axis=0) / len(test_embeddings)\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(non_zero_train, label='Train', color='blue')\n",
        "plt.plot(non_zero_val, label='Val', color='green')\n",
        "plt.plot(non_zero_test, label='Test', color='red')\n",
        "plt.title('Fraction of Non-Zero Values per Gene')\n",
        "plt.xlabel('Gene Index')\n",
        "plt.ylabel('Fraction Non-Zero')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yxL3vppBuPTh"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xDqPUGjIuPV2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e2KLsXxzuPX7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kW27O2vauPaP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pv-nNeYQuPcW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UNX_OPvJuPec"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r0jSoCE1uPgx"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6h6e-GCiuPjH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B2SJaLhPuPlM"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ALhXfHWHuPni"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QmWL2B9Wirbx"
      },
      "outputs": [],
      "source": [
        "n_clusters=10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "S6cECyI-VYhg",
        "outputId": "fe7543d7-c15e-4083-f912-62b061b7bb95"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-120-dd42f27253c9>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[0mtest_pairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_splits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_pairs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_pairs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_pairs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_clusters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{key}: {value:.4f}\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34mf\"{key}: {value}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-120-dd42f27253c9>\u001b[0m in \u001b[0;36mevaluate_splits\u001b[0;34m(train_pairs, val_pairs, test_pairs, n_clusters)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0mtest_fps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msmiles_to_morgan_fp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_smiles\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0msmiles_to_morgan_fp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m     \u001b[0mtrain_clusters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcluster_fingerprints\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_fps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_clusters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m     \u001b[0mval_clusters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcluster_fingerprints\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_fps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_clusters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0mtest_clusters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcluster_fingerprints\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_fps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_clusters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-120-dd42f27253c9>\u001b[0m in \u001b[0;36mcluster_fingerprints\u001b[0;34m(fps, n_clusters)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mn_clusters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfps_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0mkmeans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_init\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkmeans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfps_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkmeans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster_centers_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/cluster/_kmeans.py\u001b[0m in \u001b[0;36mfit_predict\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1068\u001b[0m             \u001b[0mIndex\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mcluster\u001b[0m \u001b[0meach\u001b[0m \u001b[0msample\u001b[0m \u001b[0mbelongs\u001b[0m \u001b[0mto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1069\u001b[0m         \"\"\"\n\u001b[0;32m-> 1070\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1071\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1072\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1471\u001b[0m                 )\n\u001b[1;32m   1472\u001b[0m             ):\n\u001b[0;32m-> 1473\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1475\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/cluster/_kmeans.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1518\u001b[0m             \u001b[0;31m# run a k-means once\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1519\u001b[0;31m             labels, inertia, centers, n_iter_ = kmeans_single(\n\u001b[0m\u001b[1;32m   1520\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1521\u001b[0m                 \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0mcontroller\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_threadpool_controller\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mcontroller\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlimits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlimits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_api\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muser_api\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/cluster/_kmeans.py\u001b[0m in \u001b[0;36m_kmeans_single_lloyd\u001b[0;34m(X, sample_weight, centers_init, max_iter, verbose, tol, n_threads)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m         lloyd_iter(\n\u001b[0m\u001b[1;32m    708\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m_k_means_lloyd.pyx\u001b[0m in \u001b[0;36msklearn.cluster._k_means_lloyd.lloyd_iter_chunked_dense\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m_k_means_common.pyx\u001b[0m in \u001b[0;36msklearn.cluster._k_means_common._relocate_empty_clusters_dense\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/_core/multiarray.py\u001b[0m in \u001b[0;36mwhere\u001b[0;34m(condition, x, y)\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 361\u001b[0;31m \u001b[0;34m@\u001b[0m\u001b[0marray_function_from_c_func_and_dispatcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_multiarray_umath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    362\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m     \"\"\"\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pickle\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import AllChem, Descriptors\n",
        "from rdkit.Chem import rdFingerprintGenerator\n",
        "from rdkit.DataStructs import ConvertToNumpyArray, BulkTanimotoSimilarity\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import numpy as np\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import Descriptors\n",
        "from rdkit.Chem import rdFingerprintGenerator\n",
        "from rdkit.DataStructs import ConvertToNumpyArray, BulkTanimotoSimilarity\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "def extract_monomer(smiles):\n",
        "    if smiles.startswith('[*]') and smiles.endswith('[*]'):\n",
        "        return smiles[3:-3]\n",
        "    return smiles\n",
        "\n",
        "def smiles_to_morgan_fp(smiles, radius=2, nBits=1024):\n",
        "    monomer = extract_monomer(smiles)\n",
        "    mol = Chem.MolFromSmiles(monomer)\n",
        "    if mol is None:\n",
        "        return None\n",
        "    fp_gen = rdFingerprintGenerator.GetMorganGenerator(radius=radius, fpSize=nBits)\n",
        "    return fp_gen.GetFingerprint(mol)\n",
        "\n",
        "def compute_tanimoto_similarity(fps):\n",
        "    fps = [fp for fp in fps if fp is not None]\n",
        "    if len(fps) < 2:\n",
        "        return 0\n",
        "    tanimoto_sim = []\n",
        "    for i in range(len(fps)):\n",
        "        sims = BulkTanimotoSimilarity(fps[i], fps[i+1:])\n",
        "        tanimoto_sim.extend(sims)\n",
        "    return np.mean(tanimoto_sim) if tanimoto_sim else 0\n",
        "\n",
        "def compute_mw_stats(smiles_list):\n",
        "    mws = [Descriptors.MolWt(Chem.MolFromSmiles(extract_monomer(smiles))) for smiles in smiles_list if Chem.MolFromSmiles(extract_monomer(smiles))]\n",
        "    return np.mean(mws) if mws else 0, np.std(mws) if mws else 0\n",
        "\n",
        "def cluster_fingerprints(fps, n_clusters=20):\n",
        "    valid_fps = [fp for fp in fps if fp is not None]\n",
        "    if not valid_fps:\n",
        "        return np.array([]), None\n",
        "    fps_array = np.zeros((len(valid_fps), 1024))\n",
        "    for i, fp in enumerate(valid_fps):\n",
        "        ConvertToNumpyArray(fp, fps_array[i])\n",
        "    if len(fps_array) < n_clusters:\n",
        "        n_clusters = len(fps_array)\n",
        "    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
        "    labels = kmeans.fit_predict(fps_array)\n",
        "    return labels, kmeans.cluster_centers_\n",
        "\n",
        "def evaluate_splits(train_pairs, val_pairs, test_pairs, n_clusters=10):\n",
        "    # Compute fingerprints and cluster assignments\n",
        "    train_smiles = [pair.smiles_string for pair in train_pairs if hasattr(pair, 'smiles_string')]\n",
        "    val_smiles = [pair.smiles_string for pair in val_pairs if hasattr(pair, 'smiles_string')]\n",
        "    test_smiles = [pair.smiles_string for pair in test_pairs if hasattr(pair, 'smiles_string')]\n",
        "\n",
        "    train_fps = [smiles_to_morgan_fp(s) for s in train_smiles if smiles_to_morgan_fp(s) is not None]\n",
        "    val_fps = [smiles_to_morgan_fp(s) for s in val_smiles if smiles_to_morgan_fp(s) is not None]\n",
        "    test_fps = [smiles_to_morgan_fp(s) for s in test_smiles if smiles_to_morgan_fp(s) is not None]\n",
        "\n",
        "    train_clusters, _ = cluster_fingerprints(train_fps, n_clusters)\n",
        "    val_clusters, _ = cluster_fingerprints(val_fps, n_clusters)\n",
        "    test_clusters, _ = cluster_fingerprints(test_fps, n_clusters)\n",
        "\n",
        "    # Total samples in each split\n",
        "    total_train = len(train_clusters) if train_clusters.size else 0\n",
        "    total_val = len(val_clusters) if val_clusters.size else 0\n",
        "    total_test = len(test_clusters) if test_clusters.size else 0\n",
        "\n",
        "    # Initialize cluster distribution dictionaries\n",
        "    train_cluster_dist = {i: 0 for i in range(n_clusters)}\n",
        "    val_cluster_dist = {i: 0 for i in range(n_clusters)}\n",
        "    test_cluster_dist = {i: 0 for i in range(n_clusters)}\n",
        "\n",
        "    # Count samples per cluster in each split\n",
        "    if train_clusters.size:\n",
        "        for cluster in train_clusters:\n",
        "            train_cluster_dist[cluster] += 1\n",
        "    if val_clusters.size:\n",
        "        for cluster in val_clusters:\n",
        "            val_cluster_dist[cluster] += 1\n",
        "    if test_clusters.size:\n",
        "        for cluster in test_clusters:\n",
        "            test_cluster_dist[cluster] += 1\n",
        "\n",
        "    # Print per-cluster distribution\n",
        "    print(\"\\nPer-Cluster Distribution (Count and Percentage of Total Samples in Each Split):\")\n",
        "    print(f\"Total Train Samples: {total_train}, Total Val Samples: {total_val}, Total Test Samples: {total_test}\")\n",
        "    for cluster in range(n_clusters):\n",
        "        train_count = train_cluster_dist[cluster]\n",
        "        val_count = val_cluster_dist[cluster]\n",
        "        test_count = test_cluster_dist[cluster]\n",
        "        train_pct = (train_count / total_train * 100) if total_train > 0 else 0\n",
        "        val_pct = (val_count / total_val * 100) if total_val > 0 else 0\n",
        "        test_pct = (test_count / total_test * 100) if total_test > 0 else 0\n",
        "        print(f\"Cluster {cluster + 1}: Train {train_count} ({train_pct:.1f}%), Val {val_count} ({val_pct:.1f}%), Test {test_count} ({test_pct:.1f}%)\")\n",
        "\n",
        "    # Compute overall overlap metrics\n",
        "    train_set = set(train_clusters) if train_clusters.size else set()\n",
        "    val_set = set(val_clusters) if val_clusters.size else set()\n",
        "    test_set = set(test_clusters) if test_clusters.size else set()\n",
        "\n",
        "    train_cluster_count = len(train_set)\n",
        "    val_cluster_count = len(val_set)\n",
        "    test_cluster_count = len(test_set)\n",
        "\n",
        "    union_train_val = len(train_set | val_set) if (train_set or val_set) else 1\n",
        "    union_train_test = len(train_set | test_set) if (train_set or test_set) else 1\n",
        "    union_val_test = len(val_set | test_set) if (val_set or test_set) else 1\n",
        "\n",
        "    overlap_train_val = len(train_set & val_set)\n",
        "    overlap_train_test = len(train_set & test_set)\n",
        "    overlap_val_test = len(val_set & test_set)\n",
        "\n",
        "    print(f\"\\nTotal clusters: {n_clusters}\")\n",
        "    print(f\"Clusters in Train: {train_cluster_count} ({train_cluster_count/n_clusters*100:.1f}%)\")\n",
        "    print(f\"Clusters in Val: {val_cluster_count} ({val_cluster_count/n_clusters*100:.1f}%)\")\n",
        "    print(f\"Clusters in Test: {test_cluster_count} ({test_cluster_count/n_clusters*100:.1f}%)\")\n",
        "    print(f\"Overlapping clusters (Train & Val): {overlap_train_val} ({overlap_train_val/union_train_val*100:.1f}%)\")\n",
        "    print(f\"Overlapping clusters (Train & Test): {overlap_train_test} ({overlap_train_test/union_train_test*100:.1f}%)\")\n",
        "    print(f\"Overlapping clusters (Val & Test): {overlap_val_test} ({overlap_val_test/union_val_test*100:.1f}%)\")\n",
        "\n",
        "    return {\n",
        "        \"train_size\": total_train,\n",
        "        \"val_size\": total_val,\n",
        "        \"test_size\": total_test,\n",
        "        \"train_mw_mean\": compute_mw_stats(train_smiles)[0],\n",
        "        \"train_mw_std\": compute_mw_stats(train_smiles)[1],\n",
        "        \"val_mw_mean\": compute_mw_stats(val_smiles)[0],\n",
        "        \"val_mw_std\": compute_mw_stats(val_smiles)[1],\n",
        "        \"test_mw_mean\": compute_mw_stats(test_smiles)[0],\n",
        "        \"test_mw_std\": compute_mw_stats(test_smiles)[1],\n",
        "        \"train_tanimoto_sim\": compute_tanimoto_similarity(train_fps),\n",
        "        \"val_tanimoto_sim\": compute_tanimoto_similarity(val_fps),\n",
        "        \"test_tanimoto_sim\": compute_tanimoto_similarity(test_fps),\n",
        "        \"train_val_cluster_overlap\": overlap_train_val / union_train_val if union_train_val else 0,\n",
        "        \"train_test_cluster_overlap\": overlap_train_test / union_train_test if union_train_test else 0,\n",
        "        \"val_test_cluster_overlap\": overlap_val_test / union_val_test if union_val_test else 0\n",
        "    }\n",
        "\n",
        "def plot_distributions(train_pairs, val_pairs, test_pairs):\n",
        "    train_smiles = [pair.smiles_string for pair in train_pairs if hasattr(pair, 'smiles_string')]\n",
        "    val_smiles = [pair.smiles_string for pair in val_pairs if hasattr(pair, 'smiles_string')]\n",
        "    test_smiles = [pair.smiles_string for pair in test_pairs if hasattr(pair, 'smiles_string')]\n",
        "    train_mws = [Descriptors.MolWt(Chem.MolFromSmiles(extract_monomer(s))) for s in train_smiles if Chem.MolFromSmiles(extract_monomer(s))]\n",
        "    val_mws = [Descriptors.MolWt(Chem.MolFromSmiles(extract_monomer(s))) for s in val_smiles if Chem.MolFromSmiles(extract_monomer(s))]\n",
        "    test_mws = [Descriptors.MolWt(Chem.MolFromSmiles(extract_monomer(s))) for s in test_smiles if Chem.MolFromSmiles(extract_monomer(s))]\n",
        "    train_fps = [smiles_to_morgan_fp(s) for s in train_smiles if smiles_to_morgan_fp(s) is not None]\n",
        "    val_fps = [smiles_to_morgan_fp(s) for s in val_smiles if smiles_to_morgan_fp(s) is not None]\n",
        "    test_fps = [smiles_to_morgan_fp(s) for s in test_smiles if smiles_to_morgan_fp(s) is not None]\n",
        "    train_clusters, _ = cluster_fingerprints(train_fps)\n",
        "    val_clusters, _ = cluster_fingerprints(val_fps)\n",
        "    test_clusters, _ = cluster_fingerprints(test_fps)\n",
        "    train_cells = np.array([pair.cell_embedding for pair in train_pairs if hasattr(pair, 'cell_embedding')])\n",
        "    val_cells = np.array([pair.cell_embedding for pair in val_pairs if hasattr(pair, 'cell_embedding')])\n",
        "    test_cells = np.array([pair.cell_embedding for pair in test_pairs if hasattr(pair, 'cell_embedding')])\n",
        "\n",
        "    plt.figure(figsize=(15, 10))\n",
        "    plt.subplot(2, 3, 1)\n",
        "    if train_mws: sns.histplot(train_mws, kde=True, color=\"blue\", label=\"Train\", bins=30)\n",
        "    if val_mws: sns.histplot(val_mws, kde=True, color=\"green\", label=\"Val\", bins=30)\n",
        "    if test_mws: sns.histplot(test_mws, kde=True, color=\"red\", label=\"Test\", bins=30)\n",
        "    plt.xlabel(\"Molecular Weight\")\n",
        "    plt.ylabel(\"Count\")\n",
        "    plt.title(\"Molecular Weight Distribution\")\n",
        "    plt.legend()\n",
        "    plt.subplot(2, 3, 2)\n",
        "    if train_clusters.size: sns.histplot(train_clusters, kde=False, color=\"blue\", label=\"Train\", bins=len(set(train_clusters)))\n",
        "    if val_clusters.size: sns.histplot(val_clusters, kde=False, color=\"green\", label=\"Val\", bins=len(set(val_clusters)))\n",
        "    if test_clusters.size: sns.histplot(test_clusters, kde=False, color=\"red\", label=\"Test\", bins=len(set(test_clusters)))\n",
        "    plt.xlabel(\"Cluster Index\")\n",
        "    plt.ylabel(\"Count\")\n",
        "    plt.title(\"Cluster Distribution\")\n",
        "    plt.legend()\n",
        "    plt.subplot(2, 3, 3)\n",
        "    train_sim = compute_tanimoto_similarity(train_fps)\n",
        "    val_sim = compute_tanimoto_similarity(val_fps)\n",
        "    test_sim = compute_tanimoto_similarity(test_fps)\n",
        "    sns.barplot(x=[\"Train\", \"Val\", \"Test\"], y=[train_sim, val_sim, test_sim], palette=[\"blue\", \"green\", \"red\"])\n",
        "    plt.ylabel(\"Avg Tanimoto Similarity\")\n",
        "    plt.title(\"Tanimoto Similarity Across Splits\")\n",
        "    plt.subplot(2, 3, 4)\n",
        "    if train_cells.size: sns.histplot(train_cells.mean(axis=1), kde=True, color=\"blue\", label=\"Train\")\n",
        "    if val_cells.size: sns.histplot(val_cells.mean(axis=1), kde=True, color=\"green\", label=\"Val\")\n",
        "    if test_cells.size: sns.histplot(test_cells.mean(axis=1), kde=True, color=\"red\", label=\"Test\")\n",
        "    plt.xlabel(\"Mean Cell Embedding\")\n",
        "    plt.ylabel(\"Count\")\n",
        "    plt.title(\"Cell Embedding Distribution\")\n",
        "    plt.legend()\n",
        "    plt.subplot(2, 3, 5)\n",
        "    if train_cells.size: sns.histplot(train_cells.std(axis=1), kde=True, color=\"blue\", label=\"Train\")\n",
        "    if val_cells.size: sns.histplot(val_cells.std(axis=1), kde=True, color=\"green\", label=\"Val\")\n",
        "    if test_cells.size: sns.histplot(test_cells.std(axis=1), kde=True, color=\"red\", label=\"Test\")\n",
        "    plt.xlabel(\"Standard Deviation of Cell Embedding\")\n",
        "    plt.ylabel(\"Count\")\n",
        "    plt.title(\"Cell Embedding Variability\")\n",
        "    plt.legend()\n",
        "    plt.subplot(2, 3, 6)\n",
        "    train_var = np.var(train_cells, axis=0).sum() if train_cells.size else 0\n",
        "    val_var = np.var(val_cells, axis=0).sum() if val_cells.size else 0\n",
        "    test_var = np.var(test_cells, axis=0).sum() if test_cells.size else 0\n",
        "    sns.barplot(x=[\"Train\", \"Val\", \"Test\"], y=[train_var, val_var, test_var], palette=[\"blue\", \"green\", \"red\"])\n",
        "    plt.ylabel(\"Total Variance\")\n",
        "    plt.title(\"Variance Captured in Cell Embeddings\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "data_path = \"/content/drive/MyDrive/Colab Notebooks/esm cell state/scgpt_workdir_fixed_1\"\n",
        "with open(f\"{data_path}/train_pairs.pkl\", 'rb') as f:\n",
        "    train_pairs = pickle.load(f)\n",
        "with open(f\"{data_path}/val_pairs.pkl\", 'rb') as f:\n",
        "    val_pairs = pickle.load(f)\n",
        "with open(f\"{data_path}/test_pairs.pkl\", 'rb') as f:\n",
        "    test_pairs = pickle.load(f)\n",
        "\n",
        "metrics = evaluate_splits(train_pairs, val_pairs, test_pairs, n_clusters=10)\n",
        "for key, value in metrics.items():\n",
        "    print(f\"{key}: {value:.4f}\" if isinstance(value, float) else f\"{key}: {value}\")\n",
        "\n",
        "plot_distributions(train_pairs, val_pairs, test_pairs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ioBg-DMM9ML"
      },
      "source": [
        "## sampling arc-dataset plate 3\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K2PdTtEkNITw",
        "outputId": "8cb60695-b077-4678-8223-3579fc9ac62d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m-XXsdydND9F"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.chdir(\"/content/drive/MyDrive/Colab Notebooks/esm cell state/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Q9xsBnhJ9Sw",
        "outputId": "79d44d46-4243-4368-eb5b-05e31a7e847b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 33l_8ep_1024t_1280.torch\t avg_counts_control\t    h5ad\t\t    smiles-gpt\n",
            " arc_flamingo.ipynb\t\t avg_counts_filtered\t    scgpt_anndata.h5ad\t    uce_workdir\n",
            "'arc virtual cell atlas.ipynb'\t cell\t\t\t    scGPT_human\n",
            " avg_counts\t\t\t chemberta_embeddings.pkl   smiles_df.csv\n",
            " avg_counts_2\t\t\t'gb check for data.ipynb'   smiles_embeddings.pkl\n"
          ]
        }
      ],
      "source": [
        "!ls \"/content/drive/MyDrive/Colab Notebooks/esm cell state/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C1Py-8tDND9F"
      },
      "outputs": [],
      "source": [
        "import scanpy as sc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CCwoyU4qJPuZ"
      },
      "outputs": [],
      "source": [
        "adata = sc.read_h5ad(\"/content/drive/MyDrive/Colab Notebooks/esm cell state/scgpt_anndata.h5ad\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oGrBNHKpKkP_"
      },
      "outputs": [],
      "source": [
        "X_scGPT = adata.obsm['X_scGPT']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kZPoNlU4MBOy",
        "outputId": "c1eea733-35de-4cc1-a32e-a7c60e625e76"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(470494, 512)\n"
          ]
        }
      ],
      "source": [
        "print(X_scGPT.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GkuX8VpSPJ3E"
      },
      "outputs": [],
      "source": [
        "# !rm -rf \"/content/drive/MyDrive/Colab Notebooks/esm cell state/difference counts\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P3y1sk1mNQKU",
        "outputId": "47c17ac4-072c-4139-a1da-977c9c601d9f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of unique drugs: 93\n",
            "['Infigratinib', 'Erdafitinib ', 'Everolimus', 'Pemigatinib', 'Abemaciclib', ..., 'Oleic acid', 'Minodronic acid', 'Anastrozole', 'Adagrasib', 'DMSO_TF']\n",
            "Length: 93\n",
            "Categories (93, object): ['4EGI-1', '9-ING-41', 'APTO-253', 'AT7519', ..., 'olaparib',\n",
            "                          'palbociclib', 'venetoclax', 'vincristine']\n",
            "Drug Infigratinib: 5167 cells\n",
            "Drug Erdafitinib : 5118 cells\n",
            "Drug Everolimus: 11703 cells\n",
            "Drug Pemigatinib: 4376 cells\n",
            "Drug Abemaciclib: 4616 cells\n",
            "Drug Capivasertib: 3121 cells\n",
            "Drug Gemcitabine: 7448 cells\n",
            "Drug Irinotecan: 7988 cells\n",
            "Drug Oxaliplatin: 4693 cells\n",
            "Drug Alpelisib: 3652 cells\n",
            "Drug Lapatinib ditosylate: 7903 cells\n",
            "Drug palbociclib: 3887 cells\n",
            "Drug Lonafarnib: 1144 cells\n",
            "Drug Volasertib: 2556 cells\n",
            "Drug c-Kit-IN-1: 3283 cells\n",
            "Drug venetoclax: 3334 cells\n",
            "Drug Capmatinib: 2998 cells\n",
            "Drug MK-8353: 3557 cells\n",
            "Drug Regorafenib: 4533 cells\n",
            "Drug Tucatinib: 8439 cells\n",
            "Drug Vemurafenib: 7289 cells\n",
            "Drug BI-3406: 6913 cells\n",
            "Drug AZD-8055: 6123 cells\n",
            "Drug Binimetinib: 2897 cells\n",
            "Drug Temuterkib: 5789 cells\n",
            "Drug ML264: 6050 cells\n",
            "Drug BI-78D3: 7437 cells\n",
            "Drug Tomivosertib: 5765 cells\n",
            "Drug Brivudine: 8259 cells\n",
            "Drug Bentamapimod: 5981 cells\n",
            "Drug Dabrafenib: 12409 cells\n",
            "Drug Encorafenib: 11730 cells\n",
            "Drug Paclitaxel: 11487 cells\n",
            "Drug RMC-6236: 9541 cells\n",
            "Drug Trametinib: 8419 cells\n",
            "Drug Afatinib: 8092 cells\n",
            "Drug NVP-BHG712: 3942 cells\n",
            "Drug Ralimetinib dimesylate: 5224 cells\n",
            "Drug ERK5-IN-2: 9430 cells\n",
            "Drug PH-797804: 4509 cells\n",
            "Drug TAK-901: 6675 cells\n",
            "Drug Ipatasertib: 9567 cells\n",
            "Drug Sapanisertib: 10111 cells\n",
            "Drug BAY1125976: 9661 cells\n",
            "Drug 9-ING-41: 10120 cells\n",
            "Drug XRK3F2: 8485 cells\n",
            "Drug Bimiralisib: 9914 cells\n",
            "Drug GSK1059615: 8279 cells\n",
            "Drug AZD2858: 1888 cells\n",
            "Drug PF-06260933: 2390 cells\n",
            "Drug SBI-0640756: 2455 cells\n",
            "Drug LJI308: 3329 cells\n",
            "Drug LY-2584702 (tosylate salt): 1474 cells\n",
            "Drug Sonidegib: 2253 cells\n",
            "Drug EX229: 3791 cells\n",
            "Drug MK-3903: 3427 cells\n",
            "Drug ULK-101: 2945 cells\n",
            "Drug IQ 1: 2897 cells\n",
            "Drug CP21R7: 3080 cells\n",
            "Drug Bortezomib: 1567 cells\n",
            "Drug Hydroxyfasudil: 2434 cells\n",
            "Drug Belumosudil (mesylate): 3291 cells\n",
            "Drug Belzutifan: 2818 cells\n",
            "Drug Futibatinib: 2752 cells\n",
            "Drug Pralsetinib: 2448 cells\n",
            "Drug TAK-733: 3919 cells\n",
            "Drug Elimusertib hydrochloride: 2056 cells\n",
            "Drug AZD1390: 4111 cells\n",
            "Drug Tubulin inhibitor 6: 3167 cells\n",
            "Drug Torkinib: 1921 cells\n",
            "Drug 4EGI-1: 3138 cells\n",
            "Drug Pimitespib: 2574 cells\n",
            "Drug olaparib: 3552 cells\n",
            "Drug Ixazomib: 2944 cells\n",
            "Drug DTP3: 3485 cells\n",
            "Drug OTS514: 2506 cells\n",
            "Drug NG25: 3942 cells\n",
            "Drug DT-061: 2970 cells\n",
            "Drug AZD-7648: 2960 cells\n",
            "Drug Methylprednisolone succinate: 3353 cells\n",
            "Drug Celecoxib: 4047 cells\n",
            "Drug vincristine: 2945 cells\n",
            "Drug HI-TOPK-032: 2948 cells\n",
            "Drug APTO-253: 4664 cells\n",
            "Drug LB-100: 4251 cells\n",
            "Drug AT7519: 5440 cells\n",
            "Drug ETC-206: 4532 cells\n",
            "Drug LY2090314: 5547 cells\n",
            "Drug Oleic acid: 3460 cells\n",
            "Drug Minodronic acid: 4331 cells\n",
            "Drug Anastrozole: 4796 cells\n",
            "Drug Adagrasib: 3014 cells\n",
            "Drug DMSO_TF: 9068 cells\n",
            "Calculating control averages...\n",
            "Total blocks to create: approximately 30807\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing drugs:   0%|          | 0/93 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Completed 10/30807 blocks\n",
            "Completed 20/30807 blocks\n",
            "Completed 30/30807 blocks\n",
            "Completed 40/30807 blocks\n",
            "Completed 50/30807 blocks\n",
            "Completed 60/30807 blocks\n",
            "Completed 70/30807 blocks\n",
            "Completed 80/30807 blocks\n",
            "Completed 90/30807 blocks\n",
            "Completed 100/30807 blocks\n",
            "Completed 110/30807 blocks\n",
            "Completed 120/30807 blocks\n",
            "Completed 130/30807 blocks\n",
            "Completed 140/30807 blocks\n",
            "Completed 150/30807 blocks\n",
            "Completed 160/30807 blocks\n",
            "Completed 170/30807 blocks\n",
            "Completed 180/30807 blocks\n",
            "Completed 190/30807 blocks\n",
            "Completed 200/30807 blocks\n",
            "Completed 210/30807 blocks\n",
            "Completed 220/30807 blocks\n",
            "Completed 230/30807 blocks\n",
            "Completed 240/30807 blocks\n",
            "Completed 250/30807 blocks\n",
            "Completed 260/30807 blocks\n",
            "Completed 270/30807 blocks\n",
            "Completed 280/30807 blocks\n",
            "Completed 290/30807 blocks\n",
            "Completed 300/30807 blocks\n",
            "Completed 310/30807 blocks\n",
            "Completed 320/30807 blocks\n",
            "Completed 330/30807 blocks\n",
            "Completed 340/30807 blocks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing drugs:   1%|          | 1/93 [00:21<32:16, 21.04s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Completed 350/30807 blocks\n",
            "Completed 360/30807 blocks\n",
            "Completed 370/30807 blocks\n",
            "Completed 380/30807 blocks\n",
            "Completed 390/30807 blocks\n",
            "Completed 400/30807 blocks\n",
            "Completed 410/30807 blocks\n",
            "Completed 420/30807 blocks\n",
            "Completed 430/30807 blocks\n",
            "Completed 440/30807 blocks\n",
            "Completed 450/30807 blocks\n",
            "Completed 460/30807 blocks\n",
            "Completed 470/30807 blocks\n",
            "Completed 480/30807 blocks\n",
            "Completed 490/30807 blocks\n",
            "Completed 500/30807 blocks\n",
            "Completed 510/30807 blocks\n",
            "Completed 520/30807 blocks\n",
            "Completed 530/30807 blocks\n",
            "Completed 540/30807 blocks\n",
            "Completed 550/30807 blocks\n",
            "Completed 560/30807 blocks\n",
            "Completed 570/30807 blocks\n",
            "Completed 580/30807 blocks\n",
            "Completed 590/30807 blocks\n",
            "Completed 600/30807 blocks\n",
            "Completed 610/30807 blocks\n",
            "Completed 620/30807 blocks\n",
            "Completed 630/30807 blocks\n",
            "Completed 640/30807 blocks\n",
            "Completed 650/30807 blocks\n",
            "Completed 660/30807 blocks\n",
            "Completed 670/30807 blocks\n",
            "Completed 680/30807 blocks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing drugs:   2%|▏         | 2/93 [00:41<31:38, 20.87s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Completed 690/30807 blocks\n",
            "Completed 700/30807 blocks\n",
            "Completed 710/30807 blocks\n",
            "Completed 720/30807 blocks\n",
            "Completed 730/30807 blocks\n",
            "Completed 740/30807 blocks\n",
            "Completed 750/30807 blocks\n",
            "Completed 760/30807 blocks\n",
            "Completed 770/30807 blocks\n",
            "Completed 780/30807 blocks\n",
            "Completed 790/30807 blocks\n",
            "Completed 800/30807 blocks\n",
            "Completed 810/30807 blocks\n",
            "Completed 820/30807 blocks\n",
            "Completed 830/30807 blocks\n",
            "Completed 840/30807 blocks\n",
            "Completed 850/30807 blocks\n",
            "Completed 860/30807 blocks\n",
            "Completed 870/30807 blocks\n",
            "Completed 880/30807 blocks\n",
            "Completed 890/30807 blocks\n",
            "Completed 900/30807 blocks\n",
            "Completed 910/30807 blocks\n",
            "Completed 920/30807 blocks\n",
            "Completed 930/30807 blocks\n",
            "Completed 940/30807 blocks\n",
            "Completed 950/30807 blocks\n",
            "Completed 960/30807 blocks\n",
            "Completed 970/30807 blocks\n",
            "Completed 980/30807 blocks\n",
            "Completed 990/30807 blocks\n",
            "Completed 1000/30807 blocks\n",
            "Completed 1010/30807 blocks\n",
            "Completed 1020/30807 blocks\n",
            "Completed 1030/30807 blocks\n",
            "Completed 1040/30807 blocks\n",
            "Completed 1050/30807 blocks\n",
            "Completed 1060/30807 blocks\n",
            "Completed 1070/30807 blocks\n",
            "Completed 1080/30807 blocks\n",
            "Completed 1090/30807 blocks\n",
            "Completed 1100/30807 blocks\n",
            "Completed 1110/30807 blocks\n",
            "Completed 1120/30807 blocks\n",
            "Completed 1130/30807 blocks\n",
            "Completed 1140/30807 blocks\n",
            "Completed 1150/30807 blocks\n",
            "Completed 1160/30807 blocks\n",
            "Completed 1170/30807 blocks\n",
            "Completed 1180/30807 blocks\n",
            "Completed 1190/30807 blocks\n",
            "Completed 1200/30807 blocks\n",
            "Completed 1210/30807 blocks\n",
            "Completed 1220/30807 blocks\n",
            "Completed 1230/30807 blocks\n",
            "Completed 1240/30807 blocks\n",
            "Completed 1250/30807 blocks\n",
            "Completed 1260/30807 blocks\n",
            "Completed 1270/30807 blocks\n",
            "Completed 1280/30807 blocks\n",
            "Completed 1290/30807 blocks\n",
            "Completed 1300/30807 blocks\n",
            "Completed 1310/30807 blocks\n",
            "Completed 1320/30807 blocks\n",
            "Completed 1330/30807 blocks\n",
            "Completed 1340/30807 blocks\n",
            "Completed 1350/30807 blocks\n",
            "Completed 1360/30807 blocks\n",
            "Completed 1370/30807 blocks\n",
            "Completed 1380/30807 blocks\n",
            "Completed 1390/30807 blocks\n",
            "Completed 1400/30807 blocks\n",
            "Completed 1410/30807 blocks\n",
            "Completed 1420/30807 blocks\n",
            "Completed 1430/30807 blocks\n",
            "Completed 1440/30807 blocks\n",
            "Completed 1450/30807 blocks\n",
            "Completed 1460/30807 blocks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing drugs:   3%|▎         | 3/93 [01:30<50:37, 33.75s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Completed 1470/30807 blocks\n",
            "Completed 1480/30807 blocks\n",
            "Completed 1490/30807 blocks\n",
            "Completed 1500/30807 blocks\n",
            "Completed 1510/30807 blocks\n",
            "Completed 1520/30807 blocks\n",
            "Completed 1530/30807 blocks\n",
            "Completed 1540/30807 blocks\n",
            "Completed 1550/30807 blocks\n",
            "Completed 1560/30807 blocks\n",
            "Completed 1570/30807 blocks\n",
            "Completed 1580/30807 blocks\n",
            "Completed 1590/30807 blocks\n",
            "Completed 1600/30807 blocks\n",
            "Completed 1610/30807 blocks\n",
            "Completed 1620/30807 blocks\n",
            "Completed 1630/30807 blocks\n",
            "Completed 1640/30807 blocks\n",
            "Completed 1650/30807 blocks\n",
            "Completed 1660/30807 blocks\n",
            "Completed 1670/30807 blocks\n",
            "Completed 1680/30807 blocks\n",
            "Completed 1690/30807 blocks\n",
            "Completed 1700/30807 blocks\n",
            "Completed 1710/30807 blocks\n",
            "Completed 1720/30807 blocks\n",
            "Completed 1730/30807 blocks\n",
            "Completed 1740/30807 blocks\n",
            "Completed 1750/30807 blocks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing drugs:   4%|▍         | 4/93 [01:48<40:47, 27.50s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Completed 1760/30807 blocks\n",
            "Completed 1770/30807 blocks\n",
            "Completed 1780/30807 blocks\n",
            "Completed 1790/30807 blocks\n",
            "Completed 1800/30807 blocks\n",
            "Completed 1810/30807 blocks\n",
            "Completed 1820/30807 blocks\n",
            "Completed 1830/30807 blocks\n",
            "Completed 1840/30807 blocks\n",
            "Completed 1850/30807 blocks\n",
            "Completed 1860/30807 blocks\n",
            "Completed 1870/30807 blocks\n",
            "Completed 1880/30807 blocks\n",
            "Completed 1890/30807 blocks\n",
            "Completed 1900/30807 blocks\n",
            "Completed 1910/30807 blocks\n",
            "Completed 1920/30807 blocks\n",
            "Completed 1930/30807 blocks\n",
            "Completed 1940/30807 blocks\n",
            "Completed 1950/30807 blocks\n",
            "Completed 1960/30807 blocks\n",
            "Completed 1970/30807 blocks\n",
            "Completed 1980/30807 blocks\n",
            "Completed 1990/30807 blocks\n",
            "Completed 2000/30807 blocks\n",
            "Completed 2010/30807 blocks\n",
            "Completed 2020/30807 blocks\n",
            "Completed 2030/30807 blocks\n",
            "Completed 2040/30807 blocks\n",
            "Completed 2050/30807 blocks\n",
            "Completed 2060/30807 blocks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing drugs:   5%|▌         | 5/93 [02:08<35:57, 24.52s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Completed 2070/30807 blocks\n",
            "Completed 2080/30807 blocks\n",
            "Completed 2090/30807 blocks\n",
            "Completed 2100/30807 blocks\n",
            "Completed 2110/30807 blocks\n",
            "Completed 2120/30807 blocks\n",
            "Completed 2130/30807 blocks\n",
            "Completed 2140/30807 blocks\n",
            "Completed 2150/30807 blocks\n",
            "Completed 2160/30807 blocks\n",
            "Completed 2170/30807 blocks\n",
            "Completed 2180/30807 blocks\n",
            "Completed 2190/30807 blocks\n",
            "Completed 2200/30807 blocks\n",
            "Completed 2210/30807 blocks\n",
            "Completed 2220/30807 blocks\n",
            "Completed 2230/30807 blocks\n",
            "Completed 2240/30807 blocks\n",
            "Completed 2250/30807 blocks\n",
            "Completed 2260/30807 blocks\n",
            "Completed 2270/30807 blocks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing drugs:   6%|▋         | 6/93 [02:21<30:04, 20.74s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Completed 2280/30807 blocks\n",
            "Completed 2290/30807 blocks\n",
            "Completed 2300/30807 blocks\n",
            "Completed 2310/30807 blocks\n",
            "Completed 2320/30807 blocks\n",
            "Completed 2330/30807 blocks\n",
            "Completed 2340/30807 blocks\n",
            "Completed 2350/30807 blocks\n",
            "Completed 2360/30807 blocks\n",
            "Completed 2370/30807 blocks\n",
            "Completed 2380/30807 blocks\n",
            "Completed 2390/30807 blocks\n",
            "Completed 2400/30807 blocks\n",
            "Completed 2410/30807 blocks\n",
            "Completed 2420/30807 blocks\n",
            "Completed 2430/30807 blocks\n",
            "Completed 2440/30807 blocks\n",
            "Completed 2450/30807 blocks\n",
            "Completed 2460/30807 blocks\n",
            "Completed 2470/30807 blocks\n",
            "Completed 2480/30807 blocks\n",
            "Completed 2490/30807 blocks\n",
            "Completed 2500/30807 blocks\n",
            "Completed 2510/30807 blocks\n",
            "Completed 2520/30807 blocks\n",
            "Completed 2530/30807 blocks\n",
            "Completed 2540/30807 blocks\n",
            "Completed 2550/30807 blocks\n",
            "Completed 2560/30807 blocks\n",
            "Completed 2570/30807 blocks\n",
            "Completed 2580/30807 blocks\n",
            "Completed 2590/30807 blocks\n",
            "Completed 2600/30807 blocks\n",
            "Completed 2610/30807 blocks\n",
            "Completed 2620/30807 blocks\n",
            "Completed 2630/30807 blocks\n",
            "Completed 2640/30807 blocks\n",
            "Completed 2650/30807 blocks\n",
            "Completed 2660/30807 blocks\n",
            "Completed 2670/30807 blocks\n",
            "Completed 2680/30807 blocks\n",
            "Completed 2690/30807 blocks\n",
            "Completed 2700/30807 blocks\n",
            "Completed 2710/30807 blocks\n",
            "Completed 2720/30807 blocks\n",
            "Completed 2730/30807 blocks\n",
            "Completed 2740/30807 blocks\n",
            "Completed 2750/30807 blocks\n",
            "Completed 2760/30807 blocks\n",
            "Completed 2770/30807 blocks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing drugs:   8%|▊         | 7/93 [02:52<34:32, 24.09s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Completed 2780/30807 blocks\n",
            "Completed 2790/30807 blocks\n",
            "Completed 2800/30807 blocks\n",
            "Completed 2810/30807 blocks\n",
            "Completed 2820/30807 blocks\n",
            "Completed 2830/30807 blocks\n",
            "Completed 2840/30807 blocks\n",
            "Completed 2850/30807 blocks\n",
            "Completed 2860/30807 blocks\n",
            "Completed 2870/30807 blocks\n",
            "Completed 2880/30807 blocks\n",
            "Completed 2890/30807 blocks\n",
            "Completed 2900/30807 blocks\n",
            "Completed 2910/30807 blocks\n",
            "Completed 2920/30807 blocks\n",
            "Completed 2930/30807 blocks\n",
            "Completed 2940/30807 blocks\n",
            "Completed 2950/30807 blocks\n",
            "Completed 2960/30807 blocks\n",
            "Completed 2970/30807 blocks\n",
            "Completed 2980/30807 blocks\n",
            "Completed 2990/30807 blocks\n",
            "Completed 3000/30807 blocks\n",
            "Completed 3010/30807 blocks\n",
            "Completed 3020/30807 blocks\n",
            "Completed 3030/30807 blocks\n",
            "Completed 3040/30807 blocks\n",
            "Completed 3050/30807 blocks\n",
            "Completed 3060/30807 blocks\n",
            "Completed 3070/30807 blocks\n",
            "Completed 3080/30807 blocks\n",
            "Completed 3090/30807 blocks\n",
            "Completed 3100/30807 blocks\n",
            "Completed 3110/30807 blocks\n",
            "Completed 3120/30807 blocks\n",
            "Completed 3130/30807 blocks\n",
            "Completed 3140/30807 blocks\n",
            "Completed 3150/30807 blocks\n",
            "Completed 3160/30807 blocks\n",
            "Completed 3170/30807 blocks\n",
            "Completed 3180/30807 blocks\n",
            "Completed 3190/30807 blocks\n",
            "Completed 3200/30807 blocks\n",
            "Completed 3210/30807 blocks\n",
            "Completed 3220/30807 blocks\n",
            "Completed 3230/30807 blocks\n",
            "Completed 3240/30807 blocks\n",
            "Completed 3250/30807 blocks\n",
            "Completed 3260/30807 blocks\n",
            "Completed 3270/30807 blocks\n",
            "Completed 3280/30807 blocks\n",
            "Completed 3290/30807 blocks\n",
            "Completed 3300/30807 blocks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing drugs:   9%|▊         | 8/93 [03:26<38:26, 27.14s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Completed 3310/30807 blocks\n",
            "Completed 3320/30807 blocks\n",
            "Completed 3330/30807 blocks\n",
            "Completed 3340/30807 blocks\n",
            "Completed 3350/30807 blocks\n",
            "Completed 3360/30807 blocks\n",
            "Completed 3370/30807 blocks\n",
            "Completed 3380/30807 blocks\n",
            "Completed 3390/30807 blocks\n",
            "Completed 3400/30807 blocks\n",
            "Completed 3410/30807 blocks\n",
            "Completed 3420/30807 blocks\n",
            "Completed 3430/30807 blocks\n",
            "Completed 3440/30807 blocks\n",
            "Completed 3450/30807 blocks\n",
            "Completed 3460/30807 blocks\n",
            "Completed 3470/30807 blocks\n",
            "Completed 3480/30807 blocks\n",
            "Completed 3490/30807 blocks\n",
            "Completed 3500/30807 blocks\n",
            "Completed 3510/30807 blocks\n",
            "Completed 3520/30807 blocks\n",
            "Completed 3530/30807 blocks\n",
            "Completed 3540/30807 blocks\n",
            "Completed 3550/30807 blocks\n",
            "Completed 3560/30807 blocks\n",
            "Completed 3570/30807 blocks\n",
            "Completed 3580/30807 blocks\n",
            "Completed 3590/30807 blocks\n",
            "Completed 3600/30807 blocks\n",
            "Completed 3610/30807 blocks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing drugs:  10%|▉         | 9/93 [03:45<34:49, 24.87s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Completed 3620/30807 blocks\n",
            "Completed 3630/30807 blocks\n",
            "Completed 3640/30807 blocks\n",
            "Completed 3650/30807 blocks\n",
            "Completed 3660/30807 blocks\n",
            "Completed 3670/30807 blocks\n",
            "Completed 3680/30807 blocks\n",
            "Completed 3690/30807 blocks\n",
            "Completed 3700/30807 blocks\n",
            "Completed 3710/30807 blocks\n",
            "Completed 3720/30807 blocks\n",
            "Completed 3730/30807 blocks\n",
            "Completed 3740/30807 blocks\n",
            "Completed 3750/30807 blocks\n",
            "Completed 3760/30807 blocks\n",
            "Completed 3770/30807 blocks\n",
            "Completed 3780/30807 blocks\n",
            "Completed 3790/30807 blocks\n",
            "Completed 3800/30807 blocks\n",
            "Completed 3810/30807 blocks\n",
            "Completed 3820/30807 blocks\n",
            "Completed 3830/30807 blocks\n",
            "Completed 3840/30807 blocks\n",
            "Completed 3850/30807 blocks\n",
            "Completed 3860/30807 blocks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing drugs:  11%|█         | 10/93 [04:01<30:20, 21.93s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Completed 3870/30807 blocks\n",
            "Completed 3880/30807 blocks\n",
            "Completed 3890/30807 blocks\n",
            "Completed 3900/30807 blocks\n",
            "Completed 3910/30807 blocks\n",
            "Completed 3920/30807 blocks\n",
            "Completed 3930/30807 blocks\n",
            "Completed 3940/30807 blocks\n",
            "Completed 3950/30807 blocks\n",
            "Completed 3960/30807 blocks\n",
            "Completed 3970/30807 blocks\n",
            "Completed 3980/30807 blocks\n",
            "Completed 3990/30807 blocks\n",
            "Completed 4000/30807 blocks\n",
            "Completed 4010/30807 blocks\n",
            "Completed 4020/30807 blocks\n",
            "Completed 4030/30807 blocks\n",
            "Completed 4040/30807 blocks\n",
            "Completed 4050/30807 blocks\n",
            "Completed 4060/30807 blocks\n",
            "Completed 4070/30807 blocks\n",
            "Completed 4080/30807 blocks\n",
            "Completed 4090/30807 blocks\n",
            "Completed 4100/30807 blocks\n",
            "Completed 4110/30807 blocks\n",
            "Completed 4120/30807 blocks\n",
            "Completed 4130/30807 blocks\n",
            "Completed 4140/30807 blocks\n",
            "Completed 4150/30807 blocks\n",
            "Completed 4160/30807 blocks\n",
            "Completed 4170/30807 blocks\n",
            "Completed 4180/30807 blocks\n",
            "Completed 4190/30807 blocks\n",
            "Completed 4200/30807 blocks\n",
            "Completed 4210/30807 blocks\n",
            "Completed 4220/30807 blocks\n",
            "Completed 4230/30807 blocks\n",
            "Completed 4240/30807 blocks\n",
            "Completed 4250/30807 blocks\n",
            "Completed 4260/30807 blocks\n",
            "Completed 4270/30807 blocks\n",
            "Completed 4280/30807 blocks\n",
            "Completed 4290/30807 blocks\n",
            "Completed 4300/30807 blocks\n",
            "Completed 4310/30807 blocks\n",
            "Completed 4320/30807 blocks\n",
            "Completed 4330/30807 blocks\n",
            "Completed 4340/30807 blocks\n",
            "Completed 4350/30807 blocks\n",
            "Completed 4360/30807 blocks\n",
            "Completed 4370/30807 blocks\n",
            "Completed 4380/30807 blocks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing drugs:  12%|█▏        | 11/93 [04:34<34:31, 25.26s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Completed 4390/30807 blocks\n",
            "Completed 4400/30807 blocks\n",
            "Completed 4410/30807 blocks\n",
            "Completed 4420/30807 blocks\n",
            "Completed 4430/30807 blocks\n",
            "Completed 4440/30807 blocks\n",
            "Completed 4450/30807 blocks\n",
            "Completed 4460/30807 blocks\n",
            "Completed 4470/30807 blocks\n",
            "Completed 4480/30807 blocks\n",
            "Completed 4490/30807 blocks\n",
            "Completed 4500/30807 blocks\n",
            "Completed 4510/30807 blocks\n",
            "Completed 4520/30807 blocks\n",
            "Completed 4530/30807 blocks\n",
            "Completed 4540/30807 blocks\n",
            "Completed 4550/30807 blocks\n",
            "Completed 4560/30807 blocks\n",
            "Completed 4570/30807 blocks\n",
            "Completed 4580/30807 blocks\n",
            "Completed 4590/30807 blocks\n",
            "Completed 4600/30807 blocks\n",
            "Completed 4610/30807 blocks\n",
            "Completed 4620/30807 blocks\n",
            "Completed 4630/30807 blocks\n",
            "Completed 4640/30807 blocks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing drugs:  13%|█▎        | 12/93 [04:50<30:21, 22.48s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Completed 4650/30807 blocks\n",
            "Completed 4660/30807 blocks\n",
            "Completed 4670/30807 blocks\n",
            "Completed 4680/30807 blocks\n",
            "Completed 4690/30807 blocks\n",
            "Completed 4700/30807 blocks\n",
            "Completed 4710/30807 blocks\n",
            "Completed 4720/30807 blocks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing drugs:  14%|█▍        | 13/93 [04:54<22:46, 17.08s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Completed 4730/30807 blocks\n",
            "Completed 4740/30807 blocks\n",
            "Completed 4750/30807 blocks\n",
            "Completed 4760/30807 blocks\n",
            "Completed 4770/30807 blocks\n",
            "Completed 4780/30807 blocks\n",
            "Completed 4790/30807 blocks\n",
            "Completed 4800/30807 blocks\n",
            "Completed 4810/30807 blocks\n",
            "Completed 4820/30807 blocks\n",
            "Completed 4830/30807 blocks\n",
            "Completed 4840/30807 blocks\n",
            "Completed 4850/30807 blocks\n",
            "Completed 4860/30807 blocks\n",
            "Completed 4870/30807 blocks\n",
            "Completed 4880/30807 blocks\n",
            "Completed 4890/30807 blocks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing drugs:  15%|█▌        | 14/93 [05:05<19:53, 15.11s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Completed 4900/30807 blocks\n",
            "Completed 4910/30807 blocks\n",
            "Completed 4920/30807 blocks\n",
            "Completed 4930/30807 blocks\n",
            "Completed 4940/30807 blocks\n",
            "Completed 4950/30807 blocks\n",
            "Completed 4960/30807 blocks\n",
            "Completed 4970/30807 blocks\n",
            "Completed 4980/30807 blocks\n",
            "Completed 4990/30807 blocks\n",
            "Completed 5000/30807 blocks\n",
            "Completed 5010/30807 blocks\n",
            "Completed 5020/30807 blocks\n",
            "Completed 5030/30807 blocks\n",
            "Completed 5040/30807 blocks\n",
            "Completed 5050/30807 blocks\n",
            "Completed 5060/30807 blocks\n",
            "Completed 5070/30807 blocks\n",
            "Completed 5080/30807 blocks\n",
            "Completed 5090/30807 blocks\n",
            "Completed 5100/30807 blocks\n",
            "Completed 5110/30807 blocks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing drugs:  16%|█▌        | 15/93 [05:19<19:08, 14.73s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Completed 5120/30807 blocks\n",
            "Completed 5130/30807 blocks\n",
            "Completed 5140/30807 blocks\n",
            "Completed 5150/30807 blocks\n",
            "Completed 5160/30807 blocks\n",
            "Completed 5170/30807 blocks\n",
            "Completed 5180/30807 blocks\n",
            "Completed 5190/30807 blocks\n",
            "Completed 5200/30807 blocks\n",
            "Completed 5210/30807 blocks\n",
            "Completed 5220/30807 blocks\n",
            "Completed 5230/30807 blocks\n",
            "Completed 5240/30807 blocks\n",
            "Completed 5250/30807 blocks\n",
            "Completed 5260/30807 blocks\n",
            "Completed 5270/30807 blocks\n",
            "Completed 5280/30807 blocks\n",
            "Completed 5290/30807 blocks\n",
            "Completed 5300/30807 blocks\n",
            "Completed 5310/30807 blocks\n",
            "Completed 5320/30807 blocks\n",
            "Completed 5330/30807 blocks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing drugs:  17%|█▋        | 16/93 [05:33<18:31, 14.44s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Completed 5340/30807 blocks\n",
            "Completed 5350/30807 blocks\n",
            "Completed 5360/30807 blocks\n",
            "Completed 5370/30807 blocks\n",
            "Completed 5380/30807 blocks\n",
            "Completed 5390/30807 blocks\n",
            "Completed 5400/30807 blocks\n",
            "Completed 5410/30807 blocks\n",
            "Completed 5420/30807 blocks\n",
            "Completed 5430/30807 blocks\n",
            "Completed 5440/30807 blocks\n",
            "Completed 5450/30807 blocks\n",
            "Completed 5460/30807 blocks\n",
            "Completed 5470/30807 blocks\n",
            "Completed 5480/30807 blocks\n",
            "Completed 5490/30807 blocks\n",
            "Completed 5500/30807 blocks\n",
            "Completed 5510/30807 blocks\n",
            "Completed 5520/30807 blocks\n",
            "Completed 5530/30807 blocks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing drugs:  18%|█▊        | 17/93 [05:45<17:42, 13.98s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Completed 5540/30807 blocks\n",
            "Completed 5550/30807 blocks\n",
            "Completed 5560/30807 blocks\n",
            "Completed 5570/30807 blocks\n",
            "Completed 5580/30807 blocks\n",
            "Completed 5590/30807 blocks\n",
            "Completed 5600/30807 blocks\n",
            "Completed 5610/30807 blocks\n",
            "Completed 5620/30807 blocks\n",
            "Completed 5630/30807 blocks\n",
            "Completed 5640/30807 blocks\n",
            "Completed 5650/30807 blocks\n",
            "Completed 5660/30807 blocks\n",
            "Completed 5670/30807 blocks\n",
            "Completed 5680/30807 blocks\n",
            "Completed 5690/30807 blocks\n",
            "Completed 5700/30807 blocks\n",
            "Completed 5710/30807 blocks\n",
            "Completed 5720/30807 blocks\n",
            "Completed 5730/30807 blocks\n",
            "Completed 5740/30807 blocks\n",
            "Completed 5750/30807 blocks\n",
            "Completed 5760/30807 blocks\n",
            "Completed 5770/30807 blocks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing drugs:  19%|█▉        | 18/93 [06:00<17:50, 14.28s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Completed 5780/30807 blocks\n",
            "Completed 5790/30807 blocks\n",
            "Completed 5800/30807 blocks\n",
            "Completed 5810/30807 blocks\n",
            "Completed 5820/30807 blocks\n",
            "Completed 5830/30807 blocks\n",
            "Completed 5840/30807 blocks\n",
            "Completed 5850/30807 blocks\n",
            "Completed 5860/30807 blocks\n",
            "Completed 5870/30807 blocks\n",
            "Completed 5880/30807 blocks\n",
            "Completed 5890/30807 blocks\n",
            "Completed 5900/30807 blocks\n",
            "Completed 5910/30807 blocks\n",
            "Completed 5920/30807 blocks\n",
            "Completed 5930/30807 blocks\n",
            "Completed 5940/30807 blocks\n",
            "Completed 5950/30807 blocks\n",
            "Completed 5960/30807 blocks\n",
            "Completed 5970/30807 blocks\n",
            "Completed 5980/30807 blocks\n",
            "Completed 5990/30807 blocks\n",
            "Completed 6000/30807 blocks\n",
            "Completed 6010/30807 blocks\n",
            "Completed 6020/30807 blocks\n",
            "Completed 6030/30807 blocks\n",
            "Completed 6040/30807 blocks\n",
            "Completed 6050/30807 blocks\n",
            "Completed 6060/30807 blocks\n",
            "Completed 6070/30807 blocks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing drugs:  20%|██        | 19/93 [06:19<19:12, 15.57s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Completed 6080/30807 blocks\n",
            "Completed 6090/30807 blocks\n",
            "Completed 6100/30807 blocks\n",
            "Completed 6110/30807 blocks\n",
            "Completed 6120/30807 blocks\n",
            "Completed 6130/30807 blocks\n",
            "Completed 6140/30807 blocks\n",
            "Completed 6150/30807 blocks\n",
            "Completed 6160/30807 blocks\n",
            "Completed 6170/30807 blocks\n",
            "Completed 6180/30807 blocks\n",
            "Completed 6190/30807 blocks\n",
            "Completed 6200/30807 blocks\n",
            "Completed 6210/30807 blocks\n",
            "Completed 6220/30807 blocks\n",
            "Completed 6230/30807 blocks\n",
            "Completed 6240/30807 blocks\n",
            "Completed 6250/30807 blocks\n",
            "Completed 6260/30807 blocks\n",
            "Completed 6270/30807 blocks\n",
            "Completed 6280/30807 blocks\n",
            "Completed 6290/30807 blocks\n",
            "Completed 6300/30807 blocks\n",
            "Completed 6310/30807 blocks\n",
            "Completed 6320/30807 blocks\n",
            "Completed 6330/30807 blocks\n",
            "Completed 6340/30807 blocks\n",
            "Completed 6350/30807 blocks\n",
            "Completed 6360/30807 blocks\n",
            "Completed 6370/30807 blocks\n",
            "Completed 6380/30807 blocks\n",
            "Completed 6390/30807 blocks\n",
            "Completed 6400/30807 blocks\n",
            "Completed 6410/30807 blocks\n",
            "Completed 6420/30807 blocks\n",
            "Completed 6430/30807 blocks\n",
            "Completed 6440/30807 blocks\n",
            "Completed 6450/30807 blocks\n",
            "Completed 6460/30807 blocks\n",
            "Completed 6470/30807 blocks\n",
            "Completed 6480/30807 blocks\n",
            "Completed 6490/30807 blocks\n",
            "Completed 6500/30807 blocks\n",
            "Completed 6510/30807 blocks\n",
            "Completed 6520/30807 blocks\n",
            "Completed 6530/30807 blocks\n",
            "Completed 6540/30807 blocks\n",
            "Completed 6550/30807 blocks\n",
            "Completed 6560/30807 blocks\n",
            "Completed 6570/30807 blocks\n",
            "Completed 6580/30807 blocks\n",
            "Completed 6590/30807 blocks\n",
            "Completed 6600/30807 blocks\n",
            "Completed 6610/30807 blocks\n",
            "Completed 6620/30807 blocks\n",
            "Completed 6630/30807 blocks\n",
            "Completed 6640/30807 blocks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing drugs:  22%|██▏       | 20/93 [06:54<26:03, 21.41s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Completed 6650/30807 blocks\n",
            "Completed 6660/30807 blocks\n",
            "Completed 6670/30807 blocks\n",
            "Completed 6680/30807 blocks\n",
            "Completed 6690/30807 blocks\n",
            "Completed 6700/30807 blocks\n",
            "Completed 6710/30807 blocks\n",
            "Completed 6720/30807 blocks\n",
            "Completed 6730/30807 blocks\n",
            "Completed 6740/30807 blocks\n",
            "Completed 6750/30807 blocks\n",
            "Completed 6760/30807 blocks\n",
            "Completed 6770/30807 blocks\n",
            "Completed 6780/30807 blocks\n",
            "Completed 6790/30807 blocks\n",
            "Completed 6800/30807 blocks\n",
            "Completed 6810/30807 blocks\n",
            "Completed 6820/30807 blocks\n",
            "Completed 6830/30807 blocks\n",
            "Completed 6840/30807 blocks\n",
            "Completed 6850/30807 blocks\n",
            "Completed 6860/30807 blocks\n",
            "Completed 6870/30807 blocks\n",
            "Completed 6880/30807 blocks\n",
            "Completed 6890/30807 blocks\n",
            "Completed 6900/30807 blocks\n",
            "Completed 6910/30807 blocks\n",
            "Completed 6920/30807 blocks\n",
            "Completed 6930/30807 blocks\n",
            "Completed 6940/30807 blocks\n",
            "Completed 6950/30807 blocks\n",
            "Completed 6960/30807 blocks\n",
            "Completed 6970/30807 blocks\n",
            "Completed 6980/30807 blocks\n",
            "Completed 6990/30807 blocks\n",
            "Completed 7000/30807 blocks\n",
            "Completed 7010/30807 blocks\n",
            "Completed 7020/30807 blocks\n",
            "Completed 7030/30807 blocks\n",
            "Completed 7040/30807 blocks\n",
            "Completed 7050/30807 blocks\n",
            "Completed 7060/30807 blocks\n",
            "Completed 7070/30807 blocks\n",
            "Completed 7080/30807 blocks\n",
            "Completed 7090/30807 blocks\n",
            "Completed 7100/30807 blocks\n",
            "Completed 7110/30807 blocks\n",
            "Completed 7120/30807 blocks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing drugs:  23%|██▎       | 21/93 [07:24<28:43, 23.94s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Completed 7130/30807 blocks\n",
            "Completed 7140/30807 blocks\n",
            "Completed 7150/30807 blocks\n",
            "Completed 7160/30807 blocks\n",
            "Completed 7170/30807 blocks\n",
            "Completed 7180/30807 blocks\n",
            "Completed 7190/30807 blocks\n",
            "Completed 7200/30807 blocks\n",
            "Completed 7210/30807 blocks\n",
            "Completed 7220/30807 blocks\n",
            "Completed 7230/30807 blocks\n",
            "Completed 7240/30807 blocks\n",
            "Completed 7250/30807 blocks\n",
            "Completed 7260/30807 blocks\n",
            "Completed 7270/30807 blocks\n",
            "Completed 7280/30807 blocks\n",
            "Completed 7290/30807 blocks\n",
            "Completed 7300/30807 blocks\n",
            "Completed 7310/30807 blocks\n",
            "Completed 7320/30807 blocks\n",
            "Completed 7330/30807 blocks\n",
            "Completed 7340/30807 blocks\n",
            "Completed 7350/30807 blocks\n",
            "Completed 7360/30807 blocks\n",
            "Completed 7370/30807 blocks\n",
            "Completed 7380/30807 blocks\n",
            "Completed 7390/30807 blocks\n",
            "Completed 7400/30807 blocks\n",
            "Completed 7410/30807 blocks\n",
            "Completed 7420/30807 blocks\n",
            "Completed 7430/30807 blocks\n",
            "Completed 7440/30807 blocks\n",
            "Completed 7450/30807 blocks\n",
            "Completed 7460/30807 blocks\n",
            "Completed 7470/30807 blocks\n",
            "Completed 7480/30807 blocks\n",
            "Completed 7490/30807 blocks\n",
            "Completed 7500/30807 blocks\n",
            "Completed 7510/30807 blocks\n",
            "Completed 7520/30807 blocks\n",
            "Completed 7530/30807 blocks\n",
            "Completed 7540/30807 blocks\n",
            "Completed 7550/30807 blocks\n",
            "Completed 7560/30807 blocks\n",
            "Completed 7570/30807 blocks\n",
            "Completed 7580/30807 blocks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing drugs:  24%|██▎       | 22/93 [07:53<30:04, 25.41s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Completed 7590/30807 blocks\n",
            "Completed 7600/30807 blocks\n",
            "Completed 7610/30807 blocks\n",
            "Completed 7620/30807 blocks\n",
            "Completed 7630/30807 blocks\n",
            "Completed 7640/30807 blocks\n",
            "Completed 7650/30807 blocks\n",
            "Completed 7660/30807 blocks\n",
            "Completed 7670/30807 blocks\n",
            "Completed 7680/30807 blocks\n",
            "Completed 7690/30807 blocks\n",
            "Completed 7700/30807 blocks\n",
            "Completed 7710/30807 blocks\n",
            "Completed 7720/30807 blocks\n",
            "Completed 7730/30807 blocks\n",
            "Completed 7740/30807 blocks\n",
            "Completed 7750/30807 blocks\n",
            "Completed 7760/30807 blocks\n",
            "Completed 7770/30807 blocks\n",
            "Completed 7780/30807 blocks\n",
            "Completed 7790/30807 blocks\n",
            "Completed 7800/30807 blocks\n",
            "Completed 7810/30807 blocks\n",
            "Completed 7820/30807 blocks\n",
            "Completed 7830/30807 blocks\n",
            "Completed 7840/30807 blocks\n",
            "Completed 7850/30807 blocks\n",
            "Completed 7860/30807 blocks\n",
            "Completed 7870/30807 blocks\n",
            "Completed 7880/30807 blocks\n",
            "Completed 7890/30807 blocks\n",
            "Completed 7900/30807 blocks\n",
            "Completed 7910/30807 blocks\n",
            "Completed 7920/30807 blocks\n",
            "Completed 7930/30807 blocks\n",
            "Completed 7940/30807 blocks\n",
            "Completed 7950/30807 blocks\n",
            "Completed 7960/30807 blocks\n",
            "Completed 7970/30807 blocks\n",
            "Completed 7980/30807 blocks\n",
            "Completed 7990/30807 blocks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing drugs:  25%|██▍       | 23/93 [08:18<29:40, 25.44s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Completed 8000/30807 blocks\n",
            "Completed 8010/30807 blocks\n",
            "Completed 8020/30807 blocks\n",
            "Completed 8030/30807 blocks\n",
            "Completed 8040/30807 blocks\n",
            "Completed 8050/30807 blocks\n",
            "Completed 8060/30807 blocks\n",
            "Completed 8070/30807 blocks\n",
            "Completed 8080/30807 blocks\n",
            "Completed 8090/30807 blocks\n",
            "Completed 8100/30807 blocks\n",
            "Completed 8110/30807 blocks\n",
            "Completed 8120/30807 blocks\n",
            "Completed 8130/30807 blocks\n",
            "Completed 8140/30807 blocks\n",
            "Completed 8150/30807 blocks\n",
            "Completed 8160/30807 blocks\n",
            "Completed 8170/30807 blocks\n",
            "Completed 8180/30807 blocks\n",
            "Completed 8190/30807 blocks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing drugs:  26%|██▌       | 24/93 [08:30<24:40, 21.46s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Completed 8200/30807 blocks\n",
            "Completed 8210/30807 blocks\n",
            "Completed 8220/30807 blocks\n",
            "Completed 8230/30807 blocks\n",
            "Completed 8240/30807 blocks\n",
            "Completed 8250/30807 blocks\n",
            "Completed 8260/30807 blocks\n",
            "Completed 8270/30807 blocks\n",
            "Completed 8280/30807 blocks\n",
            "Completed 8290/30807 blocks\n",
            "Completed 8300/30807 blocks\n",
            "Completed 8310/30807 blocks\n",
            "Completed 8320/30807 blocks\n",
            "Completed 8330/30807 blocks\n",
            "Completed 8340/30807 blocks\n",
            "Completed 8350/30807 blocks\n",
            "Completed 8360/30807 blocks\n",
            "Completed 8370/30807 blocks\n",
            "Completed 8380/30807 blocks\n",
            "Completed 8390/30807 blocks\n",
            "Completed 8400/30807 blocks\n",
            "Completed 8410/30807 blocks\n",
            "Completed 8420/30807 blocks\n",
            "Completed 8430/30807 blocks\n",
            "Completed 8440/30807 blocks\n",
            "Completed 8450/30807 blocks\n",
            "Completed 8460/30807 blocks\n",
            "Completed 8470/30807 blocks\n",
            "Completed 8480/30807 blocks\n",
            "Completed 8490/30807 blocks\n",
            "Completed 8500/30807 blocks\n",
            "Completed 8510/30807 blocks\n",
            "Completed 8520/30807 blocks\n",
            "Completed 8530/30807 blocks\n",
            "Completed 8540/30807 blocks\n",
            "Completed 8550/30807 blocks\n",
            "Completed 8560/30807 blocks\n",
            "Completed 8570/30807 blocks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing drugs:  27%|██▋       | 25/93 [08:54<25:05, 22.14s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Completed 8580/30807 blocks\n",
            "Completed 8590/30807 blocks\n",
            "Completed 8600/30807 blocks\n",
            "Completed 8610/30807 blocks\n",
            "Completed 8620/30807 blocks\n",
            "Completed 8630/30807 blocks\n",
            "Completed 8640/30807 blocks\n",
            "Completed 8650/30807 blocks\n",
            "Completed 8660/30807 blocks\n",
            "Completed 8670/30807 blocks\n",
            "Completed 8680/30807 blocks\n",
            "Completed 8690/30807 blocks\n",
            "Completed 8700/30807 blocks\n",
            "Completed 8710/30807 blocks\n",
            "Completed 8720/30807 blocks\n",
            "Completed 8730/30807 blocks\n",
            "Completed 8740/30807 blocks\n",
            "Completed 8750/30807 blocks\n",
            "Completed 8760/30807 blocks\n",
            "Completed 8770/30807 blocks\n",
            "Completed 8780/30807 blocks\n",
            "Completed 8790/30807 blocks\n",
            "Completed 8800/30807 blocks\n",
            "Completed 8810/30807 blocks\n",
            "Completed 8820/30807 blocks\n",
            "Completed 8830/30807 blocks\n",
            "Completed 8840/30807 blocks\n",
            "Completed 8850/30807 blocks\n",
            "Completed 8860/30807 blocks\n",
            "Completed 8870/30807 blocks\n",
            "Completed 8880/30807 blocks\n",
            "Completed 8890/30807 blocks\n",
            "Completed 8900/30807 blocks\n",
            "Completed 8910/30807 blocks\n",
            "Completed 8920/30807 blocks\n",
            "Completed 8930/30807 blocks\n",
            "Completed 8940/30807 blocks\n",
            "Completed 8950/30807 blocks\n",
            "Completed 8960/30807 blocks\n",
            "Completed 8970/30807 blocks\n",
            "Completed 8980/30807 blocks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing drugs:  28%|██▊       | 26/93 [09:19<25:36, 22.94s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Completed 8990/30807 blocks\n",
            "Completed 9000/30807 blocks\n",
            "Completed 9010/30807 blocks\n",
            "Completed 9020/30807 blocks\n",
            "Completed 9030/30807 blocks\n",
            "Completed 9040/30807 blocks\n",
            "Completed 9050/30807 blocks\n",
            "Completed 9060/30807 blocks\n",
            "Completed 9070/30807 blocks\n",
            "Completed 9080/30807 blocks\n",
            "Completed 9090/30807 blocks\n",
            "Completed 9100/30807 blocks\n",
            "Completed 9110/30807 blocks\n",
            "Completed 9120/30807 blocks\n",
            "Completed 9130/30807 blocks\n",
            "Completed 9140/30807 blocks\n",
            "Completed 9150/30807 blocks\n",
            "Completed 9160/30807 blocks\n",
            "Completed 9170/30807 blocks\n",
            "Completed 9180/30807 blocks\n",
            "Completed 9190/30807 blocks\n",
            "Completed 9200/30807 blocks\n",
            "Completed 9210/30807 blocks\n",
            "Completed 9220/30807 blocks\n",
            "Completed 9230/30807 blocks\n",
            "Completed 9240/30807 blocks\n",
            "Completed 9250/30807 blocks\n",
            "Completed 9260/30807 blocks\n",
            "Completed 9270/30807 blocks\n",
            "Completed 9280/30807 blocks\n",
            "Completed 9290/30807 blocks\n",
            "Completed 9300/30807 blocks\n",
            "Completed 9310/30807 blocks\n",
            "Completed 9320/30807 blocks\n",
            "Completed 9330/30807 blocks\n",
            "Completed 9340/30807 blocks\n",
            "Completed 9350/30807 blocks\n",
            "Completed 9360/30807 blocks\n",
            "Completed 9370/30807 blocks\n",
            "Completed 9380/30807 blocks\n",
            "Completed 9390/30807 blocks\n",
            "Completed 9400/30807 blocks\n",
            "Completed 9410/30807 blocks\n",
            "Completed 9420/30807 blocks\n",
            "Completed 9430/30807 blocks\n",
            "Completed 9440/30807 blocks\n",
            "Completed 9450/30807 blocks\n",
            "Completed 9460/30807 blocks\n",
            "Completed 9470/30807 blocks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing drugs:  29%|██▉       | 27/93 [09:50<27:56, 25.40s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Completed 9480/30807 blocks\n",
            "Completed 9490/30807 blocks\n",
            "Completed 9500/30807 blocks\n",
            "Completed 9510/30807 blocks\n",
            "Completed 9520/30807 blocks\n",
            "Completed 9530/30807 blocks\n",
            "Completed 9540/30807 blocks\n",
            "Completed 9550/30807 blocks\n",
            "Completed 9560/30807 blocks\n",
            "Completed 9570/30807 blocks\n",
            "Completed 9580/30807 blocks\n",
            "Completed 9590/30807 blocks\n",
            "Completed 9600/30807 blocks\n",
            "Completed 9610/30807 blocks\n",
            "Completed 9620/30807 blocks\n",
            "Completed 9630/30807 blocks\n",
            "Completed 9640/30807 blocks\n",
            "Completed 9650/30807 blocks\n",
            "Completed 9660/30807 blocks\n",
            "Completed 9670/30807 blocks\n",
            "Completed 9680/30807 blocks\n",
            "Completed 9690/30807 blocks\n",
            "Completed 9700/30807 blocks\n",
            "Completed 9710/30807 blocks\n",
            "Completed 9720/30807 blocks\n",
            "Completed 9730/30807 blocks\n",
            "Completed 9740/30807 blocks\n",
            "Completed 9750/30807 blocks\n",
            "Completed 9760/30807 blocks\n",
            "Completed 9770/30807 blocks\n",
            "Completed 9780/30807 blocks\n",
            "Completed 9790/30807 blocks\n",
            "Completed 9800/30807 blocks\n",
            "Completed 9810/30807 blocks\n",
            "Completed 9820/30807 blocks\n",
            "Completed 9830/30807 blocks\n",
            "Completed 9840/30807 blocks\n",
            "Completed 9850/30807 blocks\n",
            "Completed 9860/30807 blocks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing drugs:  30%|███       | 28/93 [10:15<27:11, 25.10s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Completed 9870/30807 blocks\n",
            "Completed 9880/30807 blocks\n",
            "Completed 9890/30807 blocks\n",
            "Completed 9900/30807 blocks\n",
            "Completed 9910/30807 blocks\n",
            "Completed 9920/30807 blocks\n",
            "Completed 9930/30807 blocks\n",
            "Completed 9940/30807 blocks\n",
            "Completed 9950/30807 blocks\n",
            "Completed 9960/30807 blocks\n",
            "Completed 9970/30807 blocks\n",
            "Completed 9980/30807 blocks\n",
            "Completed 9990/30807 blocks\n",
            "Completed 10000/30807 blocks\n",
            "Completed 10010/30807 blocks\n",
            "Completed 10020/30807 blocks\n",
            "Completed 10030/30807 blocks\n",
            "Completed 10040/30807 blocks\n",
            "Completed 10050/30807 blocks\n",
            "Completed 10060/30807 blocks\n",
            "Completed 10070/30807 blocks\n",
            "Completed 10080/30807 blocks\n",
            "Completed 10090/30807 blocks\n",
            "Completed 10100/30807 blocks\n",
            "Completed 10110/30807 blocks\n",
            "Completed 10120/30807 blocks\n",
            "Completed 10130/30807 blocks\n",
            "Completed 10140/30807 blocks\n",
            "Completed 10150/30807 blocks\n",
            "Completed 10160/30807 blocks\n",
            "Completed 10170/30807 blocks\n",
            "Completed 10180/30807 blocks\n",
            "Completed 10190/30807 blocks\n",
            "Completed 10200/30807 blocks\n",
            "Completed 10210/30807 blocks\n",
            "Completed 10220/30807 blocks\n",
            "Completed 10230/30807 blocks\n",
            "Completed 10240/30807 blocks\n",
            "Completed 10250/30807 blocks\n",
            "Completed 10260/30807 blocks\n",
            "Completed 10270/30807 blocks\n",
            "Completed 10280/30807 blocks\n",
            "Completed 10290/30807 blocks\n",
            "Completed 10300/30807 blocks\n",
            "Completed 10310/30807 blocks\n",
            "Completed 10320/30807 blocks\n",
            "Completed 10330/30807 blocks\n",
            "Completed 10340/30807 blocks\n",
            "Completed 10350/30807 blocks\n",
            "Completed 10360/30807 blocks\n",
            "Completed 10370/30807 blocks\n",
            "Completed 10380/30807 blocks\n",
            "Completed 10390/30807 blocks\n",
            "Completed 10400/30807 blocks\n",
            "Completed 10410/30807 blocks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing drugs:  31%|███       | 29/93 [10:49<29:39, 27.80s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Completed 10420/30807 blocks\n",
            "Completed 10430/30807 blocks\n",
            "Completed 10440/30807 blocks\n",
            "Completed 10450/30807 blocks\n",
            "Completed 10460/30807 blocks\n",
            "Completed 10470/30807 blocks\n",
            "Completed 10480/30807 blocks\n",
            "Completed 10490/30807 blocks\n",
            "Completed 10500/30807 blocks\n",
            "Completed 10510/30807 blocks\n",
            "Completed 10520/30807 blocks\n",
            "Completed 10530/30807 blocks\n",
            "Completed 10540/30807 blocks\n",
            "Completed 10550/30807 blocks\n",
            "Completed 10560/30807 blocks\n",
            "Completed 10570/30807 blocks\n",
            "Completed 10580/30807 blocks\n",
            "Completed 10590/30807 blocks\n",
            "Completed 10600/30807 blocks\n",
            "Completed 10610/30807 blocks\n",
            "Completed 10620/30807 blocks\n",
            "Completed 10630/30807 blocks\n",
            "Completed 10640/30807 blocks\n",
            "Completed 10650/30807 blocks\n",
            "Completed 10660/30807 blocks\n",
            "Completed 10670/30807 blocks\n",
            "Completed 10680/30807 blocks\n",
            "Completed 10690/30807 blocks\n",
            "Completed 10700/30807 blocks\n",
            "Completed 10710/30807 blocks\n",
            "Completed 10720/30807 blocks\n",
            "Completed 10730/30807 blocks\n",
            "Completed 10740/30807 blocks\n",
            "Completed 10750/30807 blocks\n",
            "Completed 10760/30807 blocks\n",
            "Completed 10770/30807 blocks\n",
            "Completed 10780/30807 blocks\n",
            "Completed 10790/30807 blocks\n",
            "Completed 10800/30807 blocks\n",
            "Completed 10810/30807 blocks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing drugs:  32%|███▏      | 30/93 [11:13<28:07, 26.78s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Completed 10820/30807 blocks\n",
            "Completed 10830/30807 blocks\n",
            "Completed 10840/30807 blocks\n",
            "Completed 10850/30807 blocks\n",
            "Completed 10860/30807 blocks\n",
            "Completed 10870/30807 blocks\n",
            "Completed 10880/30807 blocks\n",
            "Completed 10890/30807 blocks\n",
            "Completed 10900/30807 blocks\n",
            "Completed 10910/30807 blocks\n",
            "Completed 10920/30807 blocks\n",
            "Completed 10930/30807 blocks\n",
            "Completed 10940/30807 blocks\n",
            "Completed 10950/30807 blocks\n",
            "Completed 10960/30807 blocks\n",
            "Completed 10970/30807 blocks\n",
            "Completed 10980/30807 blocks\n",
            "Completed 10990/30807 blocks\n",
            "Completed 11000/30807 blocks\n",
            "Completed 11010/30807 blocks\n",
            "Completed 11020/30807 blocks\n",
            "Completed 11030/30807 blocks\n",
            "Completed 11040/30807 blocks\n",
            "Completed 11050/30807 blocks\n",
            "Completed 11060/30807 blocks\n",
            "Completed 11070/30807 blocks\n",
            "Completed 11080/30807 blocks\n",
            "Completed 11090/30807 blocks\n",
            "Completed 11100/30807 blocks\n",
            "Completed 11110/30807 blocks\n",
            "Completed 11120/30807 blocks\n",
            "Completed 11130/30807 blocks\n",
            "Completed 11140/30807 blocks\n",
            "Completed 11150/30807 blocks\n",
            "Completed 11160/30807 blocks\n",
            "Completed 11170/30807 blocks\n",
            "Completed 11180/30807 blocks\n",
            "Completed 11190/30807 blocks\n",
            "Completed 11200/30807 blocks\n",
            "Completed 11210/30807 blocks\n",
            "Completed 11220/30807 blocks\n",
            "Completed 11230/30807 blocks\n",
            "Completed 11240/30807 blocks\n",
            "Completed 11250/30807 blocks\n",
            "Completed 11260/30807 blocks\n",
            "Completed 11270/30807 blocks\n",
            "Completed 11280/30807 blocks\n",
            "Completed 11290/30807 blocks\n",
            "Completed 11300/30807 blocks\n",
            "Completed 11310/30807 blocks\n",
            "Completed 11320/30807 blocks\n",
            "Completed 11330/30807 blocks\n",
            "Completed 11340/30807 blocks\n",
            "Completed 11350/30807 blocks\n",
            "Completed 11360/30807 blocks\n",
            "Completed 11370/30807 blocks\n",
            "Completed 11380/30807 blocks\n",
            "Completed 11390/30807 blocks\n",
            "Completed 11400/30807 blocks\n",
            "Completed 11410/30807 blocks\n",
            "Completed 11420/30807 blocks\n",
            "Completed 11430/30807 blocks\n",
            "Completed 11440/30807 blocks\n",
            "Completed 11450/30807 blocks\n",
            "Completed 11460/30807 blocks\n",
            "Completed 11470/30807 blocks\n",
            "Completed 11480/30807 blocks\n",
            "Completed 11490/30807 blocks\n",
            "Completed 11500/30807 blocks\n",
            "Completed 11510/30807 blocks\n",
            "Completed 11520/30807 blocks\n",
            "Completed 11530/30807 blocks\n",
            "Completed 11540/30807 blocks\n",
            "Completed 11550/30807 blocks\n",
            "Completed 11560/30807 blocks\n",
            "Completed 11570/30807 blocks\n",
            "Completed 11580/30807 blocks\n",
            "Completed 11590/30807 blocks\n",
            "Completed 11600/30807 blocks\n",
            "Completed 11610/30807 blocks\n",
            "Completed 11620/30807 blocks\n",
            "Completed 11630/30807 blocks\n",
            "Completed 11640/30807 blocks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing drugs:  33%|███▎      | 31/93 [12:05<35:23, 34.25s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Completed 11650/30807 blocks\n",
            "Completed 11660/30807 blocks\n",
            "Completed 11670/30807 blocks\n",
            "Completed 11680/30807 blocks\n",
            "Completed 11690/30807 blocks\n",
            "Completed 11700/30807 blocks\n",
            "Completed 11710/30807 blocks\n",
            "Completed 11720/30807 blocks\n",
            "Completed 11730/30807 blocks\n",
            "Completed 11740/30807 blocks\n",
            "Completed 11750/30807 blocks\n",
            "Completed 11760/30807 blocks\n",
            "Completed 11770/30807 blocks\n",
            "Completed 11780/30807 blocks\n",
            "Completed 11790/30807 blocks\n",
            "Completed 11800/30807 blocks\n",
            "Completed 11810/30807 blocks\n",
            "Completed 11820/30807 blocks\n",
            "Completed 11830/30807 blocks\n",
            "Completed 11840/30807 blocks\n",
            "Completed 11850/30807 blocks\n",
            "Completed 11860/30807 blocks\n",
            "Completed 11870/30807 blocks\n",
            "Completed 11880/30807 blocks\n",
            "Completed 11890/30807 blocks\n",
            "Completed 11900/30807 blocks\n",
            "Completed 11910/30807 blocks\n",
            "Completed 11920/30807 blocks\n",
            "Completed 11930/30807 blocks\n",
            "Completed 11940/30807 blocks\n",
            "Completed 11950/30807 blocks\n",
            "Completed 11960/30807 blocks\n",
            "Completed 11970/30807 blocks\n",
            "Completed 11980/30807 blocks\n",
            "Completed 11990/30807 blocks\n",
            "Completed 12000/30807 blocks\n",
            "Completed 12010/30807 blocks\n",
            "Completed 12020/30807 blocks\n",
            "Completed 12030/30807 blocks\n",
            "Completed 12040/30807 blocks\n",
            "Completed 12050/30807 blocks\n",
            "Completed 12060/30807 blocks\n",
            "Completed 12070/30807 blocks\n",
            "Completed 12080/30807 blocks\n",
            "Completed 12090/30807 blocks\n",
            "Completed 12100/30807 blocks\n",
            "Completed 12110/30807 blocks\n",
            "Completed 12120/30807 blocks\n",
            "Completed 12130/30807 blocks\n",
            "Completed 12140/30807 blocks\n",
            "Completed 12150/30807 blocks\n",
            "Completed 12160/30807 blocks\n",
            "Completed 12170/30807 blocks\n",
            "Completed 12180/30807 blocks\n",
            "Completed 12190/30807 blocks\n",
            "Completed 12200/30807 blocks\n",
            "Completed 12210/30807 blocks\n",
            "Completed 12220/30807 blocks\n",
            "Completed 12230/30807 blocks\n",
            "Completed 12240/30807 blocks\n",
            "Completed 12250/30807 blocks\n",
            "Completed 12260/30807 blocks\n",
            "Completed 12270/30807 blocks\n",
            "Completed 12280/30807 blocks\n",
            "Completed 12290/30807 blocks\n",
            "Completed 12300/30807 blocks\n",
            "Completed 12310/30807 blocks\n",
            "Completed 12320/30807 blocks\n",
            "Completed 12330/30807 blocks\n",
            "Completed 12340/30807 blocks\n",
            "Completed 12350/30807 blocks\n",
            "Completed 12360/30807 blocks\n",
            "Completed 12370/30807 blocks\n",
            "Completed 12380/30807 blocks\n",
            "Completed 12390/30807 blocks\n",
            "Completed 12400/30807 blocks\n",
            "Completed 12410/30807 blocks\n",
            "Completed 12420/30807 blocks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing drugs:  34%|███▍      | 32/93 [12:53<39:02, 38.40s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Completed 12430/30807 blocks\n",
            "Completed 12440/30807 blocks\n",
            "Completed 12450/30807 blocks\n",
            "Completed 12460/30807 blocks\n",
            "Completed 12470/30807 blocks\n",
            "Completed 12480/30807 blocks\n",
            "Completed 12490/30807 blocks\n",
            "Completed 12500/30807 blocks\n",
            "Completed 12510/30807 blocks\n",
            "Completed 12520/30807 blocks\n",
            "Completed 12530/30807 blocks\n",
            "Completed 12540/30807 blocks\n",
            "Completed 12550/30807 blocks\n",
            "Completed 12560/30807 blocks\n",
            "Completed 12570/30807 blocks\n",
            "Completed 12580/30807 blocks\n",
            "Completed 12590/30807 blocks\n",
            "Completed 12600/30807 blocks\n",
            "Completed 12610/30807 blocks\n",
            "Completed 12620/30807 blocks\n",
            "Completed 12630/30807 blocks\n",
            "Completed 12640/30807 blocks\n",
            "Completed 12650/30807 blocks\n",
            "Completed 12660/30807 blocks\n",
            "Completed 12670/30807 blocks\n",
            "Completed 12680/30807 blocks\n",
            "Completed 12690/30807 blocks\n",
            "Completed 12700/30807 blocks\n",
            "Completed 12710/30807 blocks\n",
            "Completed 12720/30807 blocks\n",
            "Completed 12730/30807 blocks\n",
            "Completed 12740/30807 blocks\n",
            "Completed 12750/30807 blocks\n",
            "Completed 12760/30807 blocks\n",
            "Completed 12770/30807 blocks\n",
            "Completed 12780/30807 blocks\n",
            "Completed 12790/30807 blocks\n",
            "Completed 12800/30807 blocks\n",
            "Completed 12810/30807 blocks\n",
            "Completed 12820/30807 blocks\n",
            "Completed 12830/30807 blocks\n",
            "Completed 12840/30807 blocks\n",
            "Completed 12850/30807 blocks\n",
            "Completed 12860/30807 blocks\n",
            "Completed 12870/30807 blocks\n",
            "Completed 12880/30807 blocks\n",
            "Completed 12890/30807 blocks\n",
            "Completed 12900/30807 blocks\n",
            "Completed 12910/30807 blocks\n",
            "Completed 12920/30807 blocks\n",
            "Completed 12930/30807 blocks\n",
            "Completed 12940/30807 blocks\n",
            "Completed 12950/30807 blocks\n",
            "Completed 12960/30807 blocks\n",
            "Completed 12970/30807 blocks\n",
            "Completed 12980/30807 blocks\n",
            "Completed 12990/30807 blocks\n",
            "Completed 13000/30807 blocks\n",
            "Completed 13010/30807 blocks\n",
            "Completed 13020/30807 blocks\n",
            "Completed 13030/30807 blocks\n",
            "Completed 13040/30807 blocks\n",
            "Completed 13050/30807 blocks\n",
            "Completed 13060/30807 blocks\n",
            "Completed 13070/30807 blocks\n",
            "Completed 13080/30807 blocks\n",
            "Completed 13090/30807 blocks\n",
            "Completed 13100/30807 blocks\n",
            "Completed 13110/30807 blocks\n",
            "Completed 13120/30807 blocks\n",
            "Completed 13130/30807 blocks\n",
            "Completed 13140/30807 blocks\n",
            "Completed 13150/30807 blocks\n",
            "Completed 13160/30807 blocks\n",
            "Completed 13170/30807 blocks\n",
            "Completed 13180/30807 blocks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing drugs:  35%|███▌      | 33/93 [13:41<41:22, 41.38s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Completed 13190/30807 blocks\n",
            "Completed 13200/30807 blocks\n",
            "Completed 13210/30807 blocks\n",
            "Completed 13220/30807 blocks\n",
            "Completed 13230/30807 blocks\n",
            "Completed 13240/30807 blocks\n",
            "Completed 13250/30807 blocks\n",
            "Completed 13260/30807 blocks\n",
            "Completed 13270/30807 blocks\n",
            "Completed 13280/30807 blocks\n",
            "Completed 13290/30807 blocks\n",
            "Completed 13300/30807 blocks\n",
            "Completed 13310/30807 blocks\n",
            "Completed 13320/30807 blocks\n",
            "Completed 13330/30807 blocks\n",
            "Completed 13340/30807 blocks\n",
            "Completed 13350/30807 blocks\n",
            "Completed 13360/30807 blocks\n",
            "Completed 13370/30807 blocks\n",
            "Completed 13380/30807 blocks\n",
            "Completed 13390/30807 blocks\n",
            "Completed 13400/30807 blocks\n",
            "Completed 13410/30807 blocks\n",
            "Completed 13420/30807 blocks\n",
            "Completed 13430/30807 blocks\n",
            "Completed 13440/30807 blocks\n",
            "Completed 13450/30807 blocks\n",
            "Completed 13460/30807 blocks\n",
            "Completed 13470/30807 blocks\n",
            "Completed 13480/30807 blocks\n",
            "Completed 13490/30807 blocks\n",
            "Completed 13500/30807 blocks\n",
            "Completed 13510/30807 blocks\n",
            "Completed 13520/30807 blocks\n",
            "Completed 13530/30807 blocks\n",
            "Completed 13540/30807 blocks\n",
            "Completed 13550/30807 blocks\n",
            "Completed 13560/30807 blocks\n",
            "Completed 13570/30807 blocks\n",
            "Completed 13580/30807 blocks\n",
            "Completed 13590/30807 blocks\n",
            "Completed 13600/30807 blocks\n",
            "Completed 13610/30807 blocks\n",
            "Completed 13620/30807 blocks\n",
            "Completed 13630/30807 blocks\n",
            "Completed 13640/30807 blocks\n",
            "Completed 13650/30807 blocks\n",
            "Completed 13660/30807 blocks\n",
            "Completed 13670/30807 blocks\n",
            "Completed 13680/30807 blocks\n",
            "Completed 13690/30807 blocks\n",
            "Completed 13700/30807 blocks\n",
            "Completed 13710/30807 blocks\n",
            "Completed 13720/30807 blocks\n",
            "Completed 13730/30807 blocks\n",
            "Completed 13740/30807 blocks\n",
            "Completed 13750/30807 blocks\n",
            "Completed 13760/30807 blocks\n",
            "Completed 13770/30807 blocks\n",
            "Completed 13780/30807 blocks\n",
            "Completed 13790/30807 blocks\n",
            "Completed 13800/30807 blocks\n",
            "Completed 13810/30807 blocks\n",
            "Completed 13820/30807 blocks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing drugs:  37%|███▋      | 34/93 [14:20<40:04, 40.75s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Completed 13830/30807 blocks\n",
            "Completed 13840/30807 blocks\n",
            "Completed 13850/30807 blocks\n",
            "Completed 13860/30807 blocks\n",
            "Completed 13870/30807 blocks\n",
            "Completed 13880/30807 blocks\n",
            "Completed 13890/30807 blocks\n",
            "Completed 13900/30807 blocks\n",
            "Completed 13910/30807 blocks\n",
            "Completed 13920/30807 blocks\n",
            "Completed 13930/30807 blocks\n",
            "Completed 13940/30807 blocks\n",
            "Completed 13950/30807 blocks\n",
            "Completed 13960/30807 blocks\n",
            "Completed 13970/30807 blocks\n",
            "Completed 13980/30807 blocks\n",
            "Completed 13990/30807 blocks\n",
            "Completed 14000/30807 blocks\n",
            "Completed 14010/30807 blocks\n",
            "Completed 14020/30807 blocks\n",
            "Completed 14030/30807 blocks\n",
            "Completed 14040/30807 blocks\n",
            "Completed 14050/30807 blocks\n",
            "Completed 14060/30807 blocks\n",
            "Completed 14070/30807 blocks\n",
            "Completed 14080/30807 blocks\n",
            "Completed 14090/30807 blocks\n",
            "Completed 14100/30807 blocks\n",
            "Completed 14110/30807 blocks\n",
            "Completed 14120/30807 blocks\n",
            "Completed 14130/30807 blocks\n",
            "Completed 14140/30807 blocks\n",
            "Completed 14150/30807 blocks\n",
            "Completed 14160/30807 blocks\n",
            "Completed 14170/30807 blocks\n",
            "Completed 14180/30807 blocks\n",
            "Completed 14190/30807 blocks\n",
            "Completed 14200/30807 blocks\n",
            "Completed 14210/30807 blocks\n",
            "Completed 14220/30807 blocks\n",
            "Completed 14230/30807 blocks\n",
            "Completed 14240/30807 blocks\n",
            "Completed 14250/30807 blocks\n",
            "Completed 14260/30807 blocks\n",
            "Completed 14270/30807 blocks\n",
            "Completed 14280/30807 blocks\n",
            "Completed 14290/30807 blocks\n",
            "Completed 14300/30807 blocks\n",
            "Completed 14310/30807 blocks\n",
            "Completed 14320/30807 blocks\n",
            "Completed 14330/30807 blocks\n",
            "Completed 14340/30807 blocks\n",
            "Completed 14350/30807 blocks\n",
            "Completed 14360/30807 blocks\n",
            "Completed 14370/30807 blocks\n",
            "Completed 14380/30807 blocks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing drugs:  38%|███▊      | 35/93 [14:55<37:37, 38.93s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Completed 14390/30807 blocks\n",
            "Completed 14400/30807 blocks\n",
            "Completed 14410/30807 blocks\n",
            "Completed 14420/30807 blocks\n",
            "Completed 14430/30807 blocks\n",
            "Completed 14440/30807 blocks\n",
            "Completed 14450/30807 blocks\n",
            "Completed 14460/30807 blocks\n",
            "Completed 14470/30807 blocks\n",
            "Completed 14480/30807 blocks\n",
            "Completed 14490/30807 blocks\n",
            "Completed 14500/30807 blocks\n",
            "Completed 14510/30807 blocks\n",
            "Completed 14520/30807 blocks\n",
            "Completed 14530/30807 blocks\n",
            "Completed 14540/30807 blocks\n",
            "Completed 14550/30807 blocks\n",
            "Completed 14560/30807 blocks\n",
            "Completed 14570/30807 blocks\n",
            "Completed 14580/30807 blocks\n",
            "Completed 14590/30807 blocks\n",
            "Completed 14600/30807 blocks\n",
            "Completed 14610/30807 blocks\n",
            "Completed 14620/30807 blocks\n",
            "Completed 14630/30807 blocks\n",
            "Completed 14640/30807 blocks\n",
            "Completed 14650/30807 blocks\n",
            "Completed 14660/30807 blocks\n",
            "Completed 14670/30807 blocks\n",
            "Completed 14680/30807 blocks\n",
            "Completed 14690/30807 blocks\n",
            "Completed 14700/30807 blocks\n",
            "Completed 14710/30807 blocks\n",
            "Completed 14720/30807 blocks\n",
            "Completed 14730/30807 blocks\n",
            "Completed 14740/30807 blocks\n",
            "Completed 14750/30807 blocks\n",
            "Completed 14760/30807 blocks\n",
            "Completed 14770/30807 blocks\n",
            "Completed 14780/30807 blocks\n",
            "Completed 14790/30807 blocks\n",
            "Completed 14800/30807 blocks\n",
            "Completed 14810/30807 blocks\n",
            "Completed 14820/30807 blocks\n",
            "Completed 14830/30807 blocks\n",
            "Completed 14840/30807 blocks\n",
            "Completed 14850/30807 blocks\n",
            "Completed 14860/30807 blocks\n",
            "Completed 14870/30807 blocks\n",
            "Completed 14880/30807 blocks\n",
            "Completed 14890/30807 blocks\n",
            "Completed 14900/30807 blocks\n",
            "Completed 14910/30807 blocks\n",
            "Completed 14920/30807 blocks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing drugs:  39%|███▊      | 36/93 [15:28<35:17, 37.15s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Completed 14930/30807 blocks\n",
            "Completed 14940/30807 blocks\n",
            "Completed 14950/30807 blocks\n",
            "Completed 14960/30807 blocks\n",
            "Completed 14970/30807 blocks\n",
            "Completed 14980/30807 blocks\n",
            "Completed 14990/30807 blocks\n",
            "Completed 15000/30807 blocks\n",
            "Completed 15010/30807 blocks\n",
            "Completed 15020/30807 blocks\n",
            "Completed 15030/30807 blocks\n",
            "Completed 15040/30807 blocks\n",
            "Completed 15050/30807 blocks\n",
            "Completed 15060/30807 blocks\n",
            "Completed 15070/30807 blocks\n",
            "Completed 15080/30807 blocks\n",
            "Completed 15090/30807 blocks\n",
            "Completed 15100/30807 blocks\n",
            "Completed 15110/30807 blocks\n",
            "Completed 15120/30807 blocks\n",
            "Completed 15130/30807 blocks\n",
            "Completed 15140/30807 blocks\n",
            "Completed 15150/30807 blocks\n",
            "Completed 15160/30807 blocks\n",
            "Completed 15170/30807 blocks\n",
            "Completed 15180/30807 blocks\n",
            "Completed 15190/30807 blocks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing drugs:  40%|███▉      | 37/93 [15:44<28:45, 30.82s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Completed 15200/30807 blocks\n",
            "Completed 15210/30807 blocks\n",
            "Completed 15220/30807 blocks\n",
            "Completed 15230/30807 blocks\n",
            "Completed 15240/30807 blocks\n",
            "Completed 15250/30807 blocks\n",
            "Completed 15260/30807 blocks\n",
            "Completed 15270/30807 blocks\n",
            "Completed 15280/30807 blocks\n",
            "Completed 15290/30807 blocks\n",
            "Completed 15300/30807 blocks\n",
            "Completed 15310/30807 blocks\n",
            "Completed 15320/30807 blocks\n",
            "Completed 15330/30807 blocks\n",
            "Completed 15340/30807 blocks\n",
            "Completed 15350/30807 blocks\n",
            "Completed 15360/30807 blocks\n",
            "Completed 15370/30807 blocks\n",
            "Completed 15380/30807 blocks\n",
            "Completed 15390/30807 blocks\n",
            "Completed 15400/30807 blocks\n",
            "Completed 15410/30807 blocks\n",
            "Completed 15420/30807 blocks\n",
            "Completed 15430/30807 blocks\n",
            "Completed 15440/30807 blocks\n",
            "Completed 15450/30807 blocks\n",
            "Completed 15460/30807 blocks\n",
            "Completed 15470/30807 blocks\n",
            "Completed 15480/30807 blocks\n",
            "Completed 15490/30807 blocks\n",
            "Completed 15500/30807 blocks\n",
            "Completed 15510/30807 blocks\n",
            "Completed 15520/30807 blocks\n",
            "Completed 15530/30807 blocks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing drugs:  41%|████      | 38/93 [16:06<25:47, 28.14s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Completed 15540/30807 blocks\n",
            "Completed 15550/30807 blocks\n",
            "Completed 15560/30807 blocks\n",
            "Completed 15570/30807 blocks\n",
            "Completed 15580/30807 blocks\n",
            "Completed 15590/30807 blocks\n",
            "Completed 15600/30807 blocks\n",
            "Completed 15610/30807 blocks\n",
            "Completed 15620/30807 blocks\n",
            "Completed 15630/30807 blocks\n",
            "Completed 15640/30807 blocks\n",
            "Completed 15650/30807 blocks\n",
            "Completed 15660/30807 blocks\n",
            "Completed 15670/30807 blocks\n",
            "Completed 15680/30807 blocks\n",
            "Completed 15690/30807 blocks\n",
            "Completed 15700/30807 blocks\n",
            "Completed 15710/30807 blocks\n",
            "Completed 15720/30807 blocks\n",
            "Completed 15730/30807 blocks\n",
            "Completed 15740/30807 blocks\n",
            "Completed 15750/30807 blocks\n",
            "Completed 15760/30807 blocks\n",
            "Completed 15770/30807 blocks\n",
            "Completed 15780/30807 blocks\n",
            "Completed 15790/30807 blocks\n",
            "Completed 15800/30807 blocks\n",
            "Completed 15810/30807 blocks\n",
            "Completed 15820/30807 blocks\n",
            "Completed 15830/30807 blocks\n",
            "Completed 15840/30807 blocks\n",
            "Completed 15850/30807 blocks\n",
            "Completed 15860/30807 blocks\n",
            "Completed 15870/30807 blocks\n",
            "Completed 15880/30807 blocks\n",
            "Completed 15890/30807 blocks\n",
            "Completed 15900/30807 blocks\n",
            "Completed 15910/30807 blocks\n",
            "Completed 15920/30807 blocks\n",
            "Completed 15930/30807 blocks\n",
            "Completed 15940/30807 blocks\n",
            "Completed 15950/30807 blocks\n",
            "Completed 15960/30807 blocks\n",
            "Completed 15970/30807 blocks\n",
            "Completed 15980/30807 blocks\n",
            "Completed 15990/30807 blocks\n",
            "Completed 16000/30807 blocks\n",
            "Completed 16010/30807 blocks\n",
            "Completed 16020/30807 blocks\n",
            "Completed 16030/30807 blocks\n",
            "Completed 16040/30807 blocks\n",
            "Completed 16050/30807 blocks\n",
            "Completed 16060/30807 blocks\n",
            "Completed 16070/30807 blocks\n",
            "Completed 16080/30807 blocks\n",
            "Completed 16090/30807 blocks\n",
            "Completed 16100/30807 blocks\n",
            "Completed 16110/30807 blocks\n",
            "Completed 16120/30807 blocks\n",
            "Completed 16130/30807 blocks\n",
            "Completed 16140/30807 blocks\n",
            "Completed 16150/30807 blocks\n",
            "Completed 16160/30807 blocks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing drugs:  42%|████▏     | 39/93 [16:45<28:09, 31.28s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Completed 16170/30807 blocks\n",
            "Completed 16180/30807 blocks\n",
            "Completed 16190/30807 blocks\n",
            "Completed 16200/30807 blocks\n",
            "Completed 16210/30807 blocks\n",
            "Completed 16220/30807 blocks\n",
            "Completed 16230/30807 blocks\n",
            "Completed 16240/30807 blocks\n",
            "Completed 16250/30807 blocks\n",
            "Completed 16260/30807 blocks\n",
            "Completed 16270/30807 blocks\n",
            "Completed 16280/30807 blocks\n",
            "Completed 16290/30807 blocks\n",
            "Completed 16300/30807 blocks\n",
            "Completed 16310/30807 blocks\n",
            "Completed 16320/30807 blocks\n",
            "Completed 16330/30807 blocks\n",
            "Completed 16340/30807 blocks\n",
            "Completed 16350/30807 blocks\n",
            "Completed 16360/30807 blocks\n",
            "Completed 16370/30807 blocks\n",
            "Completed 16380/30807 blocks\n",
            "Completed 16390/30807 blocks\n",
            "Completed 16400/30807 blocks\n",
            "Completed 16410/30807 blocks\n",
            "Completed 16420/30807 blocks\n",
            "Completed 16430/30807 blocks\n",
            "Completed 16440/30807 blocks\n",
            "Completed 16450/30807 blocks\n",
            "Completed 16460/30807 blocks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing drugs:  43%|████▎     | 40/93 [17:04<24:22, 27.60s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Completed 16470/30807 blocks\n",
            "Completed 16480/30807 blocks\n",
            "Completed 16490/30807 blocks\n",
            "Completed 16500/30807 blocks\n",
            "Completed 16510/30807 blocks\n",
            "Completed 16520/30807 blocks\n",
            "Completed 16530/30807 blocks\n",
            "Completed 16540/30807 blocks\n",
            "Completed 16550/30807 blocks\n",
            "Completed 16560/30807 blocks\n",
            "Completed 16570/30807 blocks\n",
            "Completed 16580/30807 blocks\n",
            "Completed 16590/30807 blocks\n",
            "Completed 16600/30807 blocks\n",
            "Completed 16610/30807 blocks\n",
            "Completed 16620/30807 blocks\n",
            "Completed 16630/30807 blocks\n",
            "Completed 16640/30807 blocks\n",
            "Completed 16650/30807 blocks\n",
            "Completed 16660/30807 blocks\n",
            "Completed 16670/30807 blocks\n",
            "Completed 16680/30807 blocks\n",
            "Completed 16690/30807 blocks\n",
            "Completed 16700/30807 blocks\n",
            "Completed 16710/30807 blocks\n",
            "Completed 16720/30807 blocks\n",
            "Completed 16730/30807 blocks\n",
            "Completed 16740/30807 blocks\n",
            "Completed 16750/30807 blocks\n",
            "Completed 16760/30807 blocks\n",
            "Completed 16770/30807 blocks\n",
            "Completed 16780/30807 blocks\n",
            "Completed 16790/30807 blocks\n",
            "Completed 16800/30807 blocks\n",
            "Completed 16810/30807 blocks\n",
            "Completed 16820/30807 blocks\n",
            "Completed 16830/30807 blocks\n",
            "Completed 16840/30807 blocks\n",
            "Completed 16850/30807 blocks\n",
            "Completed 16860/30807 blocks\n",
            "Completed 16870/30807 blocks\n",
            "Completed 16880/30807 blocks\n",
            "Completed 16890/30807 blocks\n",
            "Completed 16900/30807 blocks\n",
            "Completed 16910/30807 blocks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing drugs:  44%|████▍     | 41/93 [17:31<23:53, 27.57s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Completed 16920/30807 blocks\n",
            "Completed 16930/30807 blocks\n",
            "Completed 16940/30807 blocks\n",
            "Completed 16950/30807 blocks\n",
            "Completed 16960/30807 blocks\n",
            "Completed 16970/30807 blocks\n",
            "Completed 16980/30807 blocks\n",
            "Completed 16990/30807 blocks\n",
            "Completed 17000/30807 blocks\n",
            "Completed 17010/30807 blocks\n",
            "Completed 17020/30807 blocks\n",
            "Completed 17030/30807 blocks\n",
            "Completed 17040/30807 blocks\n",
            "Completed 17050/30807 blocks\n",
            "Completed 17060/30807 blocks\n",
            "Completed 17070/30807 blocks\n",
            "Completed 17080/30807 blocks\n",
            "Completed 17090/30807 blocks\n",
            "Completed 17100/30807 blocks\n",
            "Completed 17110/30807 blocks\n",
            "Completed 17120/30807 blocks\n",
            "Completed 17130/30807 blocks\n",
            "Completed 17140/30807 blocks\n",
            "Completed 17150/30807 blocks\n",
            "Completed 17160/30807 blocks\n",
            "Completed 17170/30807 blocks\n",
            "Completed 17180/30807 blocks\n",
            "Completed 17190/30807 blocks\n",
            "Completed 17200/30807 blocks\n",
            "Completed 17210/30807 blocks\n",
            "Completed 17220/30807 blocks\n",
            "Completed 17230/30807 blocks\n",
            "Completed 17240/30807 blocks\n",
            "Completed 17250/30807 blocks\n",
            "Completed 17260/30807 blocks\n",
            "Completed 17270/30807 blocks\n",
            "Completed 17280/30807 blocks\n",
            "Completed 17290/30807 blocks\n",
            "Completed 17300/30807 blocks\n",
            "Completed 17310/30807 blocks\n",
            "Completed 17320/30807 blocks\n",
            "Completed 17330/30807 blocks\n",
            "Completed 17340/30807 blocks\n",
            "Completed 17350/30807 blocks\n",
            "Completed 17360/30807 blocks\n",
            "Completed 17370/30807 blocks\n",
            "Completed 17380/30807 blocks\n",
            "Completed 17390/30807 blocks\n",
            "Completed 17400/30807 blocks\n",
            "Completed 17410/30807 blocks\n",
            "Completed 17420/30807 blocks\n",
            "Completed 17430/30807 blocks\n",
            "Completed 17440/30807 blocks\n",
            "Completed 17450/30807 blocks\n",
            "Completed 17460/30807 blocks\n",
            "Completed 17470/30807 blocks\n",
            "Completed 17480/30807 blocks\n",
            "Completed 17490/30807 blocks\n",
            "Completed 17500/30807 blocks\n",
            "Completed 17510/30807 blocks\n",
            "Completed 17520/30807 blocks\n",
            "Completed 17530/30807 blocks\n",
            "Completed 17540/30807 blocks\n",
            "Completed 17550/30807 blocks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing drugs:  45%|████▌     | 42/93 [18:10<26:24, 31.08s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Completed 17560/30807 blocks\n",
            "Completed 17570/30807 blocks\n",
            "Completed 17580/30807 blocks\n",
            "Completed 17590/30807 blocks\n",
            "Completed 17600/30807 blocks\n",
            "Completed 17610/30807 blocks\n",
            "Completed 17620/30807 blocks\n",
            "Completed 17630/30807 blocks\n",
            "Completed 17640/30807 blocks\n",
            "Completed 17650/30807 blocks\n",
            "Completed 17660/30807 blocks\n",
            "Completed 17670/30807 blocks\n",
            "Completed 17680/30807 blocks\n",
            "Completed 17690/30807 blocks\n",
            "Completed 17700/30807 blocks\n",
            "Completed 17710/30807 blocks\n",
            "Completed 17720/30807 blocks\n",
            "Completed 17730/30807 blocks\n",
            "Completed 17740/30807 blocks\n",
            "Completed 17750/30807 blocks\n",
            "Completed 17760/30807 blocks\n",
            "Completed 17770/30807 blocks\n",
            "Completed 17780/30807 blocks\n",
            "Completed 17790/30807 blocks\n",
            "Completed 17800/30807 blocks\n",
            "Completed 17810/30807 blocks\n",
            "Completed 17820/30807 blocks\n",
            "Completed 17830/30807 blocks\n",
            "Completed 17840/30807 blocks\n",
            "Completed 17850/30807 blocks\n",
            "Completed 17860/30807 blocks\n",
            "Completed 17870/30807 blocks\n",
            "Completed 17880/30807 blocks\n",
            "Completed 17890/30807 blocks\n",
            "Completed 17900/30807 blocks\n",
            "Completed 17910/30807 blocks\n",
            "Completed 17920/30807 blocks\n",
            "Completed 17930/30807 blocks\n",
            "Completed 17940/30807 blocks\n",
            "Completed 17950/30807 blocks\n",
            "Completed 17960/30807 blocks\n",
            "Completed 17970/30807 blocks\n",
            "Completed 17980/30807 blocks\n",
            "Completed 17990/30807 blocks\n",
            "Completed 18000/30807 blocks\n",
            "Completed 18010/30807 blocks\n",
            "Completed 18020/30807 blocks\n",
            "Completed 18030/30807 blocks\n",
            "Completed 18040/30807 blocks\n",
            "Completed 18050/30807 blocks\n",
            "Completed 18060/30807 blocks\n",
            "Completed 18070/30807 blocks\n",
            "Completed 18080/30807 blocks\n",
            "Completed 18090/30807 blocks\n",
            "Completed 18100/30807 blocks\n",
            "Completed 18110/30807 blocks\n",
            "Completed 18120/30807 blocks\n",
            "Completed 18130/30807 blocks\n",
            "Completed 18140/30807 blocks\n",
            "Completed 18150/30807 blocks\n",
            "Completed 18160/30807 blocks\n",
            "Completed 18170/30807 blocks\n",
            "Completed 18180/30807 blocks\n",
            "Completed 18190/30807 blocks\n",
            "Completed 18200/30807 blocks\n",
            "Completed 18210/30807 blocks\n",
            "Completed 18220/30807 blocks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing drugs:  46%|████▌     | 43/93 [18:53<28:41, 34.43s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Completed 18230/30807 blocks\n",
            "Completed 18240/30807 blocks\n",
            "Completed 18250/30807 blocks\n",
            "Completed 18260/30807 blocks\n",
            "Completed 18270/30807 blocks\n",
            "Completed 18280/30807 blocks\n",
            "Completed 18290/30807 blocks\n",
            "Completed 18300/30807 blocks\n",
            "Completed 18310/30807 blocks\n",
            "Completed 18320/30807 blocks\n",
            "Completed 18330/30807 blocks\n",
            "Completed 18340/30807 blocks\n",
            "Completed 18350/30807 blocks\n",
            "Completed 18360/30807 blocks\n",
            "Completed 18370/30807 blocks\n",
            "Completed 18380/30807 blocks\n",
            "Completed 18390/30807 blocks\n",
            "Completed 18400/30807 blocks\n",
            "Completed 18410/30807 blocks\n",
            "Completed 18420/30807 blocks\n",
            "Completed 18430/30807 blocks\n",
            "Completed 18440/30807 blocks\n",
            "Completed 18450/30807 blocks\n",
            "Completed 18460/30807 blocks\n",
            "Completed 18470/30807 blocks\n",
            "Completed 18480/30807 blocks\n",
            "Completed 18490/30807 blocks\n",
            "Completed 18500/30807 blocks\n",
            "Completed 18510/30807 blocks\n",
            "Completed 18520/30807 blocks\n",
            "Completed 18530/30807 blocks\n",
            "Completed 18540/30807 blocks\n",
            "Completed 18550/30807 blocks\n",
            "Completed 18560/30807 blocks\n",
            "Completed 18570/30807 blocks\n",
            "Completed 18580/30807 blocks\n",
            "Completed 18590/30807 blocks\n",
            "Completed 18600/30807 blocks\n",
            "Completed 18610/30807 blocks\n",
            "Completed 18620/30807 blocks\n",
            "Completed 18630/30807 blocks\n",
            "Completed 18640/30807 blocks\n",
            "Completed 18650/30807 blocks\n",
            "Completed 18660/30807 blocks\n",
            "Completed 18670/30807 blocks\n",
            "Completed 18680/30807 blocks\n",
            "Completed 18690/30807 blocks\n",
            "Completed 18700/30807 blocks\n",
            "Completed 18710/30807 blocks\n",
            "Completed 18720/30807 blocks\n",
            "Completed 18730/30807 blocks\n",
            "Completed 18740/30807 blocks\n",
            "Completed 18750/30807 blocks\n",
            "Completed 18760/30807 blocks\n",
            "Completed 18770/30807 blocks\n",
            "Completed 18780/30807 blocks\n",
            "Completed 18790/30807 blocks\n",
            "Completed 18800/30807 blocks\n",
            "Completed 18810/30807 blocks\n",
            "Completed 18820/30807 blocks\n",
            "Completed 18830/30807 blocks\n",
            "Completed 18840/30807 blocks\n",
            "Completed 18850/30807 blocks\n",
            "Completed 18860/30807 blocks\n",
            "Completed 18870/30807 blocks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing drugs:  47%|████▋     | 44/93 [19:32<29:26, 36.06s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Completed 18880/30807 blocks\n",
            "Completed 18890/30807 blocks\n",
            "Completed 18900/30807 blocks\n",
            "Completed 18910/30807 blocks\n",
            "Completed 18920/30807 blocks\n",
            "Completed 18930/30807 blocks\n",
            "Completed 18940/30807 blocks\n",
            "Completed 18950/30807 blocks\n",
            "Completed 18960/30807 blocks\n",
            "Completed 18970/30807 blocks\n",
            "Completed 18980/30807 blocks\n",
            "Completed 18990/30807 blocks\n",
            "Completed 19000/30807 blocks\n",
            "Completed 19010/30807 blocks\n",
            "Completed 19020/30807 blocks\n",
            "Completed 19030/30807 blocks\n",
            "Completed 19040/30807 blocks\n",
            "Completed 19050/30807 blocks\n",
            "Completed 19060/30807 blocks\n",
            "Completed 19070/30807 blocks\n",
            "Completed 19080/30807 blocks\n",
            "Completed 19090/30807 blocks\n",
            "Completed 19100/30807 blocks\n",
            "Completed 19110/30807 blocks\n",
            "Completed 19120/30807 blocks\n",
            "Completed 19130/30807 blocks\n",
            "Completed 19140/30807 blocks\n",
            "Completed 19150/30807 blocks\n",
            "Completed 19160/30807 blocks\n",
            "Completed 19170/30807 blocks\n",
            "Completed 19180/30807 blocks\n",
            "Completed 19190/30807 blocks\n",
            "Completed 19200/30807 blocks\n",
            "Completed 19210/30807 blocks\n",
            "Completed 19220/30807 blocks\n",
            "Completed 19230/30807 blocks\n",
            "Completed 19240/30807 blocks\n",
            "Completed 19250/30807 blocks\n",
            "Completed 19260/30807 blocks\n",
            "Completed 19270/30807 blocks\n",
            "Completed 19280/30807 blocks\n",
            "Completed 19290/30807 blocks\n",
            "Completed 19300/30807 blocks\n",
            "Completed 19310/30807 blocks\n",
            "Completed 19320/30807 blocks\n",
            "Completed 19330/30807 blocks\n",
            "Completed 19340/30807 blocks\n",
            "Completed 19350/30807 blocks\n",
            "Completed 19360/30807 blocks\n",
            "Completed 19370/30807 blocks\n",
            "Completed 19380/30807 blocks\n",
            "Completed 19390/30807 blocks\n",
            "Completed 19400/30807 blocks\n",
            "Completed 19410/30807 blocks\n",
            "Completed 19420/30807 blocks\n",
            "Completed 19430/30807 blocks\n",
            "Completed 19440/30807 blocks\n",
            "Completed 19450/30807 blocks\n",
            "Completed 19460/30807 blocks\n",
            "Completed 19470/30807 blocks\n",
            "Completed 19480/30807 blocks\n",
            "Completed 19490/30807 blocks\n",
            "Completed 19500/30807 blocks\n",
            "Completed 19510/30807 blocks\n",
            "Completed 19520/30807 blocks\n",
            "Completed 19530/30807 blocks\n",
            "Completed 19540/30807 blocks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing drugs:  48%|████▊     | 45/93 [20:14<30:10, 37.72s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Completed 19550/30807 blocks\n",
            "Completed 19560/30807 blocks\n",
            "Completed 19570/30807 blocks\n",
            "Completed 19580/30807 blocks\n",
            "Completed 19590/30807 blocks\n",
            "Completed 19600/30807 blocks\n",
            "Completed 19610/30807 blocks\n",
            "Completed 19620/30807 blocks\n",
            "Completed 19630/30807 blocks\n",
            "Completed 19640/30807 blocks\n",
            "Completed 19650/30807 blocks\n",
            "Completed 19660/30807 blocks\n",
            "Completed 19670/30807 blocks\n",
            "Completed 19680/30807 blocks\n",
            "Completed 19690/30807 blocks\n",
            "Completed 19700/30807 blocks\n",
            "Completed 19710/30807 blocks\n",
            "Completed 19720/30807 blocks\n",
            "Completed 19730/30807 blocks\n",
            "Completed 19740/30807 blocks\n",
            "Completed 19750/30807 blocks\n",
            "Completed 19760/30807 blocks\n",
            "Completed 19770/30807 blocks\n",
            "Completed 19780/30807 blocks\n",
            "Completed 19790/30807 blocks\n",
            "Completed 19800/30807 blocks\n",
            "Completed 19810/30807 blocks\n",
            "Completed 19820/30807 blocks\n",
            "Completed 19830/30807 blocks\n",
            "Completed 19840/30807 blocks\n",
            "Completed 19850/30807 blocks\n",
            "Completed 19860/30807 blocks\n",
            "Completed 19870/30807 blocks\n",
            "Completed 19880/30807 blocks\n",
            "Completed 19890/30807 blocks\n",
            "Completed 19900/30807 blocks\n",
            "Completed 19910/30807 blocks\n",
            "Completed 19920/30807 blocks\n",
            "Completed 19930/30807 blocks\n",
            "Completed 19940/30807 blocks\n",
            "Completed 19950/30807 blocks\n",
            "Completed 19960/30807 blocks\n",
            "Completed 19970/30807 blocks\n",
            "Completed 19980/30807 blocks\n",
            "Completed 19990/30807 blocks\n",
            "Completed 20000/30807 blocks\n",
            "Completed 20010/30807 blocks\n",
            "Completed 20020/30807 blocks\n",
            "Completed 20030/30807 blocks\n",
            "Completed 20040/30807 blocks\n",
            "Completed 20050/30807 blocks\n",
            "Completed 20060/30807 blocks\n",
            "Completed 20070/30807 blocks\n",
            "Completed 20080/30807 blocks\n",
            "Completed 20090/30807 blocks\n",
            "Completed 20100/30807 blocks\n",
            "Completed 20110/30807 blocks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing drugs:  49%|████▉     | 46/93 [20:50<29:02, 37.07s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Completed 20120/30807 blocks\n",
            "Completed 20130/30807 blocks\n",
            "Completed 20140/30807 blocks\n",
            "Completed 20150/30807 blocks\n",
            "Completed 20160/30807 blocks\n",
            "Completed 20170/30807 blocks\n",
            "Completed 20180/30807 blocks\n",
            "Completed 20190/30807 blocks\n",
            "Completed 20200/30807 blocks\n",
            "Completed 20210/30807 blocks\n",
            "Completed 20220/30807 blocks\n",
            "Completed 20230/30807 blocks\n",
            "Completed 20240/30807 blocks\n",
            "Completed 20250/30807 blocks\n",
            "Completed 20260/30807 blocks\n",
            "Completed 20270/30807 blocks\n",
            "Completed 20280/30807 blocks\n",
            "Completed 20290/30807 blocks\n",
            "Completed 20300/30807 blocks\n",
            "Completed 20310/30807 blocks\n",
            "Completed 20320/30807 blocks\n",
            "Completed 20330/30807 blocks\n",
            "Completed 20340/30807 blocks\n",
            "Completed 20350/30807 blocks\n",
            "Completed 20360/30807 blocks\n",
            "Completed 20370/30807 blocks\n",
            "Completed 20380/30807 blocks\n",
            "Completed 20390/30807 blocks\n",
            "Completed 20400/30807 blocks\n",
            "Completed 20410/30807 blocks\n",
            "Completed 20420/30807 blocks\n",
            "Completed 20430/30807 blocks\n",
            "Completed 20440/30807 blocks\n",
            "Completed 20450/30807 blocks\n",
            "Completed 20460/30807 blocks\n",
            "Completed 20470/30807 blocks\n",
            "Completed 20480/30807 blocks\n",
            "Completed 20490/30807 blocks\n",
            "Completed 20500/30807 blocks\n",
            "Completed 20510/30807 blocks\n",
            "Completed 20520/30807 blocks\n",
            "Completed 20530/30807 blocks\n",
            "Completed 20540/30807 blocks\n",
            "Completed 20550/30807 blocks\n",
            "Completed 20560/30807 blocks\n",
            "Completed 20570/30807 blocks\n",
            "Completed 20580/30807 blocks\n",
            "Completed 20590/30807 blocks\n",
            "Completed 20600/30807 blocks\n",
            "Completed 20610/30807 blocks\n",
            "Completed 20620/30807 blocks\n",
            "Completed 20630/30807 blocks\n",
            "Completed 20640/30807 blocks\n",
            "Completed 20650/30807 blocks\n",
            "Completed 20660/30807 blocks\n",
            "Completed 20670/30807 blocks\n",
            "Completed 20680/30807 blocks\n",
            "Completed 20690/30807 blocks\n",
            "Completed 20700/30807 blocks\n",
            "Completed 20710/30807 blocks\n",
            "Completed 20720/30807 blocks\n",
            "Completed 20730/30807 blocks\n",
            "Completed 20740/30807 blocks\n",
            "Completed 20750/30807 blocks\n",
            "Completed 20760/30807 blocks\n",
            "Completed 20770/30807 blocks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing drugs:  51%|█████     | 47/93 [21:30<29:16, 38.19s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Completed 20780/30807 blocks\n",
            "Completed 20790/30807 blocks\n",
            "Completed 20800/30807 blocks\n",
            "Completed 20810/30807 blocks\n",
            "Completed 20820/30807 blocks\n",
            "Completed 20830/30807 blocks\n",
            "Completed 20840/30807 blocks\n",
            "Completed 20850/30807 blocks\n",
            "Completed 20860/30807 blocks\n",
            "Completed 20870/30807 blocks\n",
            "Completed 20880/30807 blocks\n",
            "Completed 20890/30807 blocks\n",
            "Completed 20900/30807 blocks\n",
            "Completed 20910/30807 blocks\n",
            "Completed 20920/30807 blocks\n",
            "Completed 20930/30807 blocks\n",
            "Completed 20940/30807 blocks\n",
            "Completed 20950/30807 blocks\n",
            "Completed 20960/30807 blocks\n",
            "Completed 20970/30807 blocks\n",
            "Completed 20980/30807 blocks\n",
            "Completed 20990/30807 blocks\n",
            "Completed 21000/30807 blocks\n",
            "Completed 21010/30807 blocks\n",
            "Completed 21020/30807 blocks\n",
            "Completed 21030/30807 blocks\n",
            "Completed 21040/30807 blocks\n",
            "Completed 21050/30807 blocks\n",
            "Completed 21060/30807 blocks\n",
            "Completed 21070/30807 blocks\n",
            "Completed 21080/30807 blocks\n",
            "Completed 21090/30807 blocks\n",
            "Completed 21100/30807 blocks\n",
            "Completed 21110/30807 blocks\n",
            "Completed 21120/30807 blocks\n",
            "Completed 21130/30807 blocks\n",
            "Completed 21140/30807 blocks\n",
            "Completed 21150/30807 blocks\n",
            "Completed 21160/30807 blocks\n",
            "Completed 21170/30807 blocks\n",
            "Completed 21180/30807 blocks\n",
            "Completed 21190/30807 blocks\n",
            "Completed 21200/30807 blocks\n",
            "Completed 21210/30807 blocks\n",
            "Completed 21220/30807 blocks\n",
            "Completed 21230/30807 blocks\n",
            "Completed 21240/30807 blocks\n",
            "Completed 21250/30807 blocks\n",
            "Completed 21260/30807 blocks\n",
            "Completed 21270/30807 blocks\n",
            "Completed 21280/30807 blocks\n",
            "Completed 21290/30807 blocks\n",
            "Completed 21300/30807 blocks\n",
            "Completed 21310/30807 blocks\n",
            "Completed 21320/30807 blocks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing drugs:  52%|█████▏    | 48/93 [22:04<27:40, 36.91s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Completed 21330/30807 blocks\n",
            "Completed 21340/30807 blocks\n",
            "Completed 21350/30807 blocks\n",
            "Completed 21360/30807 blocks\n",
            "Completed 21370/30807 blocks\n",
            "Completed 21380/30807 blocks\n",
            "Completed 21390/30807 blocks\n",
            "Completed 21400/30807 blocks\n",
            "Completed 21410/30807 blocks\n",
            "Completed 21420/30807 blocks\n",
            "Completed 21430/30807 blocks\n",
            "Completed 21440/30807 blocks\n",
            "Completed 21450/30807 blocks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing drugs:  53%|█████▎    | 49/93 [22:12<20:38, 28.15s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Completed 21460/30807 blocks\n",
            "Completed 21470/30807 blocks\n",
            "Completed 21480/30807 blocks\n",
            "Completed 21490/30807 blocks\n",
            "Completed 21500/30807 blocks\n",
            "Completed 21510/30807 blocks\n",
            "Completed 21520/30807 blocks\n",
            "Completed 21530/30807 blocks\n",
            "Completed 21540/30807 blocks\n",
            "Completed 21550/30807 blocks\n",
            "Completed 21560/30807 blocks\n",
            "Completed 21570/30807 blocks\n",
            "Completed 21580/30807 blocks\n",
            "Completed 21590/30807 blocks\n",
            "Completed 21600/30807 blocks\n",
            "Completed 21610/30807 blocks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing drugs:  54%|█████▍    | 50/93 [22:22<16:12, 22.63s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Completed 21620/30807 blocks\n",
            "Completed 21630/30807 blocks\n",
            "Completed 21640/30807 blocks\n",
            "Completed 21650/30807 blocks\n",
            "Completed 21660/30807 blocks\n",
            "Completed 21670/30807 blocks\n",
            "Completed 21680/30807 blocks\n",
            "Completed 21690/30807 blocks\n",
            "Completed 21700/30807 blocks\n",
            "Completed 21710/30807 blocks\n",
            "Completed 21720/30807 blocks\n",
            "Completed 21730/30807 blocks\n",
            "Completed 21740/30807 blocks\n",
            "Completed 21750/30807 blocks\n",
            "Completed 21760/30807 blocks\n",
            "Completed 21770/30807 blocks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing drugs:  55%|█████▍    | 51/93 [22:32<13:11, 18.84s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Completed 21780/30807 blocks\n",
            "Completed 21790/30807 blocks\n",
            "Completed 21800/30807 blocks\n",
            "Completed 21810/30807 blocks\n",
            "Completed 21820/30807 blocks\n",
            "Completed 21830/30807 blocks\n",
            "Completed 21840/30807 blocks\n",
            "Completed 21850/30807 blocks\n",
            "Completed 21860/30807 blocks\n",
            "Completed 21870/30807 blocks\n",
            "Completed 21880/30807 blocks\n",
            "Completed 21890/30807 blocks\n",
            "Completed 21900/30807 blocks\n",
            "Completed 21910/30807 blocks\n",
            "Completed 21920/30807 blocks\n",
            "Completed 21930/30807 blocks\n",
            "Completed 21940/30807 blocks\n",
            "Completed 21950/30807 blocks\n",
            "Completed 21960/30807 blocks\n",
            "Completed 21970/30807 blocks\n",
            "Completed 21980/30807 blocks\n",
            "Completed 21990/30807 blocks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing drugs:  56%|█████▌    | 52/93 [22:46<11:50, 17.33s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Completed 22000/30807 blocks\n",
            "Completed 22010/30807 blocks\n",
            "Completed 22020/30807 blocks\n",
            "Completed 22030/30807 blocks\n",
            "Completed 22040/30807 blocks\n",
            "Completed 22050/30807 blocks\n",
            "Completed 22060/30807 blocks\n",
            "Completed 22070/30807 blocks\n",
            "Completed 22080/30807 blocks\n",
            "Completed 22090/30807 blocks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing drugs:  57%|█████▋    | 53/93 [22:52<09:17, 13.95s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Completed 22100/30807 blocks\n",
            "Completed 22110/30807 blocks\n",
            "Completed 22120/30807 blocks\n",
            "Completed 22130/30807 blocks\n",
            "Completed 22140/30807 blocks\n",
            "Completed 22150/30807 blocks\n",
            "Completed 22160/30807 blocks\n",
            "Completed 22170/30807 blocks\n",
            "Completed 22180/30807 blocks\n",
            "Completed 22190/30807 blocks\n",
            "Completed 22200/30807 blocks\n",
            "Completed 22210/30807 blocks\n",
            "Completed 22220/30807 blocks\n",
            "Completed 22230/30807 blocks\n",
            "Completed 22240/30807 blocks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing drugs:  58%|█████▊    | 54/93 [23:01<08:12, 12.63s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Completed 22250/30807 blocks\n",
            "Completed 22260/30807 blocks\n",
            "Completed 22270/30807 blocks\n",
            "Completed 22280/30807 blocks\n",
            "Completed 22290/30807 blocks\n",
            "Completed 22300/30807 blocks\n",
            "Completed 22310/30807 blocks\n",
            "Completed 22320/30807 blocks\n",
            "Completed 22330/30807 blocks\n",
            "Completed 22340/30807 blocks\n",
            "Completed 22350/30807 blocks\n",
            "Completed 22360/30807 blocks\n",
            "Completed 22370/30807 blocks\n",
            "Completed 22380/30807 blocks\n",
            "Completed 22390/30807 blocks\n",
            "Completed 22400/30807 blocks\n",
            "Completed 22410/30807 blocks\n",
            "Completed 22420/30807 blocks\n",
            "Completed 22430/30807 blocks\n",
            "Completed 22440/30807 blocks\n",
            "Completed 22450/30807 blocks\n",
            "Completed 22460/30807 blocks\n",
            "Completed 22470/30807 blocks\n",
            "Completed 22480/30807 blocks\n",
            "Completed 22490/30807 blocks\n",
            "Completed 22500/30807 blocks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing drugs:  59%|█████▉    | 55/93 [23:17<08:40, 13.71s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Completed 22510/30807 blocks\n",
            "Completed 22520/30807 blocks\n",
            "Completed 22530/30807 blocks\n",
            "Completed 22540/30807 blocks\n",
            "Completed 22550/30807 blocks\n",
            "Completed 22560/30807 blocks\n",
            "Completed 22570/30807 blocks\n",
            "Completed 22580/30807 blocks\n",
            "Completed 22590/30807 blocks\n",
            "Completed 22600/30807 blocks\n",
            "Completed 22610/30807 blocks\n",
            "Completed 22620/30807 blocks\n",
            "Completed 22630/30807 blocks\n",
            "Completed 22640/30807 blocks\n",
            "Completed 22650/30807 blocks\n",
            "Completed 22660/30807 blocks\n",
            "Completed 22670/30807 blocks\n",
            "Completed 22680/30807 blocks\n",
            "Completed 22690/30807 blocks\n",
            "Completed 22700/30807 blocks\n",
            "Completed 22710/30807 blocks\n",
            "Completed 22720/30807 blocks\n",
            "Completed 22730/30807 blocks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing drugs:  60%|██████    | 56/93 [23:32<08:34, 13.92s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Completed 22740/30807 blocks\n",
            "Completed 22750/30807 blocks\n",
            "Completed 22760/30807 blocks\n",
            "Completed 22770/30807 blocks\n",
            "Completed 22780/30807 blocks\n",
            "Completed 22790/30807 blocks\n",
            "Completed 22800/30807 blocks\n",
            "Completed 22810/30807 blocks\n",
            "Completed 22820/30807 blocks\n",
            "Completed 22830/30807 blocks\n",
            "Completed 22840/30807 blocks\n",
            "Completed 22850/30807 blocks\n",
            "Completed 22860/30807 blocks\n",
            "Completed 22870/30807 blocks\n",
            "Completed 22880/30807 blocks\n",
            "Completed 22890/30807 blocks\n",
            "Completed 22900/30807 blocks\n",
            "Completed 22910/30807 blocks\n",
            "Completed 22920/30807 blocks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing drugs:  61%|██████▏   | 57/93 [23:44<08:02, 13.41s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Completed 22930/30807 blocks\n",
            "Completed 22940/30807 blocks\n",
            "Completed 22950/30807 blocks\n",
            "Completed 22960/30807 blocks\n",
            "Completed 22970/30807 blocks\n",
            "Completed 22980/30807 blocks\n",
            "Completed 22990/30807 blocks\n",
            "Completed 23000/30807 blocks\n",
            "Completed 23010/30807 blocks\n",
            "Completed 23020/30807 blocks\n",
            "Completed 23030/30807 blocks\n",
            "Completed 23040/30807 blocks\n",
            "Completed 23050/30807 blocks\n",
            "Completed 23060/30807 blocks\n",
            "Completed 23070/30807 blocks\n",
            "Completed 23080/30807 blocks\n",
            "Completed 23090/30807 blocks\n",
            "Completed 23100/30807 blocks\n",
            "Completed 23110/30807 blocks\n",
            "Completed 23120/30807 blocks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing drugs:  62%|██████▏   | 58/93 [23:56<07:32, 12.93s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Completed 23130/30807 blocks\n",
            "Completed 23140/30807 blocks\n",
            "Completed 23150/30807 blocks\n",
            "Completed 23160/30807 blocks\n",
            "Completed 23170/30807 blocks\n",
            "Completed 23180/30807 blocks\n",
            "Completed 23190/30807 blocks\n",
            "Completed 23200/30807 blocks\n",
            "Completed 23210/30807 blocks\n",
            "Completed 23220/30807 blocks\n",
            "Completed 23230/30807 blocks\n",
            "Completed 23240/30807 blocks\n",
            "Completed 23250/30807 blocks\n",
            "Completed 23260/30807 blocks\n",
            "Completed 23270/30807 blocks\n",
            "Completed 23280/30807 blocks\n",
            "Completed 23290/30807 blocks\n",
            "Completed 23300/30807 blocks\n",
            "Completed 23310/30807 blocks\n",
            "Completed 23320/30807 blocks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing drugs:  63%|██████▎   | 59/93 [24:09<07:18, 12.89s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Completed 23330/30807 blocks\n",
            "Completed 23340/30807 blocks\n",
            "Completed 23350/30807 blocks\n",
            "Completed 23360/30807 blocks\n",
            "Completed 23370/30807 blocks\n",
            "Completed 23380/30807 blocks\n",
            "Completed 23390/30807 blocks\n",
            "Completed 23400/30807 blocks\n",
            "Completed 23410/30807 blocks\n",
            "Completed 23420/30807 blocks\n",
            "Completed 23430/30807 blocks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing drugs:  65%|██████▍   | 60/93 [24:15<05:59, 10.90s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Completed 23440/30807 blocks\n",
            "Completed 23450/30807 blocks\n",
            "Completed 23460/30807 blocks\n",
            "Completed 23470/30807 blocks\n",
            "Completed 23480/30807 blocks\n",
            "Completed 23490/30807 blocks\n",
            "Completed 23500/30807 blocks\n",
            "Completed 23510/30807 blocks\n",
            "Completed 23520/30807 blocks\n",
            "Completed 23530/30807 blocks\n",
            "Completed 23540/30807 blocks\n",
            "Completed 23550/30807 blocks\n",
            "Completed 23560/30807 blocks\n",
            "Completed 23570/30807 blocks\n",
            "Completed 23580/30807 blocks\n",
            "Completed 23590/30807 blocks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing drugs:  66%|██████▌   | 61/93 [24:25<05:39, 10.61s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Completed 23600/30807 blocks\n",
            "Completed 23610/30807 blocks\n",
            "Completed 23620/30807 blocks\n",
            "Completed 23630/30807 blocks\n",
            "Completed 23640/30807 blocks\n",
            "Completed 23650/30807 blocks\n",
            "Completed 23660/30807 blocks\n",
            "Completed 23670/30807 blocks\n",
            "Completed 23680/30807 blocks\n",
            "Completed 23690/30807 blocks\n",
            "Completed 23700/30807 blocks\n",
            "Completed 23710/30807 blocks\n",
            "Completed 23720/30807 blocks\n",
            "Completed 23730/30807 blocks\n",
            "Completed 23740/30807 blocks\n",
            "Completed 23750/30807 blocks\n",
            "Completed 23760/30807 blocks\n",
            "Completed 23770/30807 blocks\n",
            "Completed 23780/30807 blocks\n",
            "Completed 23790/30807 blocks\n",
            "Completed 23800/30807 blocks\n",
            "Completed 23810/30807 blocks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing drugs:  67%|██████▋   | 62/93 [24:39<05:59, 11.61s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Completed 23820/30807 blocks\n",
            "Completed 23830/30807 blocks\n",
            "Completed 23840/30807 blocks\n",
            "Completed 23850/30807 blocks\n",
            "Completed 23860/30807 blocks\n",
            "Completed 23870/30807 blocks\n",
            "Completed 23880/30807 blocks\n",
            "Completed 23890/30807 blocks\n",
            "Completed 23900/30807 blocks\n",
            "Completed 23910/30807 blocks\n",
            "Completed 23920/30807 blocks\n",
            "Completed 23930/30807 blocks\n",
            "Completed 23940/30807 blocks\n",
            "Completed 23950/30807 blocks\n",
            "Completed 23960/30807 blocks\n",
            "Completed 23970/30807 blocks\n",
            "Completed 23980/30807 blocks\n",
            "Completed 23990/30807 blocks\n",
            "Completed 24000/30807 blocks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing drugs:  68%|██████▊   | 63/93 [24:51<05:49, 11.66s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Completed 24010/30807 blocks\n",
            "Completed 24020/30807 blocks\n",
            "Completed 24030/30807 blocks\n",
            "Completed 24040/30807 blocks\n",
            "Completed 24050/30807 blocks\n",
            "Completed 24060/30807 blocks\n",
            "Completed 24070/30807 blocks\n",
            "Completed 24080/30807 blocks\n",
            "Completed 24090/30807 blocks\n",
            "Completed 24100/30807 blocks\n",
            "Completed 24110/30807 blocks\n",
            "Completed 24120/30807 blocks\n",
            "Completed 24130/30807 blocks\n",
            "Completed 24140/30807 blocks\n",
            "Completed 24150/30807 blocks\n",
            "Completed 24160/30807 blocks\n",
            "Completed 24170/30807 blocks\n",
            "Completed 24180/30807 blocks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing drugs:  69%|██████▉   | 64/93 [25:02<05:34, 11.55s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Completed 24190/30807 blocks\n",
            "Completed 24200/30807 blocks\n",
            "Completed 24210/30807 blocks\n",
            "Completed 24220/30807 blocks\n",
            "Completed 24230/30807 blocks\n",
            "Completed 24240/30807 blocks\n",
            "Completed 24250/30807 blocks\n",
            "Completed 24260/30807 blocks\n",
            "Completed 24270/30807 blocks\n",
            "Completed 24280/30807 blocks\n",
            "Completed 24290/30807 blocks\n",
            "Completed 24300/30807 blocks\n",
            "Completed 24310/30807 blocks\n",
            "Completed 24320/30807 blocks\n",
            "Completed 24330/30807 blocks\n",
            "Completed 24340/30807 blocks\n",
            "Completed 24350/30807 blocks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing drugs:  70%|██████▉   | 65/93 [25:12<05:11, 11.11s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Completed 24360/30807 blocks\n",
            "Completed 24370/30807 blocks\n",
            "Completed 24380/30807 blocks\n",
            "Completed 24390/30807 blocks\n",
            "Completed 24400/30807 blocks\n",
            "Completed 24410/30807 blocks\n",
            "Completed 24420/30807 blocks\n",
            "Completed 24430/30807 blocks\n",
            "Completed 24440/30807 blocks\n",
            "Completed 24450/30807 blocks\n",
            "Completed 24460/30807 blocks\n",
            "Completed 24470/30807 blocks\n",
            "Completed 24480/30807 blocks\n",
            "Completed 24490/30807 blocks\n",
            "Completed 24500/30807 blocks\n",
            "Completed 24510/30807 blocks\n",
            "Completed 24520/30807 blocks\n",
            "Completed 24530/30807 blocks\n",
            "Completed 24540/30807 blocks\n",
            "Completed 24550/30807 blocks\n",
            "Completed 24560/30807 blocks\n",
            "Completed 24570/30807 blocks\n",
            "Completed 24580/30807 blocks\n",
            "Completed 24590/30807 blocks\n",
            "Completed 24600/30807 blocks\n",
            "Completed 24610/30807 blocks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing drugs:  71%|███████   | 66/93 [25:28<05:41, 12.66s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Completed 24620/30807 blocks\n",
            "Completed 24630/30807 blocks\n",
            "Completed 24640/30807 blocks\n",
            "Completed 24650/30807 blocks\n",
            "Completed 24660/30807 blocks\n",
            "Completed 24670/30807 blocks\n",
            "Completed 24680/30807 blocks\n",
            "Completed 24690/30807 blocks\n",
            "Completed 24700/30807 blocks\n",
            "Completed 24710/30807 blocks\n",
            "Completed 24720/30807 blocks\n",
            "Completed 24730/30807 blocks\n",
            "Completed 24740/30807 blocks\n",
            "Completed 24750/30807 blocks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing drugs:  72%|███████▏  | 67/93 [25:37<04:58, 11.48s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Completed 24760/30807 blocks\n",
            "Completed 24770/30807 blocks\n",
            "Completed 24780/30807 blocks\n",
            "Completed 24790/30807 blocks\n",
            "Completed 24800/30807 blocks\n",
            "Completed 24810/30807 blocks\n",
            "Completed 24820/30807 blocks\n",
            "Completed 24830/30807 blocks\n",
            "Completed 24840/30807 blocks\n",
            "Completed 24850/30807 blocks\n",
            "Completed 24860/30807 blocks\n",
            "Completed 24870/30807 blocks\n",
            "Completed 24880/30807 blocks\n",
            "Completed 24890/30807 blocks\n",
            "Completed 24900/30807 blocks\n",
            "Completed 24910/30807 blocks\n",
            "Completed 24920/30807 blocks\n",
            "Completed 24930/30807 blocks\n",
            "Completed 24940/30807 blocks\n",
            "Completed 24950/30807 blocks\n",
            "Completed 24960/30807 blocks\n",
            "Completed 24970/30807 blocks\n",
            "Completed 24980/30807 blocks\n",
            "Completed 24990/30807 blocks\n",
            "Completed 25000/30807 blocks\n",
            "Completed 25010/30807 blocks\n",
            "Completed 25020/30807 blocks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing drugs:  73%|███████▎  | 68/93 [25:54<05:28, 13.12s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Completed 25030/30807 blocks\n",
            "Completed 25040/30807 blocks\n",
            "Completed 25050/30807 blocks\n",
            "Completed 25060/30807 blocks\n",
            "Completed 25070/30807 blocks\n",
            "Completed 25080/30807 blocks\n",
            "Completed 25090/30807 blocks\n",
            "Completed 25100/30807 blocks\n",
            "Completed 25110/30807 blocks\n",
            "Completed 25120/30807 blocks\n",
            "Completed 25130/30807 blocks\n",
            "Completed 25140/30807 blocks\n",
            "Completed 25150/30807 blocks\n",
            "Completed 25160/30807 blocks\n",
            "Completed 25170/30807 blocks\n",
            "Completed 25180/30807 blocks\n",
            "Completed 25190/30807 blocks\n",
            "Completed 25200/30807 blocks\n",
            "Completed 25210/30807 blocks\n",
            "Completed 25220/30807 blocks\n",
            "Completed 25230/30807 blocks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing drugs:  74%|███████▍  | 69/93 [26:07<05:14, 13.10s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Completed 25240/30807 blocks\n",
            "Completed 25250/30807 blocks\n",
            "Completed 25260/30807 blocks\n",
            "Completed 25270/30807 blocks\n",
            "Completed 25280/30807 blocks\n",
            "Completed 25290/30807 blocks\n",
            "Completed 25300/30807 blocks\n",
            "Completed 25310/30807 blocks\n",
            "Completed 25320/30807 blocks\n",
            "Completed 25330/30807 blocks\n",
            "Completed 25340/30807 blocks\n",
            "Completed 25350/30807 blocks\n",
            "Completed 25360/30807 blocks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing drugs:  75%|███████▌  | 70/93 [26:15<04:26, 11.57s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Completed 25370/30807 blocks\n",
            "Completed 25380/30807 blocks\n",
            "Completed 25390/30807 blocks\n",
            "Completed 25400/30807 blocks\n",
            "Completed 25410/30807 blocks\n",
            "Completed 25420/30807 blocks\n",
            "Completed 25430/30807 blocks\n",
            "Completed 25440/30807 blocks\n",
            "Completed 25450/30807 blocks\n",
            "Completed 25460/30807 blocks\n",
            "Completed 25470/30807 blocks\n",
            "Completed 25480/30807 blocks\n",
            "Completed 25490/30807 blocks\n",
            "Completed 25500/30807 blocks\n",
            "Completed 25510/30807 blocks\n",
            "Completed 25520/30807 blocks\n",
            "Completed 25530/30807 blocks\n",
            "Completed 25540/30807 blocks\n",
            "Completed 25550/30807 blocks\n",
            "Completed 25560/30807 blocks\n",
            "Completed 25570/30807 blocks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing drugs:  76%|███████▋  | 71/93 [26:28<04:23, 11.99s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Completed 25580/30807 blocks\n",
            "Completed 25590/30807 blocks\n",
            "Completed 25600/30807 blocks\n",
            "Completed 25610/30807 blocks\n",
            "Completed 25620/30807 blocks\n",
            "Completed 25630/30807 blocks\n",
            "Completed 25640/30807 blocks\n",
            "Completed 25650/30807 blocks\n",
            "Completed 25660/30807 blocks\n",
            "Completed 25670/30807 blocks\n",
            "Completed 25680/30807 blocks\n",
            "Completed 25690/30807 blocks\n",
            "Completed 25700/30807 blocks\n",
            "Completed 25710/30807 blocks\n",
            "Completed 25720/30807 blocks\n",
            "Completed 25730/30807 blocks\n",
            "Completed 25740/30807 blocks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing drugs:  77%|███████▋  | 72/93 [26:39<04:04, 11.63s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Completed 25750/30807 blocks\n",
            "Completed 25760/30807 blocks\n",
            "Completed 25770/30807 blocks\n",
            "Completed 25780/30807 blocks\n",
            "Completed 25790/30807 blocks\n",
            "Completed 25800/30807 blocks\n",
            "Completed 25810/30807 blocks\n",
            "Completed 25820/30807 blocks\n",
            "Completed 25830/30807 blocks\n",
            "Completed 25840/30807 blocks\n",
            "Completed 25850/30807 blocks\n",
            "Completed 25860/30807 blocks\n",
            "Completed 25870/30807 blocks\n",
            "Completed 25880/30807 blocks\n",
            "Completed 25890/30807 blocks\n",
            "Completed 25900/30807 blocks\n",
            "Completed 25910/30807 blocks\n",
            "Completed 25920/30807 blocks\n",
            "Completed 25930/30807 blocks\n",
            "Completed 25940/30807 blocks\n",
            "Completed 25950/30807 blocks\n",
            "Completed 25960/30807 blocks\n",
            "Completed 25970/30807 blocks\n",
            "Completed 25980/30807 blocks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing drugs:  78%|███████▊  | 73/93 [26:54<04:11, 12.58s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Completed 25990/30807 blocks\n",
            "Completed 26000/30807 blocks\n",
            "Completed 26010/30807 blocks\n",
            "Completed 26020/30807 blocks\n",
            "Completed 26030/30807 blocks\n",
            "Completed 26040/30807 blocks\n",
            "Completed 26050/30807 blocks\n",
            "Completed 26060/30807 blocks\n",
            "Completed 26070/30807 blocks\n",
            "Completed 26080/30807 blocks\n",
            "Completed 26090/30807 blocks\n",
            "Completed 26100/30807 blocks\n",
            "Completed 26110/30807 blocks\n",
            "Completed 26120/30807 blocks\n",
            "Completed 26130/30807 blocks\n",
            "Completed 26140/30807 blocks\n",
            "Completed 26150/30807 blocks\n",
            "Completed 26160/30807 blocks\n",
            "Completed 26170/30807 blocks\n",
            "Completed 26180/30807 blocks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing drugs:  80%|███████▉  | 74/93 [27:06<03:55, 12.40s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Completed 26190/30807 blocks\n",
            "Completed 26200/30807 blocks\n",
            "Completed 26210/30807 blocks\n",
            "Completed 26220/30807 blocks\n",
            "Completed 26230/30807 blocks\n",
            "Completed 26240/30807 blocks\n",
            "Completed 26250/30807 blocks\n",
            "Completed 26260/30807 blocks\n",
            "Completed 26270/30807 blocks\n",
            "Completed 26280/30807 blocks\n",
            "Completed 26290/30807 blocks\n",
            "Completed 26300/30807 blocks\n",
            "Completed 26310/30807 blocks\n",
            "Completed 26320/30807 blocks\n",
            "Completed 26330/30807 blocks\n",
            "Completed 26340/30807 blocks\n",
            "Completed 26350/30807 blocks\n",
            "Completed 26360/30807 blocks\n",
            "Completed 26370/30807 blocks\n",
            "Completed 26380/30807 blocks\n",
            "Completed 26390/30807 blocks\n",
            "Completed 26400/30807 blocks\n",
            "Completed 26410/30807 blocks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing drugs:  81%|████████  | 75/93 [27:20<03:55, 13.10s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Completed 26420/30807 blocks\n",
            "Completed 26430/30807 blocks\n",
            "Completed 26440/30807 blocks\n",
            "Completed 26450/30807 blocks\n",
            "Completed 26460/30807 blocks\n",
            "Completed 26470/30807 blocks\n",
            "Completed 26480/30807 blocks\n",
            "Completed 26490/30807 blocks\n",
            "Completed 26500/30807 blocks\n",
            "Completed 26510/30807 blocks\n",
            "Completed 26520/30807 blocks\n",
            "Completed 26530/30807 blocks\n",
            "Completed 26540/30807 blocks\n",
            "Completed 26550/30807 blocks\n",
            "Completed 26560/30807 blocks\n",
            "Completed 26570/30807 blocks\n",
            "Completed 26580/30807 blocks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing drugs:  82%|████████▏ | 76/93 [27:31<03:28, 12.29s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Completed 26590/30807 blocks\n",
            "Completed 26600/30807 blocks\n",
            "Completed 26610/30807 blocks\n",
            "Completed 26620/30807 blocks\n",
            "Completed 26630/30807 blocks\n",
            "Completed 26640/30807 blocks\n",
            "Completed 26650/30807 blocks\n",
            "Completed 26660/30807 blocks\n",
            "Completed 26670/30807 blocks\n",
            "Completed 26680/30807 blocks\n",
            "Completed 26690/30807 blocks\n",
            "Completed 26700/30807 blocks\n",
            "Completed 26710/30807 blocks\n",
            "Completed 26720/30807 blocks\n",
            "Completed 26730/30807 blocks\n",
            "Completed 26740/30807 blocks\n",
            "Completed 26750/30807 blocks\n",
            "Completed 26760/30807 blocks\n",
            "Completed 26770/30807 blocks\n",
            "Completed 26780/30807 blocks\n",
            "Completed 26790/30807 blocks\n",
            "Completed 26800/30807 blocks\n",
            "Completed 26810/30807 blocks\n",
            "Completed 26820/30807 blocks\n",
            "Completed 26830/30807 blocks\n",
            "Completed 26840/30807 blocks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing drugs:  83%|████████▎ | 77/93 [27:47<03:35, 13.49s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Completed 26850/30807 blocks\n",
            "Completed 26860/30807 blocks\n",
            "Completed 26870/30807 blocks\n",
            "Completed 26880/30807 blocks\n",
            "Completed 26890/30807 blocks\n",
            "Completed 26900/30807 blocks\n",
            "Completed 26910/30807 blocks\n",
            "Completed 26920/30807 blocks\n",
            "Completed 26930/30807 blocks\n",
            "Completed 26940/30807 blocks\n",
            "Completed 26950/30807 blocks\n",
            "Completed 26960/30807 blocks\n",
            "Completed 26970/30807 blocks\n",
            "Completed 26980/30807 blocks\n",
            "Completed 26990/30807 blocks\n",
            "Completed 27000/30807 blocks\n",
            "Completed 27010/30807 blocks\n",
            "Completed 27020/30807 blocks\n",
            "Completed 27030/30807 blocks\n",
            "Completed 27040/30807 blocks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing drugs:  84%|████████▍ | 78/93 [27:59<03:16, 13.10s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Completed 27050/30807 blocks\n",
            "Completed 27060/30807 blocks\n",
            "Completed 27070/30807 blocks\n",
            "Completed 27080/30807 blocks\n",
            "Completed 27090/30807 blocks\n",
            "Completed 27100/30807 blocks\n",
            "Completed 27110/30807 blocks\n",
            "Completed 27120/30807 blocks\n",
            "Completed 27130/30807 blocks\n",
            "Completed 27140/30807 blocks\n",
            "Completed 27150/30807 blocks\n",
            "Completed 27160/30807 blocks\n",
            "Completed 27170/30807 blocks\n",
            "Completed 27180/30807 blocks\n",
            "Completed 27190/30807 blocks\n",
            "Completed 27200/30807 blocks\n",
            "Completed 27210/30807 blocks\n",
            "Completed 27220/30807 blocks\n",
            "Completed 27230/30807 blocks\n",
            "Completed 27240/30807 blocks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing drugs:  85%|████████▍ | 79/93 [28:11<03:00, 12.86s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Completed 27250/30807 blocks\n",
            "Completed 27260/30807 blocks\n",
            "Completed 27270/30807 blocks\n",
            "Completed 27280/30807 blocks\n",
            "Completed 27290/30807 blocks\n",
            "Completed 27300/30807 blocks\n",
            "Completed 27310/30807 blocks\n",
            "Completed 27320/30807 blocks\n",
            "Completed 27330/30807 blocks\n",
            "Completed 27340/30807 blocks\n",
            "Completed 27350/30807 blocks\n",
            "Completed 27360/30807 blocks\n",
            "Completed 27370/30807 blocks\n",
            "Completed 27380/30807 blocks\n",
            "Completed 27390/30807 blocks\n",
            "Completed 27400/30807 blocks\n",
            "Completed 27410/30807 blocks\n",
            "Completed 27420/30807 blocks\n",
            "Completed 27430/30807 blocks\n",
            "Completed 27440/30807 blocks\n",
            "Completed 27450/30807 blocks\n",
            "Completed 27460/30807 blocks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing drugs:  86%|████████▌ | 80/93 [28:25<02:50, 13.14s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Completed 27470/30807 blocks\n",
            "Completed 27480/30807 blocks\n",
            "Completed 27490/30807 blocks\n",
            "Completed 27500/30807 blocks\n",
            "Completed 27510/30807 blocks\n",
            "Completed 27520/30807 blocks\n",
            "Completed 27530/30807 blocks\n",
            "Completed 27540/30807 blocks\n",
            "Completed 27550/30807 blocks\n",
            "Completed 27560/30807 blocks\n",
            "Completed 27570/30807 blocks\n",
            "Completed 27580/30807 blocks\n",
            "Completed 27590/30807 blocks\n",
            "Completed 27600/30807 blocks\n",
            "Completed 27610/30807 blocks\n",
            "Completed 27620/30807 blocks\n",
            "Completed 27630/30807 blocks\n",
            "Completed 27640/30807 blocks\n",
            "Completed 27650/30807 blocks\n",
            "Completed 27660/30807 blocks\n",
            "Completed 27670/30807 blocks\n",
            "Completed 27680/30807 blocks\n",
            "Completed 27690/30807 blocks\n",
            "Completed 27700/30807 blocks\n",
            "Completed 27710/30807 blocks\n",
            "Completed 27720/30807 blocks\n",
            "Completed 27730/30807 blocks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing drugs:  87%|████████▋ | 81/93 [28:42<02:50, 14.22s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Completed 27740/30807 blocks\n",
            "Completed 27750/30807 blocks\n",
            "Completed 27760/30807 blocks\n",
            "Completed 27770/30807 blocks\n",
            "Completed 27780/30807 blocks\n",
            "Completed 27790/30807 blocks\n",
            "Completed 27800/30807 blocks\n",
            "Completed 27810/30807 blocks\n",
            "Completed 27820/30807 blocks\n",
            "Completed 27830/30807 blocks\n",
            "Completed 27840/30807 blocks\n",
            "Completed 27850/30807 blocks\n",
            "Completed 27860/30807 blocks\n",
            "Completed 27870/30807 blocks\n",
            "Completed 27880/30807 blocks\n",
            "Completed 27890/30807 blocks\n",
            "Completed 27900/30807 blocks\n",
            "Completed 27910/30807 blocks\n",
            "Completed 27920/30807 blocks\n",
            "Completed 27930/30807 blocks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing drugs:  88%|████████▊ | 82/93 [28:54<02:30, 13.66s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Completed 27940/30807 blocks\n",
            "Completed 27950/30807 blocks\n",
            "Completed 27960/30807 blocks\n",
            "Completed 27970/30807 blocks\n",
            "Completed 27980/30807 blocks\n",
            "Completed 27990/30807 blocks\n",
            "Completed 28000/30807 blocks\n",
            "Completed 28010/30807 blocks\n",
            "Completed 28020/30807 blocks\n",
            "Completed 28030/30807 blocks\n",
            "Completed 28040/30807 blocks\n",
            "Completed 28050/30807 blocks\n",
            "Completed 28060/30807 blocks\n",
            "Completed 28070/30807 blocks\n",
            "Completed 28080/30807 blocks\n",
            "Completed 28090/30807 blocks\n",
            "Completed 28100/30807 blocks\n",
            "Completed 28110/30807 blocks\n",
            "Completed 28120/30807 blocks\n",
            "Completed 28130/30807 blocks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing drugs:  89%|████████▉ | 83/93 [29:06<02:11, 13.13s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Completed 28140/30807 blocks\n",
            "Completed 28150/30807 blocks\n",
            "Completed 28160/30807 blocks\n",
            "Completed 28170/30807 blocks\n",
            "Completed 28180/30807 blocks\n",
            "Completed 28190/30807 blocks\n",
            "Completed 28200/30807 blocks\n",
            "Completed 28210/30807 blocks\n",
            "Completed 28220/30807 blocks\n",
            "Completed 28230/30807 blocks\n",
            "Completed 28240/30807 blocks\n",
            "Completed 28250/30807 blocks\n",
            "Completed 28260/30807 blocks\n",
            "Completed 28270/30807 blocks\n",
            "Completed 28280/30807 blocks\n",
            "Completed 28290/30807 blocks\n",
            "Completed 28300/30807 blocks\n",
            "Completed 28310/30807 blocks\n",
            "Completed 28320/30807 blocks\n",
            "Completed 28330/30807 blocks\n",
            "Completed 28340/30807 blocks\n",
            "Completed 28350/30807 blocks\n",
            "Completed 28360/30807 blocks\n",
            "Completed 28370/30807 blocks\n",
            "Completed 28380/30807 blocks\n",
            "Completed 28390/30807 blocks\n",
            "Completed 28400/30807 blocks\n",
            "Completed 28410/30807 blocks\n",
            "Completed 28420/30807 blocks\n",
            "Completed 28430/30807 blocks\n",
            "Completed 28440/30807 blocks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing drugs:  90%|█████████ | 84/93 [29:26<02:14, 14.99s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Completed 28450/30807 blocks\n",
            "Completed 28460/30807 blocks\n",
            "Completed 28470/30807 blocks\n",
            "Completed 28480/30807 blocks\n",
            "Completed 28490/30807 blocks\n",
            "Completed 28500/30807 blocks\n",
            "Completed 28510/30807 blocks\n",
            "Completed 28520/30807 blocks\n",
            "Completed 28530/30807 blocks\n",
            "Completed 28540/30807 blocks\n",
            "Completed 28550/30807 blocks\n",
            "Completed 28560/30807 blocks\n",
            "Completed 28570/30807 blocks\n",
            "Completed 28580/30807 blocks\n",
            "Completed 28590/30807 blocks\n",
            "Completed 28600/30807 blocks\n",
            "Completed 28610/30807 blocks\n",
            "Completed 28620/30807 blocks\n",
            "Completed 28630/30807 blocks\n",
            "Completed 28640/30807 blocks\n",
            "Completed 28650/30807 blocks\n",
            "Completed 28660/30807 blocks\n",
            "Completed 28670/30807 blocks\n",
            "Completed 28680/30807 blocks\n",
            "Completed 28690/30807 blocks\n",
            "Completed 28700/30807 blocks\n",
            "Completed 28710/30807 blocks\n",
            "Completed 28720/30807 blocks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing drugs:  91%|█████████▏| 85/93 [29:43<02:05, 15.71s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Completed 28730/30807 blocks\n",
            "Completed 28740/30807 blocks\n",
            "Completed 28750/30807 blocks\n",
            "Completed 28760/30807 blocks\n",
            "Completed 28770/30807 blocks\n",
            "Completed 28780/30807 blocks\n",
            "Completed 28790/30807 blocks\n",
            "Completed 28800/30807 blocks\n",
            "Completed 28810/30807 blocks\n",
            "Completed 28820/30807 blocks\n",
            "Completed 28830/30807 blocks\n",
            "Completed 28840/30807 blocks\n",
            "Completed 28850/30807 blocks\n",
            "Completed 28860/30807 blocks\n",
            "Completed 28870/30807 blocks\n",
            "Completed 28880/30807 blocks\n",
            "Completed 28890/30807 blocks\n",
            "Completed 28900/30807 blocks\n",
            "Completed 28910/30807 blocks\n",
            "Completed 28920/30807 blocks\n",
            "Completed 28930/30807 blocks\n",
            "Completed 28940/30807 blocks\n",
            "Completed 28950/30807 blocks\n",
            "Completed 28960/30807 blocks\n",
            "Completed 28970/30807 blocks\n",
            "Completed 28980/30807 blocks\n",
            "Completed 28990/30807 blocks\n",
            "Completed 29000/30807 blocks\n",
            "Completed 29010/30807 blocks\n",
            "Completed 29020/30807 blocks\n",
            "Completed 29030/30807 blocks\n",
            "Completed 29040/30807 blocks\n",
            "Completed 29050/30807 blocks\n",
            "Completed 29060/30807 blocks\n",
            "Completed 29070/30807 blocks\n",
            "Completed 29080/30807 blocks\n",
            "Completed 29090/30807 blocks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing drugs:  92%|█████████▏| 86/93 [30:05<02:04, 17.73s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Completed 29100/30807 blocks\n",
            "Completed 29110/30807 blocks\n",
            "Completed 29120/30807 blocks\n",
            "Completed 29130/30807 blocks\n",
            "Completed 29140/30807 blocks\n",
            "Completed 29150/30807 blocks\n",
            "Completed 29160/30807 blocks\n",
            "Completed 29170/30807 blocks\n",
            "Completed 29180/30807 blocks\n",
            "Completed 29190/30807 blocks\n",
            "Completed 29200/30807 blocks\n",
            "Completed 29210/30807 blocks\n",
            "Completed 29220/30807 blocks\n",
            "Completed 29230/30807 blocks\n",
            "Completed 29240/30807 blocks\n",
            "Completed 29250/30807 blocks\n",
            "Completed 29260/30807 blocks\n",
            "Completed 29270/30807 blocks\n",
            "Completed 29280/30807 blocks\n",
            "Completed 29290/30807 blocks\n",
            "Completed 29300/30807 blocks\n",
            "Completed 29310/30807 blocks\n",
            "Completed 29320/30807 blocks\n",
            "Completed 29330/30807 blocks\n",
            "Completed 29340/30807 blocks\n",
            "Completed 29350/30807 blocks\n",
            "Completed 29360/30807 blocks\n",
            "Completed 29370/30807 blocks\n",
            "Completed 29380/30807 blocks\n",
            "Completed 29390/30807 blocks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing drugs:  94%|█████████▎| 87/93 [30:24<01:48, 18.05s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Completed 29400/30807 blocks\n",
            "Completed 29410/30807 blocks\n",
            "Completed 29420/30807 blocks\n",
            "Completed 29430/30807 blocks\n",
            "Completed 29440/30807 blocks\n",
            "Completed 29450/30807 blocks\n",
            "Completed 29460/30807 blocks\n",
            "Completed 29470/30807 blocks\n",
            "Completed 29480/30807 blocks\n",
            "Completed 29490/30807 blocks\n",
            "Completed 29500/30807 blocks\n",
            "Completed 29510/30807 blocks\n",
            "Completed 29520/30807 blocks\n",
            "Completed 29530/30807 blocks\n",
            "Completed 29540/30807 blocks\n",
            "Completed 29550/30807 blocks\n",
            "Completed 29560/30807 blocks\n",
            "Completed 29570/30807 blocks\n",
            "Completed 29580/30807 blocks\n",
            "Completed 29590/30807 blocks\n",
            "Completed 29600/30807 blocks\n",
            "Completed 29610/30807 blocks\n",
            "Completed 29620/30807 blocks\n",
            "Completed 29630/30807 blocks\n",
            "Completed 29640/30807 blocks\n",
            "Completed 29650/30807 blocks\n",
            "Completed 29660/30807 blocks\n",
            "Completed 29670/30807 blocks\n",
            "Completed 29680/30807 blocks\n",
            "Completed 29690/30807 blocks\n",
            "Completed 29700/30807 blocks\n",
            "Completed 29710/30807 blocks\n",
            "Completed 29720/30807 blocks\n",
            "Completed 29730/30807 blocks\n",
            "Completed 29740/30807 blocks\n",
            "Completed 29750/30807 blocks\n",
            "Completed 29760/30807 blocks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing drugs:  95%|█████████▍| 88/93 [30:47<01:37, 19.54s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Completed 29770/30807 blocks\n",
            "Completed 29780/30807 blocks\n",
            "Completed 29790/30807 blocks\n",
            "Completed 29800/30807 blocks\n",
            "Completed 29810/30807 blocks\n",
            "Completed 29820/30807 blocks\n",
            "Completed 29830/30807 blocks\n",
            "Completed 29840/30807 blocks\n",
            "Completed 29850/30807 blocks\n",
            "Completed 29860/30807 blocks\n",
            "Completed 29870/30807 blocks\n",
            "Completed 29880/30807 blocks\n",
            "Completed 29890/30807 blocks\n",
            "Completed 29900/30807 blocks\n",
            "Completed 29910/30807 blocks\n",
            "Completed 29920/30807 blocks\n",
            "Completed 29930/30807 blocks\n",
            "Completed 29940/30807 blocks\n",
            "Completed 29950/30807 blocks\n",
            "Completed 29960/30807 blocks\n",
            "Completed 29970/30807 blocks\n",
            "Completed 29980/30807 blocks\n",
            "Completed 29990/30807 blocks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing drugs:  96%|█████████▌| 89/93 [31:02<01:12, 18.11s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Completed 30000/30807 blocks\n",
            "Completed 30010/30807 blocks\n",
            "Completed 30020/30807 blocks\n",
            "Completed 30030/30807 blocks\n",
            "Completed 30040/30807 blocks\n",
            "Completed 30050/30807 blocks\n",
            "Completed 30060/30807 blocks\n",
            "Completed 30070/30807 blocks\n",
            "Completed 30080/30807 blocks\n",
            "Completed 30090/30807 blocks\n",
            "Completed 30100/30807 blocks\n",
            "Completed 30110/30807 blocks\n",
            "Completed 30120/30807 blocks\n",
            "Completed 30130/30807 blocks\n",
            "Completed 30140/30807 blocks\n",
            "Completed 30150/30807 blocks\n",
            "Completed 30160/30807 blocks\n",
            "Completed 30170/30807 blocks\n",
            "Completed 30180/30807 blocks\n",
            "Completed 30190/30807 blocks\n",
            "Completed 30200/30807 blocks\n",
            "Completed 30210/30807 blocks\n",
            "Completed 30220/30807 blocks\n",
            "Completed 30230/30807 blocks\n",
            "Completed 30240/30807 blocks\n",
            "Completed 30250/30807 blocks\n",
            "Completed 30260/30807 blocks\n",
            "Completed 30270/30807 blocks\n",
            "Completed 30280/30807 blocks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing drugs:  97%|█████████▋| 90/93 [31:20<00:54, 18.15s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Completed 30290/30807 blocks\n",
            "Completed 30300/30807 blocks\n",
            "Completed 30310/30807 blocks\n",
            "Completed 30320/30807 blocks\n",
            "Completed 30330/30807 blocks\n",
            "Completed 30340/30807 blocks\n",
            "Completed 30350/30807 blocks\n",
            "Completed 30360/30807 blocks\n",
            "Completed 30370/30807 blocks\n",
            "Completed 30380/30807 blocks\n",
            "Completed 30390/30807 blocks\n",
            "Completed 30400/30807 blocks\n",
            "Completed 30410/30807 blocks\n",
            "Completed 30420/30807 blocks\n",
            "Completed 30430/30807 blocks\n",
            "Completed 30440/30807 blocks\n",
            "Completed 30450/30807 blocks\n",
            "Completed 30460/30807 blocks\n",
            "Completed 30470/30807 blocks\n",
            "Completed 30480/30807 blocks\n",
            "Completed 30490/30807 blocks\n",
            "Completed 30500/30807 blocks\n",
            "Completed 30510/30807 blocks\n",
            "Completed 30520/30807 blocks\n",
            "Completed 30530/30807 blocks\n",
            "Completed 30540/30807 blocks\n",
            "Completed 30550/30807 blocks\n",
            "Completed 30560/30807 blocks\n",
            "Completed 30570/30807 blocks\n",
            "Completed 30580/30807 blocks\n",
            "Completed 30590/30807 blocks\n",
            "Completed 30600/30807 blocks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing drugs:  98%|█████████▊| 91/93 [31:40<00:37, 18.62s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Completed 30610/30807 blocks\n",
            "Completed 30620/30807 blocks\n",
            "Completed 30630/30807 blocks\n",
            "Completed 30640/30807 blocks\n",
            "Completed 30650/30807 blocks\n",
            "Completed 30660/30807 blocks\n",
            "Completed 30670/30807 blocks\n",
            "Completed 30680/30807 blocks\n",
            "Completed 30690/30807 blocks\n",
            "Completed 30700/30807 blocks\n",
            "Completed 30710/30807 blocks\n",
            "Completed 30720/30807 blocks\n",
            "Completed 30730/30807 blocks\n",
            "Completed 30740/30807 blocks\n",
            "Completed 30750/30807 blocks\n",
            "Completed 30760/30807 blocks\n",
            "Completed 30770/30807 blocks\n",
            "Completed 30780/30807 blocks\n",
            "Completed 30790/30807 blocks\n",
            "Completed 30800/30807 blocks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing drugs: 100%|██████████| 93/93 [31:52<00:00, 20.57s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing complete. Created 30807 difference files.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import scanpy as sc\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "unique_drugs = adata.obs['drug'].unique()\n",
        "print(f\"Number of unique drugs: {len(unique_drugs)}\")\n",
        "print(unique_drugs)\n",
        "\n",
        "block_size = 15  # Each block will contain 15 samples\n",
        "cells_per_drug = {}\n",
        "for drug in unique_drugs:\n",
        "    cells_per_drug[drug] = sum(adata.obs['drug'] == drug)\n",
        "    print(f\"Drug {drug}: {cells_per_drug[drug]} cells\")\n",
        "\n",
        "print(\"Calculating control averages...\")\n",
        "control_data = adata[adata.obs['drug'] == 'DMSO_TF']\n",
        "# Calculate average for adata.X\n",
        "control_avg_X = control_data.X.mean(axis=0)\n",
        "# Calculate average for adata.obsm['X_scGPT']\n",
        "control_avg_scGPT = control_data.obsm['X_scGPT'].mean(axis=0)\n",
        "\n",
        "output_dir = \"/content/drive/MyDrive/Colab Notebooks/esm cell state/difference counts\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Calculate how many blocks we'll create\n",
        "total_blocks = sum([cells_per_drug[drug] // block_size + (1 if cells_per_drug[drug] % block_size > 0 else 0)\n",
        "                   for drug in unique_drugs if drug != 'DMSO_TF'])\n",
        "print(f\"Total blocks to create: approximately {total_blocks}\")\n",
        "\n",
        "block_counter = 0\n",
        "for drug in tqdm(unique_drugs, desc=\"Processing drugs\"):\n",
        "    if drug == 'DMSO_TF':\n",
        "        continue\n",
        "\n",
        "    drug_data = adata[adata.obs['drug'] == drug]\n",
        "    n_cells = drug_data.shape[0]\n",
        "\n",
        "    # Create blocks of 15 samples each\n",
        "    n_blocks = n_cells // block_size + (1 if n_cells % block_size > 0 else 0)\n",
        "\n",
        "    for block_idx in range(n_blocks):\n",
        "        start_idx = block_idx * block_size\n",
        "        end_idx = min(start_idx + block_size, n_cells)\n",
        "\n",
        "        # Get indices for this block\n",
        "        block_indices = list(range(start_idx, end_idx))\n",
        "\n",
        "        block_data = drug_data[block_indices].copy()\n",
        "\n",
        "        # Calculate difference for adata.X\n",
        "        block_avg_X = block_data.X.mean(axis=0)\n",
        "        diff_values_X = block_avg_X - control_avg_X\n",
        "        block_data.X = diff_values_X.reshape(1, -1).repeat(len(block_indices), axis=0)\n",
        "\n",
        "        # Calculate difference for adata.obsm['X_scGPT']\n",
        "        if hasattr(block_data, 'obsm') and 'X_scGPT' in block_data.obsm:\n",
        "            block_avg_scGPT = block_data.obsm['X_scGPT'].mean(axis=0)\n",
        "            diff_values_scGPT = block_avg_scGPT - control_avg_scGPT\n",
        "\n",
        "            # Replace original values with differences\n",
        "            block_data.obsm['X_scGPT'] = diff_values_scGPT.reshape(1, -1).repeat(len(block_indices), axis=0)\n",
        "\n",
        "        block_data.write_h5ad(f\"{output_dir}/difference_{drug}_block{block_idx+1}.h5ad\")\n",
        "\n",
        "        block_counter += 1\n",
        "        if block_counter % 10 == 0:\n",
        "            print(f\"Completed {block_counter}/{total_blocks} blocks\")\n",
        "\n",
        "print(f\"Processing complete. Created {block_counter} difference files.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LvlGu63POFpq",
        "outputId": "65fbcac5-67d0-40f7-a4c0-a16e198760e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total number of files in output directory: 30807\n",
            "\n",
            "Files per drug:\n",
            "4EGI-1: 210 files\n",
            "9-ING-41: 675 files\n",
            "APTO-253: 311 files\n",
            "AT7519: 363 files\n",
            "AZD-7648: 198 files\n",
            "AZD-8055: 409 files\n",
            "AZD1390: 275 files\n",
            "AZD2858: 126 files\n",
            "Abemaciclib: 308 files\n",
            "Adagrasib: 201 files\n",
            "Afatinib: 540 files\n",
            "Alpelisib: 244 files\n",
            "Anastrozole: 320 files\n",
            "BAY1125976: 645 files\n",
            "BI-3406: 461 files\n",
            "BI-78D3: 496 files\n",
            "Belumosudil (mesylate): 220 files\n",
            "Belzutifan: 188 files\n",
            "Bentamapimod: 399 files\n",
            "Bimiralisib: 661 files\n",
            "Binimetinib: 194 files\n",
            "Bortezomib: 105 files\n",
            "Brivudine: 551 files\n",
            "CP21R7: 206 files\n",
            "Capivasertib: 209 files\n",
            "Capmatinib: 200 files\n",
            "Celecoxib: 270 files\n",
            "DT-061: 198 files\n",
            "DTP3: 233 files\n",
            "Dabrafenib: 828 files\n",
            "ERK5-IN-2: 629 files\n",
            "ETC-206: 303 files\n",
            "EX229: 253 files\n",
            "Elimusertib hydrochloride: 138 files\n",
            "Encorafenib: 782 files\n",
            "Erdafitinib : 342 files\n",
            "Everolimus: 781 files\n",
            "Futibatinib: 184 files\n",
            "GSK1059615: 552 files\n",
            "Gemcitabine: 497 files\n",
            "HI-TOPK-032: 197 files\n",
            "Hydroxyfasudil: 163 files\n",
            "IQ 1: 194 files\n",
            "Infigratinib: 345 files\n",
            "Ipatasertib: 638 files\n",
            "Irinotecan: 533 files\n",
            "Ixazomib: 197 files\n",
            "LB-100: 284 files\n",
            "LJI308: 222 files\n",
            "LY-2584702 (tosylate salt): 99 files\n",
            "LY2090314: 370 files\n",
            "Lapatinib ditosylate: 527 files\n",
            "Lonafarnib: 77 files\n",
            "MK-3903: 229 files\n",
            "MK-8353: 238 files\n",
            "ML264: 404 files\n",
            "Methylprednisolone succinate: 224 files\n",
            "Minodronic acid: 289 files\n",
            "NG25: 263 files\n",
            "NVP-BHG712: 263 files\n",
            "OTS514: 168 files\n",
            "Oleic acid: 231 files\n",
            "Oxaliplatin: 313 files\n",
            "PF-06260933: 160 files\n",
            "PH-797804: 301 files\n",
            "Paclitaxel: 766 files\n",
            "Pemigatinib: 292 files\n",
            "Pimitespib: 172 files\n",
            "Pralsetinib: 164 files\n",
            "RMC-6236: 637 files\n",
            "Ralimetinib dimesylate: 349 files\n",
            "Regorafenib: 303 files\n",
            "SBI-0640756: 164 files\n",
            "Sapanisertib: 675 files\n",
            "Sonidegib: 151 files\n",
            "TAK-733: 262 files\n",
            "TAK-901: 445 files\n",
            "Temuterkib: 386 files\n",
            "Tomivosertib: 385 files\n",
            "Torkinib: 129 files\n",
            "Trametinib: 562 files\n",
            "Tubulin inhibitor 6: 212 files\n",
            "Tucatinib: 563 files\n",
            "ULK-101: 197 files\n",
            "Vemurafenib: 486 files\n",
            "Volasertib: 171 files\n",
            "XRK3F2: 566 files\n",
            "c-Kit-IN-1: 219 files\n",
            "olaparib: 237 files\n",
            "palbociclib: 260 files\n",
            "venetoclax: 223 files\n",
            "vincristine: 197 files\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "output_dir = \"/content/drive/MyDrive/Colab Notebooks/esm cell state/difference counts\"\n",
        "files = os.listdir(output_dir)\n",
        "print(f\"Total number of files in output directory: {len(files)}\")\n",
        "\n",
        "# Optional: count files per drug\n",
        "drug_counts = {}\n",
        "for file in files:\n",
        "    if file.startswith(\"difference_\"):\n",
        "        drug_name = file.split(\"_\")[1]\n",
        "        if \"block\" in drug_name:\n",
        "            # Handle case where drug name might contain underscore\n",
        "            parts = file.split(\"_\")\n",
        "            block_part = [p for p in parts if \"block\" in p][0]\n",
        "            drug_name = \"_\".join(parts[1:parts.index(block_part)])\n",
        "        else:\n",
        "            # Handle case where block is separated by underscore\n",
        "            drug_name = \"_\".join(file.split(\"_\")[1:-1])\n",
        "\n",
        "        drug_counts[drug_name] = drug_counts.get(drug_name, 0) + 1\n",
        "\n",
        "print(\"\\nFiles per drug:\")\n",
        "for drug, count in sorted(drug_counts.items()):\n",
        "    print(f\"{drug}: {count} files\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lK7RrqkIIha"
      },
      "source": [
        "# drug embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGql7adQi1rJ"
      },
      "source": [
        "## mol-gpt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83QBot3Hi2pJ",
        "outputId": "c2299b55-8eaa-485d-ec26-cbb1704a2de6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting molgpt_loader.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile molgpt_loader.py\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "def load_mol_gpt_model():\n",
        "    \"\"\"\n",
        "    Load the molGPT model from Hugging Face.\n",
        "    \"\"\"\n",
        "    model_name = \"msb-roshan/molgpt\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "    # Make sure we have a padding token\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "        model.config.pad_token_id = model.config.eos_token_id\n",
        "\n",
        "    return model, tokenizer\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Test the model loading\n",
        "    print(\"Loading molGPT model...\")\n",
        "    model, tokenizer = load_mol_gpt_model()\n",
        "\n",
        "    # Print model and tokenizer info\n",
        "    print(f\"Model type: {type(model)}\")\n",
        "    print(f\"Model config: {model.config}\")\n",
        "    print(f\"Tokenizer vocab size: {tokenizer.vocab_size}\")\n",
        "    print(f\"Special tokens: {tokenizer.all_special_tokens}\")\n",
        "\n",
        "    # Test generating a molecule\n",
        "    print(\"\\nGenerating a molecule...\")\n",
        "    input_text = \"C\"  # Start with carbon atom\n",
        "    input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
        "\n",
        "    # Generate\n",
        "    with torch.no_grad():\n",
        "        output = model.generate(\n",
        "            input_ids,\n",
        "            max_length=50,\n",
        "            num_return_sequences=1,\n",
        "            do_sample=True,\n",
        "            top_p=0.95,\n",
        "            temperature=0.7,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "            eos_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    # Decode and print\n",
        "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    print(f\"Generated SMILES: {generated_text}\")\n",
        "\n",
        "    # Try to import RDKit for validation\n",
        "    try:\n",
        "        from rdkit import Chem\n",
        "        mol = Chem.MolFromSmiles(generated_text)\n",
        "        if mol:\n",
        "            print(\"Generated a valid molecule!\")\n",
        "        else:\n",
        "            print(\"Generated string is not a valid SMILES.\")\n",
        "    except ImportError:\n",
        "        print(\"RDKit not available for validation.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "-Rc2iCEgx93L"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "YAH58xPtx95f"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "gqv0mtvPx98G"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4oKTuiyJsOaq"
      },
      "source": [
        "# cell embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jt5UeGYvyC0n"
      },
      "source": [
        "## uce torch model load test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ILfmsbcix9-c",
        "outputId": "eff41235-ceaa-48a5-b297-91b33acbc015"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-69-0361e4380a6b>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model = torch.load(\"/content/drive/MyDrive/Colab Notebooks/esm cell state/33l_8ep_1024t_1280.torch\", map_location=\"cpu\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OrderedDict([('pos_encoder.pe', tensor([[[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  ...,  1.0000e+00,\n",
            "           0.0000e+00,  1.0000e+00]],\n",
            "\n",
            "        [[ 8.4147e-01,  5.4030e-01,  8.3367e-01,  ...,  1.0000e+00,\n",
            "           1.0145e-04,  1.0000e+00]],\n",
            "\n",
            "        [[ 9.0930e-01, -4.1615e-01,  9.2082e-01,  ...,  1.0000e+00,\n",
            "           2.0290e-04,  1.0000e+00]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-9.7062e-02,  9.9528e-01,  9.7527e-03,  ...,  9.8758e-01,\n",
            "           1.5490e-01,  9.8793e-01]],\n",
            "\n",
            "        [[ 7.8506e-01,  6.1943e-01, -8.2824e-01,  ...,  9.8756e-01,\n",
            "           1.5500e-01,  9.8792e-01]],\n",
            "\n",
            "        [[ 9.4540e-01, -3.2592e-01, -9.2457e-01,  ...,  9.8755e-01,\n",
            "           1.5510e-01,  9.8790e-01]]])), ('encoder.0.weight', tensor([[-0.0027, -0.0104,  0.0077,  ..., -0.0128, -0.0048,  0.0116],\n",
            "        [-0.0114,  0.0136, -0.0028,  ..., -0.0074, -0.0332,  0.0067],\n",
            "        [-0.0174, -0.0208, -0.0382,  ...,  0.0023, -0.0410,  0.0140],\n",
            "        ...,\n",
            "        [-0.0089,  0.0364,  0.0464,  ...,  0.0295, -0.0050, -0.0363],\n",
            "        [ 0.0002, -0.0031, -0.0196,  ..., -0.0287, -0.0130,  0.0090],\n",
            "        [-0.0332,  0.0277, -0.0144,  ..., -0.0378, -0.0051,  0.0158]])), ('encoder.0.bias', tensor([-0.0226,  0.0238,  0.0173,  ..., -0.0175, -0.0177,  0.0260])), ('encoder.2.weight', tensor([1.0287, 0.9928, 0.9988,  ..., 1.0174, 1.0141, 0.9923])), ('encoder.2.bias', tensor([ 0.0077,  0.0123, -0.0040,  ..., -0.0027, -0.0122, -0.0038])), ('transformer_encoder.layers.0.self_attn.in_proj_weight', tensor([[ 0.0434, -0.0396,  0.0778,  ..., -0.0002,  0.0406,  0.0313],\n",
            "        [ 0.0251, -0.0620,  0.0544,  ...,  0.0545,  0.0052,  0.0112],\n",
            "        [-0.0162, -0.0060,  0.0380,  ..., -0.0222,  0.0174,  0.0573],\n",
            "        ...,\n",
            "        [ 0.0061,  0.0397, -0.0250,  ...,  0.0025,  0.0277,  0.0194],\n",
            "        [ 0.0018, -0.0052, -0.0262,  ...,  0.0349, -0.0067,  0.0053],\n",
            "        [-0.0221, -0.0018,  0.0095,  ..., -0.0104,  0.0080, -0.0213]])), ('transformer_encoder.layers.0.self_attn.in_proj_bias', tensor([-0.0119, -0.0544, -0.0341,  ..., -0.0044,  0.0001, -0.0017])), ('transformer_encoder.layers.0.self_attn.out_proj.weight', tensor([[-0.0030,  0.0017,  0.0018,  ...,  0.0050, -0.0225,  0.0141],\n",
            "        [ 0.0067,  0.0082,  0.0151,  ...,  0.0065,  0.0055,  0.0163],\n",
            "        [-0.0147,  0.0017, -0.0130,  ...,  0.0233,  0.0113, -0.0097],\n",
            "        ...,\n",
            "        [ 0.0064,  0.0003, -0.0020,  ...,  0.0168,  0.0110, -0.0029],\n",
            "        [ 0.0139,  0.0075, -0.0203,  ...,  0.0020,  0.0142, -0.0087],\n",
            "        [ 0.0085, -0.0094,  0.0221,  ..., -0.0070,  0.0076,  0.0065]])), ('transformer_encoder.layers.0.self_attn.out_proj.bias', tensor([-0.0122,  0.0006,  0.0279,  ..., -0.0035,  0.0174,  0.0331])), ('transformer_encoder.layers.0.linear1.weight', tensor([[-0.0605, -0.0076,  0.0142,  ...,  0.0732,  0.0037, -0.0441],\n",
            "        [ 0.0423, -0.0036,  0.0503,  ..., -0.0209,  0.0107,  0.0289],\n",
            "        [ 0.0120,  0.0135, -0.0232,  ...,  0.0065,  0.0184,  0.0232],\n",
            "        ...,\n",
            "        [-0.0249,  0.0170, -0.0468,  ...,  0.0177, -0.0442, -0.0298],\n",
            "        [ 0.0275, -0.0318,  0.0313,  ..., -0.0133, -0.0316,  0.0255],\n",
            "        [-0.0209, -0.0225,  0.0457,  ...,  0.0357,  0.0008,  0.0024]])), ('transformer_encoder.layers.0.linear1.bias', tensor([-0.0003, -0.0097, -0.0338,  ..., -0.0120, -0.0217, -0.0087])), ('transformer_encoder.layers.0.linear2.weight', tensor([[-0.0034,  0.0316,  0.0383,  ..., -0.0062, -0.0024,  0.0176],\n",
            "        [ 0.0068, -0.0180,  0.0071,  ...,  0.0091, -0.0104,  0.0246],\n",
            "        [-0.0071,  0.0183, -0.0029,  ...,  0.0009,  0.0131,  0.0138],\n",
            "        ...,\n",
            "        [ 0.0072, -0.0143,  0.0106,  ..., -0.0205,  0.0318, -0.0239],\n",
            "        [-0.0232,  0.0104,  0.0238,  ..., -0.0194, -0.0060,  0.0100],\n",
            "        [ 0.0270, -0.0119,  0.0089,  ..., -0.0035,  0.0105,  0.0171]])), ('transformer_encoder.layers.0.linear2.bias', tensor([-0.0084, -0.0109, -0.0140,  ...,  0.0175, -0.0016, -0.0182])), ('transformer_encoder.layers.0.norm1.weight', tensor([1.1642, 1.0716, 1.0828,  ..., 1.0761, 0.9408, 1.1255])), ('transformer_encoder.layers.0.norm1.bias', tensor([-0.0030, -0.0022,  0.0432,  ...,  0.0179,  0.0536,  0.0637])), ('transformer_encoder.layers.0.norm2.weight', tensor([1.0156, 0.9776, 0.9921,  ..., 1.0045, 0.9647, 1.0151])), ('transformer_encoder.layers.0.norm2.bias', tensor([ 0.0072, -0.0291, -0.0010,  ...,  0.0105,  0.0268, -0.0165])), ('transformer_encoder.layers.1.self_attn.in_proj_weight', tensor([[ 0.0022, -0.0032,  0.0172,  ..., -0.0131,  0.0308,  0.0445],\n",
            "        [ 0.0306, -0.0376,  0.0223,  ...,  0.0135, -0.0016,  0.0195],\n",
            "        [-0.0335,  0.0178, -0.0015,  ..., -0.0308, -0.0251,  0.0335],\n",
            "        ...,\n",
            "        [-0.0106,  0.0003, -0.0303,  ...,  0.0092,  0.0379,  0.0184],\n",
            "        [ 0.0123, -0.0156, -0.0372,  ..., -0.0075, -0.0167, -0.0064],\n",
            "        [-0.0382, -0.0012,  0.0138,  ...,  0.0209,  0.0039, -0.0297]])), ('transformer_encoder.layers.1.self_attn.in_proj_bias', tensor([-2.2534e-03, -7.0385e-03,  9.8420e-05,  ..., -2.6333e-03,\n",
            "        -5.8376e-04,  3.6002e-03])), ('transformer_encoder.layers.1.self_attn.out_proj.weight', tensor([[ 0.0070, -0.0107, -0.0032,  ...,  0.0148, -0.0331, -0.0142],\n",
            "        [-0.0097, -0.0100,  0.0245,  ...,  0.0163,  0.0009,  0.0041],\n",
            "        [-0.0294, -0.0369, -0.0059,  ...,  0.0066,  0.0346, -0.0211],\n",
            "        ...,\n",
            "        [ 0.0093, -0.0149, -0.0017,  ...,  0.0066,  0.0354, -0.0022],\n",
            "        [-0.0084,  0.0073, -0.0148,  ..., -0.0091,  0.0073, -0.0119],\n",
            "        [ 0.0158, -0.0110,  0.0122,  ..., -0.0220,  0.0062,  0.0078]])), ('transformer_encoder.layers.1.self_attn.out_proj.bias', tensor([ 0.0230, -0.0026,  0.0020,  ...,  0.0110,  0.0053,  0.0145])), ('transformer_encoder.layers.1.linear1.weight', tensor([[-0.0766, -0.0059,  0.0031,  ...,  0.0097,  0.0196, -0.0304],\n",
            "        [ 0.0135,  0.0097,  0.0599,  ..., -0.0336,  0.0154,  0.0016],\n",
            "        [ 0.0219,  0.0102, -0.0129,  ..., -0.0714,  0.0096,  0.0111],\n",
            "        ...,\n",
            "        [-0.0279,  0.0221, -0.0539,  ...,  0.0022, -0.0217, -0.0151],\n",
            "        [ 0.0253, -0.0222,  0.0470,  ..., -0.0042,  0.0021,  0.0246],\n",
            "        [-0.0350, -0.0028,  0.0346,  ...,  0.0214, -0.0146, -0.0095]])), ('transformer_encoder.layers.1.linear1.bias', tensor([ 0.0078, -0.0124, -0.0329,  ..., -0.0148, -0.0294, -0.0093])), ('transformer_encoder.layers.1.linear2.weight', tensor([[ 0.0034,  0.0029,  0.0048,  ..., -0.0258,  0.0129, -0.0049],\n",
            "        [ 0.0122, -0.0309,  0.0148,  ...,  0.0040,  0.0062,  0.0295],\n",
            "        [-0.0010,  0.0313,  0.0012,  ..., -0.0078, -0.0067, -0.0023],\n",
            "        ...,\n",
            "        [ 0.0035, -0.0441, -0.0021,  ..., -0.0430, -0.0029,  0.0048],\n",
            "        [-0.0096,  0.0355,  0.0013,  ..., -0.0284,  0.0001,  0.0129],\n",
            "        [ 0.0020, -0.0101, -0.0216,  ..., -0.0001,  0.0176,  0.0110]])), ('transformer_encoder.layers.1.linear2.bias', tensor([-0.0216,  0.0055, -0.0190,  ...,  0.0127,  0.0029, -0.0094])), ('transformer_encoder.layers.1.norm1.weight', tensor([1.0319, 0.9996, 1.0078,  ..., 1.0123, 0.9748, 1.0284])), ('transformer_encoder.layers.1.norm1.bias', tensor([ 0.0322, -0.0015,  0.0060,  ...,  0.0182,  0.0113,  0.0244])), ('transformer_encoder.layers.1.norm2.weight', tensor([0.9794, 0.9779, 0.9831,  ..., 0.9792, 0.9814, 0.9976])), ('transformer_encoder.layers.1.norm2.bias', tensor([-0.0182, -0.0057, -0.0133,  ..., -0.0033,  0.0135, -0.0193])), ('transformer_encoder.layers.2.self_attn.in_proj_weight', tensor([[ 0.0072, -0.0416,  0.0406,  ...,  0.0045,  0.0142,  0.0462],\n",
            "        [ 0.0389, -0.0097,  0.0376,  ...,  0.0328, -0.0178, -0.0019],\n",
            "        [-0.0115,  0.0047,  0.0346,  ..., -0.0226, -0.0100,  0.0288],\n",
            "        ...,\n",
            "        [ 0.0096,  0.0328, -0.0361,  ...,  0.0146,  0.0332,  0.0304],\n",
            "        [ 0.0166, -0.0037, -0.0350,  ...,  0.0233, -0.0126, -0.0097],\n",
            "        [-0.0211,  0.0131,  0.0010,  ...,  0.0022,  0.0057, -0.0228]])), ('transformer_encoder.layers.2.self_attn.in_proj_bias', tensor([-0.0005, -0.0030, -0.0044,  ..., -0.0026, -0.0015, -0.0003])), ('transformer_encoder.layers.2.self_attn.out_proj.weight', tensor([[ 0.0010, -0.0024, -0.0005,  ...,  0.0097, -0.0264,  0.0316],\n",
            "        [ 0.0129,  0.0210,  0.0090,  ...,  0.0260,  0.0240,  0.0108],\n",
            "        [-0.0245, -0.0076, -0.0072,  ...,  0.0222,  0.0230, -0.0035],\n",
            "        ...,\n",
            "        [ 0.0063,  0.0263, -0.0110,  ..., -0.0076,  0.0223,  0.0084],\n",
            "        [-0.0263,  0.0235, -0.0228,  ..., -0.0076,  0.0021, -0.0239],\n",
            "        [ 0.0030,  0.0046,  0.0091,  ...,  0.0076,  0.0186,  0.0002]])), ('transformer_encoder.layers.2.self_attn.out_proj.bias', tensor([ 0.0170, -0.0020, -0.0012,  ...,  0.0006,  0.0167,  0.0040])), ('transformer_encoder.layers.2.linear1.weight', tensor([[-0.0538, -0.0124,  0.0084,  ...,  0.0159,  0.0108, -0.0593],\n",
            "        [ 0.0050, -0.0315,  0.0162,  ..., -0.0139, -0.0155,  0.0280],\n",
            "        [ 0.0284,  0.0203, -0.0053,  ..., -0.0373,  0.0336,  0.0300],\n",
            "        ...,\n",
            "        [ 0.0095,  0.0271, -0.0573,  ...,  0.0048, -0.0128,  0.0265],\n",
            "        [ 0.0302, -0.0093,  0.0153,  ..., -0.0204,  0.0118, -0.0113],\n",
            "        [-0.0215, -0.0116, -0.0004,  ...,  0.0386, -0.0009, -0.0210]])), ('transformer_encoder.layers.2.linear1.bias', tensor([ 0.0038, -0.0125, -0.0433,  ..., -0.0101, -0.0309, -0.0075])), ('transformer_encoder.layers.2.linear2.weight', tensor([[-0.0339, -0.0049,  0.0354,  ...,  0.0065,  0.0176, -0.0100],\n",
            "        [ 0.0127, -0.0273,  0.0159,  ..., -0.0051, -0.0103,  0.0225],\n",
            "        [-0.0019,  0.0033,  0.0267,  ..., -0.0152, -0.0014,  0.0090],\n",
            "        ...,\n",
            "        [ 0.0031,  0.0067, -0.0283,  ..., -0.0144,  0.0121,  0.0071],\n",
            "        [ 0.0084,  0.0162, -0.0258,  ..., -0.0501, -0.0070,  0.0020],\n",
            "        [ 0.0128,  0.0133, -0.0141,  ...,  0.0226,  0.0016,  0.0021]])), ('transformer_encoder.layers.2.linear2.bias', tensor([-0.0198,  0.0074, -0.0211,  ...,  0.0190,  0.0118, -0.0221])), ('transformer_encoder.layers.2.norm1.weight', tensor([0.9993, 1.0029, 1.0039,  ..., 1.0036, 1.0025, 1.0096])), ('transformer_encoder.layers.2.norm1.bias', tensor([ 0.0184, -0.0034, -0.0011,  ...,  0.0003,  0.0110,  0.0002])), ('transformer_encoder.layers.2.norm2.weight', tensor([0.9857, 0.9875, 0.9949,  ..., 0.9819, 0.9993, 0.9988])), ('transformer_encoder.layers.2.norm2.bias', tensor([-0.0178,  0.0066, -0.0085,  ...,  0.0066,  0.0224, -0.0145])), ('transformer_encoder.layers.3.self_attn.in_proj_weight', tensor([[-0.0040, -0.0543,  0.0238,  ...,  0.0048,  0.0289,  0.0392],\n",
            "        [ 0.0398, -0.0281,  0.0058,  ...,  0.0271, -0.0310,  0.0152],\n",
            "        [-0.0005,  0.0195,  0.0139,  ..., -0.0255, -0.0166,  0.0419],\n",
            "        ...,\n",
            "        [-0.0107,  0.0209, -0.0488,  ...,  0.0063,  0.0257,  0.0021],\n",
            "        [-0.0183, -0.0154, -0.0095,  ...,  0.0166, -0.0145, -0.0157],\n",
            "        [-0.0073,  0.0042,  0.0324,  ..., -0.0018, -0.0051, -0.0078]])), ('transformer_encoder.layers.3.self_attn.in_proj_bias', tensor([-0.0031, -0.0003, -0.0023,  ..., -0.0004, -0.0023, -0.0009])), ('transformer_encoder.layers.3.self_attn.out_proj.weight', tensor([[-0.0085, -0.0204,  0.0358,  ...,  0.0140, -0.0431,  0.0172],\n",
            "        [ 0.0229,  0.0031,  0.0383,  ..., -0.0059,  0.0195,  0.0108],\n",
            "        [-0.0062,  0.0137,  0.0034,  ...,  0.0410,  0.0173, -0.0223],\n",
            "        ...,\n",
            "        [ 0.0226,  0.0175, -0.0122,  ..., -0.0061,  0.0299, -0.0028],\n",
            "        [ 0.0020, -0.0078, -0.0231,  ...,  0.0113,  0.0201, -0.0335],\n",
            "        [-0.0033, -0.0018,  0.0153,  ...,  0.0079,  0.0105, -0.0153]])), ('transformer_encoder.layers.3.self_attn.out_proj.bias', tensor([ 0.0182, -0.0021,  0.0054,  ..., -0.0003, -0.0045,  0.0081])), ('transformer_encoder.layers.3.linear1.weight', tensor([[-0.0342, -0.0032, -0.0022,  ...,  0.0517,  0.0469, -0.0187],\n",
            "        [-0.0466, -0.0160,  0.0364,  ..., -0.0272,  0.0119, -0.0006],\n",
            "        [ 0.0222, -0.0067, -0.0256,  ..., -0.0176,  0.0509,  0.0084],\n",
            "        ...,\n",
            "        [-0.0177,  0.0223, -0.0315,  ...,  0.0024,  0.0019,  0.0104],\n",
            "        [ 0.0095, -0.0154, -0.0399,  ...,  0.0262, -0.0186, -0.0128],\n",
            "        [-0.0385, -0.0207,  0.0198,  ...,  0.0531,  0.0133, -0.0039]])), ('transformer_encoder.layers.3.linear1.bias', tensor([ 0.0081, -0.0133, -0.0388,  ..., -0.0068, -0.0271, -0.0176])), ('transformer_encoder.layers.3.linear2.weight', tensor([[-0.0673, -0.0358, -0.0036,  ...,  0.0101,  0.0439, -0.0068],\n",
            "        [ 0.0219, -0.0213, -0.0168,  ...,  0.0202, -0.0071,  0.0063],\n",
            "        [ 0.0055,  0.0255, -0.0007,  ..., -0.0237, -0.0376, -0.0061],\n",
            "        ...,\n",
            "        [ 0.0255,  0.0457, -0.0225,  ...,  0.0163,  0.0182, -0.0086],\n",
            "        [ 0.0225,  0.0059, -0.0306,  ..., -0.0167,  0.0174, -0.0132],\n",
            "        [ 0.0245, -0.0400,  0.0078,  ...,  0.0129,  0.0310,  0.0277]])), ('transformer_encoder.layers.3.linear2.bias', tensor([-0.0179, -0.0003, -0.0162,  ...,  0.0185,  0.0080, -0.0288])), ('transformer_encoder.layers.3.norm1.weight', tensor([1.0167, 0.9960, 0.9984,  ..., 0.9974, 0.9991, 0.9980])), ('transformer_encoder.layers.3.norm1.bias', tensor([ 0.0226, -0.0051,  0.0081,  ...,  0.0013, -0.0029,  0.0090])), ('transformer_encoder.layers.3.norm2.weight', tensor([0.9861, 0.9913, 0.9863,  ..., 0.9949, 0.9878, 0.9857])), ('transformer_encoder.layers.3.norm2.bias', tensor([-0.0200,  0.0023, -0.0060,  ...,  0.0080,  0.0161, -0.0192])), ('transformer_encoder.layers.4.self_attn.in_proj_weight', tensor([[-7.5725e-03,  3.1904e-03,  1.9731e-02,  ...,  2.2788e-02,\n",
            "          3.6724e-02,  4.8000e-02],\n",
            "        [ 1.5747e-02, -3.7519e-02,  3.0496e-02,  ...,  1.0228e-02,\n",
            "         -2.7446e-02,  2.4826e-02],\n",
            "        [-2.2916e-02,  2.9117e-02,  8.5762e-03,  ..., -2.1347e-02,\n",
            "          1.6348e-03,  4.7868e-02],\n",
            "        ...,\n",
            "        [ 1.3756e-02,  1.7127e-03, -3.7933e-02,  ...,  6.8819e-03,\n",
            "          2.7532e-02,  1.7329e-02],\n",
            "        [ 1.5697e-02,  8.7772e-03, -2.1100e-02,  ...,  1.9283e-03,\n",
            "         -2.5389e-02, -5.3605e-03],\n",
            "        [ 5.3583e-03,  8.4693e-03, -1.4196e-02,  ...,  2.1745e-03,\n",
            "          2.2517e-05, -2.8213e-02]])), ('transformer_encoder.layers.4.self_attn.in_proj_bias', tensor([-0.0028, -0.0025, -0.0035,  ..., -0.0023, -0.0038,  0.0009])), ('transformer_encoder.layers.4.self_attn.out_proj.weight', tensor([[-0.0171, -0.0216,  0.0171,  ...,  0.0093, -0.0158,  0.0277],\n",
            "        [ 0.0202, -0.0143,  0.0132,  ...,  0.0175,  0.0142, -0.0047],\n",
            "        [-0.0342,  0.0170,  0.0054,  ...,  0.0341,  0.0214, -0.0131],\n",
            "        ...,\n",
            "        [ 0.0154,  0.0155, -0.0236,  ...,  0.0253,  0.0345, -0.0053],\n",
            "        [-0.0098,  0.0085, -0.0258,  ..., -0.0121,  0.0099, -0.0229],\n",
            "        [-0.0079, -0.0156,  0.0338,  ...,  0.0064,  0.0215,  0.0059]])), ('transformer_encoder.layers.4.self_attn.out_proj.bias', tensor([ 0.0181, -0.0115,  0.0098,  ...,  0.0048, -0.0086,  0.0075])), ('transformer_encoder.layers.4.linear1.weight', tensor([[-0.0310,  0.0341, -0.0222,  ...,  0.0106,  0.0397, -0.0351],\n",
            "        [ 0.0012, -0.0363,  0.0076,  ..., -0.0365,  0.0117,  0.0327],\n",
            "        [-0.0036,  0.0269, -0.0494,  ..., -0.0419,  0.0304,  0.0197],\n",
            "        ...,\n",
            "        [ 0.0170,  0.0335, -0.0139,  ...,  0.0038,  0.0121, -0.0099],\n",
            "        [ 0.0286, -0.0049,  0.0080,  ..., -0.0041, -0.0440,  0.0216],\n",
            "        [-0.0394, -0.0318,  0.0070,  ...,  0.0258,  0.0035, -0.0248]])), ('transformer_encoder.layers.4.linear1.bias', tensor([ 0.0141, -0.0074, -0.0321,  ..., -0.0074, -0.0213, -0.0109])), ('transformer_encoder.layers.4.linear2.weight', tensor([[-0.0308,  0.0041, -0.0074,  ...,  0.0295,  0.0011,  0.0052],\n",
            "        [-0.0070,  0.0139,  0.0252,  ..., -0.0062, -0.0168,  0.0173],\n",
            "        [-0.0146, -0.0004,  0.0039,  ..., -0.0284, -0.0144,  0.0261],\n",
            "        ...,\n",
            "        [-0.0106,  0.0292, -0.0156,  ...,  0.0042, -0.0070, -0.0182],\n",
            "        [ 0.0074,  0.0062,  0.0022,  ...,  0.0011,  0.0066, -0.0170],\n",
            "        [ 0.0211, -0.0168,  0.0073,  ..., -0.0153,  0.0014,  0.0055]])), ('transformer_encoder.layers.4.linear2.bias', tensor([-0.0200, -0.0130, -0.0201,  ...,  0.0151,  0.0010, -0.0166])), ('transformer_encoder.layers.4.norm1.weight', tensor([1.0054, 0.9906, 0.9958,  ..., 1.0048, 1.0069, 1.0070])), ('transformer_encoder.layers.4.norm1.bias', tensor([ 0.0210, -0.0153,  0.0107,  ...,  0.0072, -0.0055,  0.0073])), ('transformer_encoder.layers.4.norm2.weight', tensor([0.9758, 0.9889, 0.9906,  ..., 0.9974, 0.9920, 0.9850])), ('transformer_encoder.layers.4.norm2.bias', tensor([-0.0206, -0.0014, -0.0096,  ...,  0.0025,  0.0072, -0.0170])), ('transformer_encoder.layers.5.self_attn.in_proj_weight', tensor([[ 0.0016, -0.0273,  0.0302,  ...,  0.0029,  0.0264,  0.0330],\n",
            "        [ 0.0151, -0.0231,  0.0296,  ...,  0.0116, -0.0108,  0.0077],\n",
            "        [-0.0246,  0.0171,  0.0345,  ..., -0.0146, -0.0156,  0.0319],\n",
            "        ...,\n",
            "        [ 0.0082,  0.0365, -0.0223,  ...,  0.0135,  0.0477,  0.0427],\n",
            "        [-0.0128, -0.0029, -0.0200,  ..., -0.0066, -0.0038, -0.0211],\n",
            "        [-0.0035,  0.0059,  0.0151,  ...,  0.0234,  0.0292, -0.0086]])), ('transformer_encoder.layers.5.self_attn.in_proj_bias', tensor([ 6.6688e-03, -8.4885e-03,  1.7997e-05,  ..., -3.8257e-03,\n",
            "        -4.2814e-03,  6.0594e-05])), ('transformer_encoder.layers.5.self_attn.out_proj.weight', tensor([[-2.5958e-03, -2.2252e-02,  3.2567e-02,  ...,  2.1000e-02,\n",
            "         -3.1368e-02,  2.7914e-02],\n",
            "        [ 1.1396e-02,  4.5492e-03, -9.9112e-03,  ...,  1.2254e-02,\n",
            "          2.4486e-03,  1.6055e-02],\n",
            "        [-2.7882e-02, -1.2267e-03,  3.4833e-03,  ...,  1.9047e-02,\n",
            "          1.3114e-02,  3.7826e-05],\n",
            "        ...,\n",
            "        [ 4.9934e-03,  1.5924e-02, -2.2672e-02,  ...,  3.5666e-02,\n",
            "          1.5823e-02,  1.8423e-03],\n",
            "        [ 1.2057e-02,  2.8525e-02, -2.4671e-02,  ..., -4.2137e-03,\n",
            "         -2.3029e-03,  5.0545e-04],\n",
            "        [ 1.4252e-02, -1.4944e-02,  1.9653e-02,  ..., -2.2772e-02,\n",
            "          3.2034e-03, -1.9766e-02]])), ('transformer_encoder.layers.5.self_attn.out_proj.bias', tensor([ 0.0197, -0.0011,  0.0062,  ...,  0.0066, -0.0122,  0.0042])), ('transformer_encoder.layers.5.linear1.weight', tensor([[-0.0397,  0.0101, -0.0050,  ...,  0.0287,  0.0568, -0.0087],\n",
            "        [ 0.0164, -0.0453,  0.0169,  ..., -0.0036,  0.0151,  0.0275],\n",
            "        [-0.0081,  0.0419, -0.0184,  ..., -0.0007,  0.0209,  0.0316],\n",
            "        ...,\n",
            "        [ 0.0066,  0.0308, -0.0320,  ...,  0.0056, -0.0003,  0.0215],\n",
            "        [-0.0069, -0.0002,  0.0222,  ...,  0.0362, -0.0112,  0.0142],\n",
            "        [-0.0688, -0.0085,  0.0356,  ...,  0.0115, -0.0034, -0.0161]])), ('transformer_encoder.layers.5.linear1.bias', tensor([ 0.0127, -0.0104, -0.0337,  ..., -0.0097, -0.0198, -0.0164])), ('transformer_encoder.layers.5.linear2.weight', tensor([[-0.0319, -0.0107,  0.0011,  ..., -0.0032, -0.0099,  0.0203],\n",
            "        [ 0.0136, -0.0210,  0.0267,  ...,  0.0123,  0.0013,  0.0485],\n",
            "        [-0.0040,  0.0171,  0.0141,  ..., -0.0122,  0.0218,  0.0176],\n",
            "        ...,\n",
            "        [-0.0043, -0.0079, -0.0007,  ..., -0.0154,  0.0503, -0.0012],\n",
            "        [ 0.0020,  0.0106,  0.0029,  ...,  0.0030, -0.0192,  0.0187],\n",
            "        [ 0.0031, -0.0078, -0.0260,  ...,  0.0324, -0.0027,  0.0123]])), ('transformer_encoder.layers.5.linear2.bias', tensor([-0.0273, -0.0158, -0.0224,  ...,  0.0100, -0.0051, -0.0093])), ('transformer_encoder.layers.5.norm1.weight', tensor([1.0129, 0.9783, 0.9927,  ..., 1.0003, 0.9964, 1.0032])), ('transformer_encoder.layers.5.norm1.bias', tensor([ 0.0218, -0.0013,  0.0073,  ...,  0.0090, -0.0074,  0.0013])), ('transformer_encoder.layers.5.norm2.weight', tensor([0.9743, 0.9799, 0.9807,  ..., 0.9927, 0.9893, 0.9984])), ('transformer_encoder.layers.5.norm2.bias', tensor([-0.0291, -0.0074, -0.0125,  ..., -0.0004,  0.0057, -0.0065])), ('transformer_encoder.layers.6.self_attn.in_proj_weight', tensor([[-0.0066, -0.0190,  0.0170,  ...,  0.0128,  0.0299,  0.0077],\n",
            "        [ 0.0461, -0.0430,  0.0415,  ...,  0.0038, -0.0101,  0.0068],\n",
            "        [-0.0205, -0.0076,  0.0285,  ..., -0.0267, -0.0078,  0.0424],\n",
            "        ...,\n",
            "        [-0.0006,  0.0009, -0.0342,  ..., -0.0060,  0.0132,  0.0282],\n",
            "        [ 0.0046,  0.0071, -0.0180,  ...,  0.0301, -0.0158,  0.0020],\n",
            "        [-0.0109, -0.0214,  0.0045,  ..., -0.0089,  0.0054, -0.0321]])), ('transformer_encoder.layers.6.self_attn.in_proj_bias', tensor([ 1.9009e-03, -4.5220e-03, -6.7303e-04,  ..., -1.5001e-03,\n",
            "        -3.8063e-03, -1.8572e-06])), ('transformer_encoder.layers.6.self_attn.out_proj.weight', tensor([[ 0.0034, -0.0107,  0.0010,  ...,  0.0026, -0.0316,  0.0230],\n",
            "        [ 0.0079,  0.0117,  0.0329,  ...,  0.0226,  0.0151, -0.0113],\n",
            "        [-0.0302, -0.0027,  0.0082,  ..., -0.0018,  0.0202,  0.0045],\n",
            "        ...,\n",
            "        [ 0.0282,  0.0157, -0.0115,  ...,  0.0230,  0.0239, -0.0037],\n",
            "        [-0.0099,  0.0073, -0.0324,  ..., -0.0196,  0.0049, -0.0103],\n",
            "        [ 0.0205, -0.0269,  0.0195,  ..., -0.0007,  0.0210, -0.0339]])), ('transformer_encoder.layers.6.self_attn.out_proj.bias', tensor([ 0.0196,  0.0017,  0.0011,  ...,  0.0039, -0.0120,  0.0065])), ('transformer_encoder.layers.6.linear1.weight', tensor([[-0.0501, -0.0040, -0.0014,  ...,  0.0381,  0.0274, -0.0172],\n",
            "        [-0.0225, -0.0100, -0.0120,  ..., -0.0199,  0.0101,  0.0482],\n",
            "        [-0.0223,  0.0580, -0.0502,  ..., -0.0514,  0.0576,  0.0076],\n",
            "        ...,\n",
            "        [ 0.0072,  0.0099, -0.0211,  ...,  0.0203,  0.0047,  0.0024],\n",
            "        [ 0.0297, -0.0324,  0.0212,  ...,  0.0027, -0.0067, -0.0184],\n",
            "        [-0.0229, -0.0248,  0.0541,  ...,  0.0236, -0.0055, -0.0230]])), ('transformer_encoder.layers.6.linear1.bias', tensor([ 0.0146, -0.0132, -0.0323,  ..., -0.0092, -0.0265, -0.0105])), ('transformer_encoder.layers.6.linear2.weight', tensor([[-0.0169,  0.0275,  0.0026,  ..., -0.0067,  0.0207, -0.0030],\n",
            "        [-0.0069,  0.0021, -0.0054,  ...,  0.0120,  0.0321,  0.0445],\n",
            "        [ 0.0048,  0.0006, -0.0107,  ..., -0.0099,  0.0090,  0.0305],\n",
            "        ...,\n",
            "        [-0.0004, -0.0398, -0.0240,  ...,  0.0048,  0.0229, -0.0114],\n",
            "        [-0.0258, -0.0221,  0.0370,  ..., -0.0165,  0.0098,  0.0239],\n",
            "        [ 0.0299,  0.0086, -0.0145,  ...,  0.0058, -0.0005, -0.0048]])), ('transformer_encoder.layers.6.linear2.bias', tensor([-0.0255, -0.0238, -0.0189,  ...,  0.0035, -0.0055, -0.0071])), ('transformer_encoder.layers.6.norm1.weight', tensor([1.0062, 0.9660, 0.9780,  ..., 0.9932, 0.9770, 0.9961])), ('transformer_encoder.layers.6.norm1.bias', tensor([ 2.0467e-02,  2.6914e-03, -1.2294e-05,  ...,  7.8820e-03,\n",
            "        -1.0675e-02,  1.9261e-03])), ('transformer_encoder.layers.6.norm2.weight', tensor([0.9729, 0.9808, 0.9783,  ..., 0.9927, 0.9776, 0.9940])), ('transformer_encoder.layers.6.norm2.bias', tensor([-0.0275, -0.0160, -0.0082,  ..., -0.0057,  0.0047, -0.0035])), ('transformer_encoder.layers.7.self_attn.in_proj_weight', tensor([[-0.0093, -0.0265,  0.0197,  ..., -0.0173,  0.0203,  0.0108],\n",
            "        [ 0.0104, -0.0382,  0.0245,  ...,  0.0338, -0.0097,  0.0124],\n",
            "        [-0.0348,  0.0248, -0.0007,  ..., -0.0196, -0.0271,  0.0406],\n",
            "        ...,\n",
            "        [-0.0139,  0.0140, -0.0206,  ..., -0.0129,  0.0256,  0.0275],\n",
            "        [-0.0017,  0.0321, -0.0126,  ...,  0.0091, -0.0368, -0.0041],\n",
            "        [-0.0272, -0.0034, -0.0036,  ...,  0.0135,  0.0212, -0.0225]])), ('transformer_encoder.layers.7.self_attn.in_proj_bias', tensor([ 0.0051, -0.0017, -0.0011,  ..., -0.0022, -0.0026,  0.0016])), ('transformer_encoder.layers.7.self_attn.out_proj.weight', tensor([[-0.0243, -0.0008,  0.0036,  ...,  0.0145, -0.0300,  0.0244],\n",
            "        [ 0.0157,  0.0229,  0.0359,  ...,  0.0109,  0.0027,  0.0087],\n",
            "        [-0.0238,  0.0255,  0.0078,  ...,  0.0227,  0.0251, -0.0035],\n",
            "        ...,\n",
            "        [ 0.0188,  0.0277, -0.0148,  ...,  0.0489,  0.0152,  0.0023],\n",
            "        [ 0.0052,  0.0259, -0.0075,  ..., -0.0129, -0.0021, -0.0251],\n",
            "        [ 0.0102, -0.0255,  0.0161,  ..., -0.0080,  0.0101, -0.0151]])), ('transformer_encoder.layers.7.self_attn.out_proj.bias', tensor([ 0.0173,  0.0067,  0.0004,  ...,  0.0037, -0.0133,  0.0010])), ('transformer_encoder.layers.7.linear1.weight', tensor([[-2.8584e-02,  9.9964e-03,  1.2204e-02,  ...,  1.8923e-02,\n",
            "          2.1561e-02, -2.2146e-02],\n",
            "        [-4.7244e-02, -1.7980e-02,  4.4675e-02,  ..., -3.5717e-03,\n",
            "          1.0541e-02,  3.6575e-02],\n",
            "        [-2.9846e-02, -8.7356e-05, -5.3361e-02,  ..., -5.5071e-02,\n",
            "          2.8072e-02,  5.2893e-02],\n",
            "        ...,\n",
            "        [ 5.2060e-03,  3.1826e-02, -1.9606e-02,  ...,  2.4255e-03,\n",
            "         -9.5514e-03, -9.8220e-03],\n",
            "        [ 1.5305e-03, -2.1462e-02,  1.9645e-02,  ..., -5.5315e-03,\n",
            "         -9.4661e-03,  7.9963e-03],\n",
            "        [-2.5193e-02,  2.1494e-02,  1.9598e-02,  ..., -7.1259e-03,\n",
            "          1.4048e-02, -3.4852e-02]])), ('transformer_encoder.layers.7.linear1.bias', tensor([ 0.0081, -0.0187, -0.0309,  ..., -0.0083, -0.0225, -0.0086])), ('transformer_encoder.layers.7.linear2.weight', tensor([[-0.0129,  0.0012,  0.0003,  ..., -0.0127,  0.0073,  0.0020],\n",
            "        [ 0.0014,  0.0070,  0.0271,  ..., -0.0112,  0.0349,  0.0448],\n",
            "        [-0.0026,  0.0335,  0.0277,  ...,  0.0114,  0.0130,  0.0347],\n",
            "        ...,\n",
            "        [-0.0097, -0.0132,  0.0038,  ..., -0.0017,  0.0109, -0.0347],\n",
            "        [ 0.0026,  0.0025, -0.0075,  ..., -0.0137, -0.0451,  0.0216],\n",
            "        [ 0.0056, -0.0114, -0.0203,  ...,  0.0160,  0.0017,  0.0055]])), ('transformer_encoder.layers.7.linear2.bias', tensor([-0.0291, -0.0272, -0.0152,  ..., -0.0032, -0.0051,  0.0019])), ('transformer_encoder.layers.7.norm1.weight', tensor([0.9946, 0.9725, 0.9563,  ..., 0.9799, 0.9603, 0.9788])), ('transformer_encoder.layers.7.norm1.bias', tensor([ 0.0172,  0.0084,  0.0002,  ...,  0.0079, -0.0101, -0.0022])), ('transformer_encoder.layers.7.norm2.weight', tensor([0.9658, 0.9856, 0.9731,  ..., 0.9915, 0.9793, 0.9903])), ('transformer_encoder.layers.7.norm2.bias', tensor([-0.0330, -0.0209, -0.0032,  ..., -0.0091,  0.0045,  0.0084])), ('transformer_encoder.layers.8.self_attn.in_proj_weight', tensor([[ 0.0255, -0.0607,  0.0421,  ..., -0.0072,  0.0302,  0.0084],\n",
            "        [ 0.0165, -0.0401,  0.0216,  ...,  0.0231, -0.0220,  0.0140],\n",
            "        [-0.0187,  0.0143,  0.0347,  ..., -0.0118, -0.0328,  0.0113],\n",
            "        ...,\n",
            "        [ 0.0073,  0.0313, -0.0339,  ..., -0.0254,  0.0255,  0.0289],\n",
            "        [-0.0141,  0.0028, -0.0177,  ...,  0.0198, -0.0382, -0.0098],\n",
            "        [-0.0117,  0.0004,  0.0044,  ...,  0.0131,  0.0012, -0.0146]])), ('transformer_encoder.layers.8.self_attn.in_proj_bias', tensor([-1.2705e-03, -1.8685e-04, -2.3092e-03,  ..., -3.6272e-03,\n",
            "         6.0830e-05, -3.8924e-05])), ('transformer_encoder.layers.8.self_attn.out_proj.weight', tensor([[-0.0013, -0.0052, -0.0002,  ...,  0.0208, -0.0157,  0.0453],\n",
            "        [ 0.0262,  0.0278,  0.0155,  ...,  0.0228, -0.0033,  0.0015],\n",
            "        [-0.0255,  0.0060, -0.0017,  ...,  0.0061,  0.0127, -0.0282],\n",
            "        ...,\n",
            "        [ 0.0199,  0.0027, -0.0131,  ...,  0.0325,  0.0064,  0.0055],\n",
            "        [ 0.0089,  0.0224, -0.0122,  ...,  0.0031,  0.0036, -0.0097],\n",
            "        [ 0.0080, -0.0170,  0.0370,  ...,  0.0040,  0.0125, -0.0025]])), ('transformer_encoder.layers.8.self_attn.out_proj.bias', tensor([ 0.0087,  0.0116, -0.0013,  ...,  0.0063, -0.0132,  0.0004])), ('transformer_encoder.layers.8.linear1.weight', tensor([[-0.0323,  0.0219,  0.0151,  ...,  0.0160,  0.0193, -0.0471],\n",
            "        [-0.0078, -0.0292,  0.0033,  ..., -0.0111,  0.0067,  0.0139],\n",
            "        [ 0.0034,  0.0121, -0.0413,  ..., -0.0343,  0.0017,  0.0442],\n",
            "        ...,\n",
            "        [ 0.0098,  0.0061, -0.0244,  ...,  0.0157,  0.0036,  0.0019],\n",
            "        [-0.0039, -0.0035,  0.0117,  ..., -0.0071, -0.0267, -0.0059],\n",
            "        [-0.0340, -0.0025,  0.0472,  ...,  0.0144, -0.0049, -0.0458]])), ('transformer_encoder.layers.8.linear1.bias', tensor([ 0.0121, -0.0134, -0.0349,  ..., -0.0047, -0.0256, -0.0059])), ('transformer_encoder.layers.8.linear2.weight', tensor([[-5.4389e-02,  3.0310e-03, -7.8855e-03,  ..., -5.8498e-03,\n",
            "         -1.2232e-03, -2.7364e-02],\n",
            "        [ 1.1292e-02, -1.1201e-02,  1.3171e-02,  ...,  5.1814e-03,\n",
            "          8.1833e-03,  1.2372e-02],\n",
            "        [-7.4916e-05,  1.4571e-02,  1.5902e-02,  ..., -4.6181e-03,\n",
            "          8.4944e-03,  8.6847e-03],\n",
            "        ...,\n",
            "        [ 1.1232e-02,  4.3570e-04,  5.6264e-03,  ..., -3.5698e-03,\n",
            "         -4.1353e-04, -2.2563e-02],\n",
            "        [ 3.0170e-02,  4.2124e-03,  2.1735e-02,  ..., -1.1882e-02,\n",
            "         -2.8098e-02,  1.8496e-02],\n",
            "        [-3.0059e-03, -2.0087e-02, -3.7501e-02,  ...,  4.9697e-03,\n",
            "          3.8829e-03,  8.1021e-03]])), ('transformer_encoder.layers.8.linear2.bias', tensor([-0.0283, -0.0225, -0.0051,  ..., -0.0031, -0.0042,  0.0049])), ('transformer_encoder.layers.8.norm1.weight', tensor([0.9754, 0.9730, 0.9576,  ..., 0.9830, 0.9601, 0.9730])), ('transformer_encoder.layers.8.norm1.bias', tensor([ 0.0083,  0.0100, -0.0032,  ...,  0.0113, -0.0137, -0.0006])), ('transformer_encoder.layers.8.norm2.weight', tensor([0.9634, 0.9748, 0.9821,  ..., 0.9947, 0.9790, 0.9816])), ('transformer_encoder.layers.8.norm2.bias', tensor([-0.0293, -0.0166,  0.0041,  ..., -0.0139,  0.0063,  0.0118])), ('transformer_encoder.layers.9.self_attn.in_proj_weight', tensor([[ 0.0116, -0.0351,  0.0252,  ..., -0.0190,  0.0155,  0.0185],\n",
            "        [ 0.0129, -0.0447,  0.0027,  ...,  0.0219, -0.0092, -0.0083],\n",
            "        [ 0.0020, -0.0076,  0.0173,  ..., -0.0068, -0.0298,  0.0279],\n",
            "        ...,\n",
            "        [ 0.0032,  0.0178, -0.0262,  ..., -0.0006,  0.0268,  0.0264],\n",
            "        [ 0.0005, -0.0031, -0.0287,  ...,  0.0042, -0.0257, -0.0046],\n",
            "        [-0.0174, -0.0008, -0.0152,  ...,  0.0030,  0.0084, -0.0232]])), ('transformer_encoder.layers.9.self_attn.in_proj_bias', tensor([-0.0006,  0.0007, -0.0045,  ..., -0.0023,  0.0017, -0.0007])), ('transformer_encoder.layers.9.self_attn.out_proj.weight', tensor([[ 0.0030, -0.0186,  0.0070,  ..., -0.0015, -0.0276,  0.0464],\n",
            "        [ 0.0072,  0.0068,  0.0095,  ...,  0.0032, -0.0104,  0.0232],\n",
            "        [-0.0382, -0.0063, -0.0081,  ...,  0.0224,  0.0299, -0.0128],\n",
            "        ...,\n",
            "        [ 0.0250,  0.0195, -0.0133,  ...,  0.0231,  0.0332, -0.0091],\n",
            "        [ 0.0063,  0.0166, -0.0138,  ..., -0.0102,  0.0164,  0.0008],\n",
            "        [-0.0009, -0.0148,  0.0185,  ...,  0.0220,  0.0120, -0.0019]])), ('transformer_encoder.layers.9.self_attn.out_proj.bias', tensor([ 0.0015,  0.0137, -0.0034,  ...,  0.0080, -0.0147, -0.0030])), ('transformer_encoder.layers.9.linear1.weight', tensor([[-1.6998e-02,  3.0738e-02,  2.1106e-02,  ...,  4.1924e-03,\n",
            "         -7.3736e-03, -1.9685e-02],\n",
            "        [-2.0963e-03, -1.4468e-02,  5.1785e-03,  ...,  5.2871e-02,\n",
            "         -1.0230e-02,  4.3301e-02],\n",
            "        [ 9.8849e-03,  9.1948e-03, -3.2452e-02,  ..., -3.0150e-02,\n",
            "          1.9644e-02,  2.2705e-02],\n",
            "        ...,\n",
            "        [ 7.5787e-03,  1.0689e-05, -3.7999e-03,  ...,  3.4963e-02,\n",
            "          4.5213e-03,  2.3935e-02],\n",
            "        [ 1.6831e-02, -1.8398e-02,  3.0597e-02,  ..., -2.1592e-04,\n",
            "         -7.3928e-03,  2.4646e-02],\n",
            "        [-5.0076e-02,  2.8444e-02,  3.9287e-02,  ...,  6.8079e-02,\n",
            "          1.8290e-02, -4.0930e-02]])), ('transformer_encoder.layers.9.linear1.bias', tensor([ 0.0123, -0.0182, -0.0348,  ..., -0.0068, -0.0291, -0.0071])), ('transformer_encoder.layers.9.linear2.weight', tensor([[-0.0333, -0.0239,  0.0080,  ..., -0.0037,  0.0067, -0.0239],\n",
            "        [-0.0183,  0.0080,  0.0054,  ...,  0.0197,  0.0050,  0.0373],\n",
            "        [ 0.0138,  0.0353, -0.0008,  ...,  0.0324,  0.0160,  0.0525],\n",
            "        ...,\n",
            "        [-0.0416,  0.0383, -0.0081,  ...,  0.0448, -0.0016, -0.0186],\n",
            "        [ 0.0207, -0.0201, -0.0035,  ..., -0.0166,  0.0107,  0.0104],\n",
            "        [ 0.0122,  0.0087, -0.0079,  ...,  0.0066,  0.0174, -0.0084]])), ('transformer_encoder.layers.9.linear2.bias', tensor([-0.0148, -0.0230, -0.0026,  ..., -0.0018, -0.0049,  0.0046])), ('transformer_encoder.layers.9.norm1.weight', tensor([0.9577, 0.9633, 0.9646,  ..., 0.9906, 0.9622, 0.9652])), ('transformer_encoder.layers.9.norm1.bias', tensor([ 0.0003,  0.0155, -0.0046,  ...,  0.0137, -0.0158, -0.0060])), ('transformer_encoder.layers.9.norm2.weight', tensor([0.9654, 0.9540, 0.9712,  ..., 0.9975, 0.9731, 0.9690])), ('transformer_encoder.layers.9.norm2.bias', tensor([-0.0152, -0.0217,  0.0101,  ..., -0.0118,  0.0071,  0.0121])), ('transformer_encoder.layers.10.self_attn.in_proj_weight', tensor([[ 0.0108, -0.0268,  0.0336,  ...,  0.0002,  0.0245,  0.0213],\n",
            "        [ 0.0360, -0.0571,  0.0054,  ...,  0.0276, -0.0267,  0.0059],\n",
            "        [-0.0025,  0.0208, -0.0009,  ..., -0.0073, -0.0177,  0.0268],\n",
            "        ...,\n",
            "        [ 0.0268,  0.0254, -0.0257,  ..., -0.0003,  0.0361,  0.0237],\n",
            "        [ 0.0057, -0.0102, -0.0312,  ...,  0.0248, -0.0237, -0.0024],\n",
            "        [-0.0351,  0.0109, -0.0026,  ...,  0.0208,  0.0009, -0.0192]])), ('transformer_encoder.layers.10.self_attn.in_proj_bias', tensor([-0.0015, -0.0008, -0.0045,  ..., -0.0046,  0.0020,  0.0002])), ('transformer_encoder.layers.10.self_attn.out_proj.weight', tensor([[ 0.0035, -0.0039,  0.0085,  ...,  0.0047, -0.0219,  0.0237],\n",
            "        [ 0.0129,  0.0153,  0.0163,  ..., -0.0046,  0.0165,  0.0226],\n",
            "        [-0.0242, -0.0080, -0.0225,  ...,  0.0091,  0.0165, -0.0082],\n",
            "        ...,\n",
            "        [ 0.0246,  0.0170, -0.0167,  ...,  0.0169,  0.0288,  0.0020],\n",
            "        [-0.0073,  0.0185, -0.0102,  ..., -0.0193,  0.0005, -0.0197],\n",
            "        [ 0.0140, -0.0244,  0.0272,  ..., -0.0153,  0.0213,  0.0083]])), ('transformer_encoder.layers.10.self_attn.out_proj.bias', tensor([-0.0015,  0.0120, -0.0037,  ...,  0.0149, -0.0157, -0.0081])), ('transformer_encoder.layers.10.linear1.weight', tensor([[ 1.1013e-02,  3.6098e-02,  6.3797e-03,  ...,  2.6506e-02,\n",
            "          1.4243e-02, -3.4093e-02],\n",
            "        [ 6.7023e-05, -3.0312e-02,  8.7822e-03,  ..., -8.5143e-03,\n",
            "          6.8996e-03,  1.7640e-02],\n",
            "        [-4.6567e-02,  1.8251e-02, -3.3092e-03,  ..., -6.4751e-02,\n",
            "          3.6615e-03,  3.2167e-02],\n",
            "        ...,\n",
            "        [ 1.4561e-02, -2.0369e-03,  4.4060e-03,  ...,  5.1075e-02,\n",
            "          9.7109e-03, -5.1574e-02],\n",
            "        [ 1.3453e-02, -2.3614e-02,  2.4785e-02,  ...,  1.3181e-02,\n",
            "         -1.7276e-02,  2.7380e-02],\n",
            "        [-2.1045e-02, -2.5394e-02,  2.0805e-02,  ...,  4.0382e-02,\n",
            "          6.8122e-03, -2.8500e-02]])), ('transformer_encoder.layers.10.linear1.bias', tensor([ 0.0104, -0.0131, -0.0368,  ..., -0.0009, -0.0212,  0.0057])), ('transformer_encoder.layers.10.linear2.weight', tensor([[-0.0142, -0.0030, -0.0223,  ...,  0.0072,  0.0080,  0.0102],\n",
            "        [-0.0279, -0.0106, -0.0438,  ...,  0.0278,  0.0135,  0.0192],\n",
            "        [ 0.0085,  0.0214,  0.0001,  ..., -0.0172,  0.0196,  0.0392],\n",
            "        ...,\n",
            "        [-0.0413,  0.0018, -0.0103,  ..., -0.0012,  0.0082, -0.0365],\n",
            "        [ 0.0276,  0.0090, -0.0378,  ..., -0.0127,  0.0005,  0.0048],\n",
            "        [-0.0043, -0.0171, -0.0104,  ..., -0.0152,  0.0146,  0.0109]])), ('transformer_encoder.layers.10.linear2.bias', tensor([-0.0086, -0.0180, -0.0030,  ..., -0.0048, -0.0032,  0.0011])), ('transformer_encoder.layers.10.norm1.weight', tensor([0.9577, 0.9439, 0.9624,  ..., 0.9970, 0.9638, 0.9643])), ('transformer_encoder.layers.10.norm1.bias', tensor([-0.0015,  0.0084, -0.0053,  ...,  0.0174, -0.0179, -0.0109])), ('transformer_encoder.layers.10.norm2.weight', tensor([0.9642, 0.9416, 0.9672,  ..., 0.9983, 0.9656, 0.9723])), ('transformer_encoder.layers.10.norm2.bias', tensor([-0.0120, -0.0148,  0.0113,  ..., -0.0140,  0.0077,  0.0080])), ('transformer_encoder.layers.11.self_attn.in_proj_weight', tensor([[ 0.0077, -0.0437,  0.0309,  ..., -0.0203,  0.0144,  0.0066],\n",
            "        [ 0.0500, -0.0602, -0.0077,  ...,  0.0241, -0.0167,  0.0111],\n",
            "        [-0.0016,  0.0121,  0.0078,  ..., -0.0123, -0.0060,  0.0350],\n",
            "        ...,\n",
            "        [ 0.0176,  0.0267, -0.0425,  ..., -0.0062,  0.0288,  0.0249],\n",
            "        [ 0.0064,  0.0043, -0.0315,  ...,  0.0069, -0.0261, -0.0170],\n",
            "        [-0.0245, -0.0147, -0.0063,  ..., -0.0056, -0.0044, -0.0306]])), ('transformer_encoder.layers.11.self_attn.in_proj_bias', tensor([ 0.0020, -0.0019, -0.0052,  ..., -0.0026,  0.0045, -0.0011])), ('transformer_encoder.layers.11.self_attn.out_proj.weight', tensor([[ 0.0107, -0.0040,  0.0061,  ...,  0.0006, -0.0235,  0.0369],\n",
            "        [ 0.0163,  0.0060,  0.0082,  ...,  0.0012,  0.0058,  0.0166],\n",
            "        [-0.0182, -0.0026, -0.0228,  ...,  0.0182,  0.0026, -0.0119],\n",
            "        ...,\n",
            "        [ 0.0183,  0.0127,  0.0099,  ...,  0.0140,  0.0284, -0.0045],\n",
            "        [ 0.0013,  0.0148, -0.0199,  ..., -0.0190,  0.0196, -0.0097],\n",
            "        [ 0.0184, -0.0287,  0.0358,  ..., -0.0064,  0.0147, -0.0032]])), ('transformer_encoder.layers.11.self_attn.out_proj.bias', tensor([-0.0065,  0.0042, -0.0040,  ...,  0.0110, -0.0183, -0.0125])), ('transformer_encoder.layers.11.linear1.weight', tensor([[ 0.0050,  0.0226,  0.0282,  ...,  0.0224,  0.0131, -0.0179],\n",
            "        [-0.0057, -0.0104,  0.0215,  ...,  0.0019,  0.0121,  0.0421],\n",
            "        [ 0.0194,  0.0173, -0.0323,  ..., -0.0219,  0.0195,  0.0189],\n",
            "        ...,\n",
            "        [-0.0098, -0.0125, -0.0190,  ..., -0.0064,  0.0005, -0.0219],\n",
            "        [ 0.0007,  0.0012,  0.0115,  ...,  0.0547, -0.0085,  0.0118],\n",
            "        [-0.0210, -0.0117,  0.0179,  ...,  0.0338,  0.0121, -0.0364]])), ('transformer_encoder.layers.11.linear1.bias', tensor([ 0.0145, -0.0146, -0.0308,  ..., -0.0028, -0.0262, -0.0099])), ('transformer_encoder.layers.11.linear2.weight', tensor([[-2.6573e-02, -2.2497e-02,  6.6972e-03,  ..., -2.9873e-02,\n",
            "         -1.1933e-02,  8.7824e-03],\n",
            "        [-8.0280e-03, -1.1823e-02,  1.5785e-02,  ...,  2.7665e-02,\n",
            "          2.6233e-02,  1.8848e-02],\n",
            "        [-4.7956e-03,  1.9454e-02,  2.6811e-03,  ...,  4.1795e-03,\n",
            "         -4.3896e-03,  7.5626e-03],\n",
            "        ...,\n",
            "        [-5.7410e-03, -4.8949e-03, -1.0771e-02,  ..., -6.2235e-04,\n",
            "         -7.8959e-03, -1.0727e-02],\n",
            "        [ 3.0871e-02, -6.1347e-03,  8.5582e-05,  ..., -3.5014e-02,\n",
            "          9.5140e-03,  1.6510e-02],\n",
            "        [-2.2516e-02, -1.7400e-02, -1.2182e-02,  ...,  1.1952e-02,\n",
            "         -1.3808e-02,  1.5317e-02]])), ('transformer_encoder.layers.11.linear2.bias', tensor([ 0.0020, -0.0119, -0.0030,  ..., -0.0048, -0.0027,  0.0005])), ('transformer_encoder.layers.11.norm1.weight', tensor([0.9505, 0.9254, 0.9719,  ..., 1.0000, 0.9640, 0.9624])), ('transformer_encoder.layers.11.norm1.bias', tensor([-0.0062,  0.0024, -0.0093,  ...,  0.0149, -0.0205, -0.0136])), ('transformer_encoder.layers.11.norm2.weight', tensor([0.9614, 0.9328, 0.9776,  ..., 0.9966, 0.9614, 0.9631])), ('transformer_encoder.layers.11.norm2.bias', tensor([ 0.0006, -0.0070,  0.0098,  ..., -0.0145,  0.0096,  0.0077])), ('transformer_encoder.layers.12.self_attn.in_proj_weight', tensor([[-0.0102, -0.0696,  0.0205,  ..., -0.0106,  0.0145,  0.0164],\n",
            "        [ 0.0289, -0.0414,  0.0054,  ...,  0.0505, -0.0465,  0.0213],\n",
            "        [-0.0414,  0.0357,  0.0091,  ..., -0.0063, -0.0069,  0.0481],\n",
            "        ...,\n",
            "        [ 0.0251,  0.0313, -0.0189,  ..., -0.0146,  0.0309,  0.0334],\n",
            "        [ 0.0060, -0.0189, -0.0355,  ...,  0.0079, -0.0350, -0.0090],\n",
            "        [-0.0292,  0.0043,  0.0005,  ...,  0.0134,  0.0050, -0.0312]])), ('transformer_encoder.layers.12.self_attn.in_proj_bias', tensor([-0.0019, -0.0024, -0.0009,  ..., -0.0013,  0.0035, -0.0032])), ('transformer_encoder.layers.12.self_attn.out_proj.weight', tensor([[ 0.0206, -0.0078,  0.0037,  ...,  0.0107, -0.0259,  0.0348],\n",
            "        [ 0.0186,  0.0007,  0.0014,  ..., -0.0052,  0.0213,  0.0174],\n",
            "        [-0.0180, -0.0133, -0.0202,  ...,  0.0227,  0.0226, -0.0268],\n",
            "        ...,\n",
            "        [ 0.0142,  0.0162,  0.0026,  ...,  0.0207,  0.0204, -0.0235],\n",
            "        [-0.0002,  0.0140, -0.0139,  ..., -0.0225,  0.0148, -0.0232],\n",
            "        [ 0.0041, -0.0400,  0.0293,  ..., -0.0115,  0.0168,  0.0009]])), ('transformer_encoder.layers.12.self_attn.out_proj.bias', tensor([-0.0110,  0.0041, -0.0126,  ...,  0.0076, -0.0234, -0.0128])), ('transformer_encoder.layers.12.linear1.weight', tensor([[ 2.0859e-02,  1.4920e-02,  3.8355e-02,  ...,  1.8390e-02,\n",
            "          7.3160e-03, -8.9056e-03],\n",
            "        [-5.2325e-03, -2.5456e-02,  1.5443e-02,  ..., -5.6071e-03,\n",
            "          6.3197e-03,  8.6329e-03],\n",
            "        [ 1.9917e-02,  1.3215e-02, -2.8986e-02,  ..., -2.8428e-02,\n",
            "          2.1238e-02,  1.7206e-02],\n",
            "        ...,\n",
            "        [-4.5992e-02, -2.2518e-02, -2.9410e-02,  ...,  2.4514e-02,\n",
            "         -1.6527e-02, -2.2419e-02],\n",
            "        [ 5.6995e-02, -3.4709e-02,  1.1390e-02,  ..., -1.6732e-02,\n",
            "         -1.7711e-02,  2.2727e-03],\n",
            "        [-3.0051e-02, -1.6041e-02,  2.4349e-02,  ...,  2.6784e-02,\n",
            "         -6.6958e-05, -2.2504e-02]])), ('transformer_encoder.layers.12.linear1.bias', tensor([ 0.0097, -0.0126, -0.0341,  ..., -0.0047, -0.0255, -0.0088])), ('transformer_encoder.layers.12.linear2.weight', tensor([[-0.0301, -0.0029,  0.0057,  ..., -0.0165,  0.0260,  0.0069],\n",
            "        [-0.0021, -0.0132,  0.0094,  ...,  0.0011, -0.0112,  0.0214],\n",
            "        [-0.0031,  0.0185, -0.0006,  ..., -0.0148,  0.0017,  0.0103],\n",
            "        ...,\n",
            "        [-0.0297,  0.0086, -0.0107,  ..., -0.0044,  0.0233, -0.0087],\n",
            "        [ 0.0330,  0.0170, -0.0046,  ..., -0.0726,  0.0193,  0.0076],\n",
            "        [-0.0081, -0.0154, -0.0073,  ..., -0.0166,  0.0039,  0.0049]])), ('transformer_encoder.layers.12.linear2.bias', tensor([ 0.0093, -0.0048, -0.0010,  ...,  0.0029,  0.0007,  0.0043])), ('transformer_encoder.layers.12.norm1.weight', tensor([0.9480, 0.9221, 0.9777,  ..., 0.9955, 0.9618, 0.9574])), ('transformer_encoder.layers.12.norm1.bias', tensor([-0.0131,  0.0026, -0.0149,  ...,  0.0110, -0.0248, -0.0173])), ('transformer_encoder.layers.12.norm2.weight', tensor([0.9580, 0.9247, 0.9669,  ..., 1.0005, 0.9612, 0.9539])), ('transformer_encoder.layers.12.norm2.bias', tensor([ 0.0084, -0.0025,  0.0123,  ..., -0.0106,  0.0127,  0.0133])), ('transformer_encoder.layers.13.self_attn.in_proj_weight', tensor([[ 0.0142, -0.0452,  0.0137,  ...,  0.0153,  0.0168,  0.0336],\n",
            "        [ 0.0494, -0.0450,  0.0122,  ...,  0.0405, -0.0386,  0.0283],\n",
            "        [-0.0530,  0.0190,  0.0178,  ..., -0.0151, -0.0117,  0.0283],\n",
            "        ...,\n",
            "        [ 0.0177,  0.0400, -0.0212,  ..., -0.0084,  0.0404,  0.0388],\n",
            "        [-0.0067, -0.0016, -0.0257,  ..., -0.0013, -0.0329, -0.0157],\n",
            "        [-0.0232, -0.0212, -0.0082,  ...,  0.0095, -0.0136, -0.0240]])), ('transformer_encoder.layers.13.self_attn.in_proj_bias', tensor([-0.0042, -0.0018,  0.0002,  ..., -0.0025,  0.0043, -0.0002])), ('transformer_encoder.layers.13.self_attn.out_proj.weight', tensor([[ 0.0109, -0.0098,  0.0088,  ..., -0.0102, -0.0314,  0.0466],\n",
            "        [ 0.0288,  0.0043,  0.0155,  ...,  0.0039,  0.0205,  0.0200],\n",
            "        [-0.0344,  0.0127, -0.0124,  ...,  0.0197,  0.0167, -0.0272],\n",
            "        ...,\n",
            "        [ 0.0286, -0.0104, -0.0095,  ...,  0.0143,  0.0226, -0.0012],\n",
            "        [-0.0183,  0.0363, -0.0289,  ..., -0.0060,  0.0280, -0.0129],\n",
            "        [ 0.0071, -0.0321,  0.0440,  ...,  0.0012,  0.0276,  0.0018]])), ('transformer_encoder.layers.13.self_attn.out_proj.bias', tensor([-0.0174, -0.0006, -0.0087,  ...,  0.0095, -0.0185, -0.0133])), ('transformer_encoder.layers.13.linear1.weight', tensor([[ 0.0131,  0.0775,  0.0643,  ...,  0.0011,  0.0186,  0.0073],\n",
            "        [-0.0006, -0.0166,  0.0219,  ..., -0.0120,  0.0074,  0.0145],\n",
            "        [ 0.0327,  0.0191, -0.0418,  ..., -0.0319,  0.0171,  0.0213],\n",
            "        ...,\n",
            "        [-0.0010,  0.0186, -0.0255,  ...,  0.0236, -0.0029, -0.0043],\n",
            "        [ 0.0288, -0.0375,  0.0349,  ..., -0.0003, -0.0297,  0.0149],\n",
            "        [-0.0184,  0.0188,  0.0330,  ...,  0.0209,  0.0336, -0.0245]])), ('transformer_encoder.layers.13.linear1.bias', tensor([ 0.0139, -0.0131, -0.0319,  ..., -0.0031, -0.0204, -0.0036])), ('transformer_encoder.layers.13.linear2.weight', tensor([[-1.0303e-02, -2.3557e-03,  9.5436e-03,  ...,  2.0313e-03,\n",
            "          4.1543e-02, -5.5910e-03],\n",
            "        [-1.8859e-03, -1.8635e-02,  2.2407e-02,  ...,  1.4628e-02,\n",
            "         -1.8951e-02,  1.7578e-02],\n",
            "        [-2.7040e-03,  2.2710e-02, -1.3478e-03,  ..., -7.5943e-03,\n",
            "          4.4279e-02, -2.1104e-02],\n",
            "        ...,\n",
            "        [-1.9099e-02, -2.2446e-03,  7.1742e-03,  ...,  2.3040e-03,\n",
            "          4.2585e-03,  1.0786e-02],\n",
            "        [ 1.1459e-02,  2.2008e-02, -6.6704e-05,  ..., -1.3812e-02,\n",
            "         -2.3770e-02,  2.0712e-02],\n",
            "        [ 3.5449e-03, -1.1019e-02, -1.3719e-03,  ..., -3.2760e-03,\n",
            "         -4.6596e-02,  7.4600e-03]])), ('transformer_encoder.layers.13.linear2.bias', tensor([0.0126, 0.0028, 0.0001,  ..., 0.0051, 0.0029, 0.0129])), ('transformer_encoder.layers.13.norm1.weight', tensor([0.9392, 0.9154, 0.9631,  ..., 1.0007, 0.9624, 0.9551])), ('transformer_encoder.layers.13.norm1.bias', tensor([-0.0170, -0.0015, -0.0114,  ...,  0.0119, -0.0213, -0.0160])), ('transformer_encoder.layers.13.norm2.weight', tensor([0.9433, 0.9229, 0.9684,  ..., 1.0054, 0.9614, 0.9446])), ('transformer_encoder.layers.13.norm2.bias', tensor([ 0.0123,  0.0045,  0.0109,  ..., -0.0068,  0.0158,  0.0189])), ('transformer_encoder.layers.14.self_attn.in_proj_weight', tensor([[-0.0127, -0.0538,  0.0198,  ...,  0.0038,  0.0035,  0.0343],\n",
            "        [ 0.0481, -0.0485,  0.0071,  ...,  0.0393, -0.0211,  0.0011],\n",
            "        [-0.0173,  0.0334,  0.0116,  ..., -0.0304, -0.0011,  0.0155],\n",
            "        ...,\n",
            "        [ 0.0101,  0.0294, -0.0368,  ...,  0.0071,  0.0292,  0.0317],\n",
            "        [-0.0002, -0.0094, -0.0265,  ...,  0.0060, -0.0310, -0.0166],\n",
            "        [-0.0141, -0.0228,  0.0087,  ...,  0.0190, -0.0203, -0.0373]])), ('transformer_encoder.layers.14.self_attn.in_proj_bias', tensor([ 0.0055, -0.0026, -0.0051,  ..., -0.0021,  0.0043, -0.0012])), ('transformer_encoder.layers.14.self_attn.out_proj.weight', tensor([[ 0.0009, -0.0126,  0.0253,  ...,  0.0053, -0.0321,  0.0236],\n",
            "        [ 0.0146,  0.0145,  0.0210,  ..., -0.0135,  0.0065,  0.0238],\n",
            "        [-0.0248, -0.0027,  0.0042,  ...,  0.0276,  0.0215, -0.0222],\n",
            "        ...,\n",
            "        [ 0.0233, -0.0210, -0.0140,  ..., -0.0018,  0.0329, -0.0051],\n",
            "        [-0.0103,  0.0312, -0.0281,  ..., -0.0108,  0.0312, -0.0128],\n",
            "        [ 0.0045, -0.0199,  0.0299,  ...,  0.0056,  0.0123, -0.0107]])), ('transformer_encoder.layers.14.self_attn.out_proj.bias', tensor([-0.0190,  0.0028, -0.0063,  ...,  0.0079, -0.0181, -0.0088])), ('transformer_encoder.layers.14.linear1.weight', tensor([[ 0.0049,  0.0718,  0.0568,  ...,  0.0298,  0.0533,  0.0423],\n",
            "        [ 0.0146, -0.0354,  0.0691,  ...,  0.0232,  0.0050,  0.0180],\n",
            "        [ 0.0341,  0.0209, -0.0497,  ..., -0.0387,  0.0176,  0.0348],\n",
            "        ...,\n",
            "        [ 0.0550,  0.0053, -0.0228,  ...,  0.0230,  0.0142,  0.0208],\n",
            "        [ 0.0089, -0.0354,  0.0038,  ..., -0.0377, -0.0170,  0.0294],\n",
            "        [-0.0371, -0.0216,  0.0346,  ..., -0.0036, -0.0123, -0.0174]])), ('transformer_encoder.layers.14.linear1.bias', tensor([ 0.0088, -0.0080, -0.0327,  ..., -0.0005, -0.0170, -0.0125])), ('transformer_encoder.layers.14.linear2.weight', tensor([[-0.0192,  0.0118, -0.0175,  ..., -0.0215,  0.0219, -0.0127],\n",
            "        [-0.0085, -0.0361,  0.0397,  ...,  0.0251, -0.0041, -0.0009],\n",
            "        [ 0.0220,  0.0453, -0.0204,  ...,  0.0076,  0.0207,  0.0162],\n",
            "        ...,\n",
            "        [-0.0372,  0.0318,  0.0078,  ..., -0.0066,  0.0290, -0.0141],\n",
            "        [ 0.0197,  0.0174, -0.0120,  ..., -0.0245, -0.0058,  0.0006],\n",
            "        [ 0.0052, -0.0084,  0.0147,  ...,  0.0105, -0.0077, -0.0126]])), ('transformer_encoder.layers.14.linear2.bias', tensor([ 0.0157,  0.0129, -0.0080,  ...,  0.0053,  0.0039,  0.0134])), ('transformer_encoder.layers.14.norm1.weight', tensor([0.9246, 0.9065, 0.9653,  ..., 1.0024, 0.9677, 0.9394])), ('transformer_encoder.layers.14.norm1.bias', tensor([-0.0182,  0.0019, -0.0112,  ...,  0.0132, -0.0204, -0.0085])), ('transformer_encoder.layers.14.norm2.weight', tensor([0.9331, 0.9139, 0.9815,  ..., 1.0115, 0.9509, 0.9394])), ('transformer_encoder.layers.14.norm2.bias', tensor([ 0.0159,  0.0168,  0.0060,  ..., -0.0054,  0.0177,  0.0190])), ('transformer_encoder.layers.15.self_attn.in_proj_weight', tensor([[ 0.0153, -0.0424,  0.0109,  ...,  0.0059,  0.0154,  0.0293],\n",
            "        [ 0.0189, -0.0541, -0.0041,  ...,  0.0303, -0.0232,  0.0056],\n",
            "        [-0.0317,  0.0451,  0.0426,  ..., -0.0152, -0.0087,  0.0319],\n",
            "        ...,\n",
            "        [ 0.0091,  0.0241, -0.0362,  ..., -0.0096,  0.0256,  0.0244],\n",
            "        [ 0.0191, -0.0176, -0.0184,  ...,  0.0083, -0.0334, -0.0087],\n",
            "        [-0.0134, -0.0242,  0.0131,  ...,  0.0155, -0.0151, -0.0131]])), ('transformer_encoder.layers.15.self_attn.in_proj_bias', tensor([-0.0010, -0.0007, -0.0039,  ..., -0.0011,  0.0030,  0.0006])), ('transformer_encoder.layers.15.self_attn.out_proj.weight', tensor([[ 1.9649e-02, -1.4401e-02,  1.5726e-02,  ...,  2.4860e-03,\n",
            "         -2.1384e-02,  4.0444e-02],\n",
            "        [ 1.3782e-02,  1.5323e-02, -1.7044e-03,  ...,  2.2015e-05,\n",
            "         -1.3770e-02,  3.1399e-02],\n",
            "        [-2.2831e-02,  1.3047e-02,  1.7453e-03,  ...,  4.0376e-02,\n",
            "          2.2192e-02, -1.7417e-02],\n",
            "        ...,\n",
            "        [ 2.7406e-02,  4.2722e-03,  8.7669e-03,  ...,  1.8153e-02,\n",
            "          1.2753e-02, -9.5664e-03],\n",
            "        [-4.2979e-03,  8.1009e-03, -3.7835e-02,  ..., -1.4744e-02,\n",
            "          3.8439e-02, -1.4183e-02],\n",
            "        [ 2.1877e-02, -1.5789e-02,  1.7659e-02,  ...,  1.5425e-03,\n",
            "          3.3112e-02, -1.6477e-02]])), ('transformer_encoder.layers.15.self_attn.out_proj.bias', tensor([-0.0207,  0.0220, -0.0126,  ...,  0.0080, -0.0128, -0.0106])), ('transformer_encoder.layers.15.linear1.weight', tensor([[-0.0074,  0.0418,  0.0301,  ...,  0.0281,  0.0163, -0.0005],\n",
            "        [ 0.0100, -0.0249,  0.0455,  ...,  0.0153, -0.0016,  0.0308],\n",
            "        [ 0.0147,  0.0131, -0.0269,  ..., -0.0256,  0.0212,  0.0215],\n",
            "        ...,\n",
            "        [ 0.0155,  0.0203, -0.0082,  ..., -0.0045, -0.0022, -0.0159],\n",
            "        [ 0.0398, -0.0510, -0.0016,  ..., -0.0403, -0.0353,  0.0469],\n",
            "        [-0.0214, -0.0194,  0.0222,  ...,  0.0152,  0.0066, -0.0260]])), ('transformer_encoder.layers.15.linear1.bias', tensor([ 0.0132, -0.0056, -0.0351,  ..., -0.0045, -0.0180, -0.0124])), ('transformer_encoder.layers.15.linear2.weight', tensor([[-3.6828e-02,  2.6630e-02,  2.6592e-03,  ..., -1.9871e-02,\n",
            "          1.3672e-02,  4.4102e-03],\n",
            "        [-3.2068e-02, -8.3235e-03,  9.1598e-03,  ...,  4.4500e-02,\n",
            "         -2.1238e-03,  1.6991e-02],\n",
            "        [-3.4210e-03,  2.7882e-02, -2.3269e-03,  ..., -5.9762e-03,\n",
            "          5.5787e-04,  8.7779e-03],\n",
            "        ...,\n",
            "        [ 2.6584e-02,  1.4002e-02, -6.5491e-03,  ..., -2.2116e-03,\n",
            "         -4.2060e-04, -5.7375e-03],\n",
            "        [ 1.0931e-03, -2.5418e-04, -2.0144e-03,  ..., -3.2526e-02,\n",
            "         -2.3736e-05,  4.5479e-03],\n",
            "        [ 1.5653e-02, -1.7087e-02,  4.6680e-04,  ...,  3.8962e-02,\n",
            "         -8.2120e-03,  3.5324e-03]])), ('transformer_encoder.layers.15.linear2.bias', tensor([ 0.0181,  0.0073, -0.0031,  ...,  0.0041, -0.0009,  0.0118])), ('transformer_encoder.layers.15.norm1.weight', tensor([0.9186, 0.9107, 0.9762,  ..., 1.0041, 0.9608, 0.9279])), ('transformer_encoder.layers.15.norm1.bias', tensor([-0.0208,  0.0243, -0.0182,  ...,  0.0135, -0.0173, -0.0096])), ('transformer_encoder.layers.15.norm2.weight', tensor([0.9190, 0.9113, 0.9886,  ..., 1.0122, 0.9550, 0.9403])), ('transformer_encoder.layers.15.norm2.bias', tensor([ 0.0203,  0.0110,  0.0083,  ..., -0.0089,  0.0138,  0.0169])), ('transformer_encoder.layers.16.self_attn.in_proj_weight', tensor([[-0.0030, -0.0411,  0.0232,  ...,  0.0018,  0.0292,  0.0590],\n",
            "        [ 0.0082, -0.0500,  0.0083,  ...,  0.0290, -0.0080,  0.0154],\n",
            "        [-0.0330,  0.0311,  0.0442,  ..., -0.0222, -0.0078,  0.0090],\n",
            "        ...,\n",
            "        [ 0.0169,  0.0203, -0.0369,  ..., -0.0187,  0.0295,  0.0349],\n",
            "        [ 0.0123,  0.0037, -0.0192,  ..., -0.0031, -0.0290, -0.0112],\n",
            "        [-0.0057, -0.0210,  0.0024,  ...,  0.0120, -0.0042, -0.0104]])), ('transformer_encoder.layers.16.self_attn.in_proj_bias', tensor([-0.0028, -0.0009,  0.0010,  ..., -0.0013,  0.0037,  0.0007])), ('transformer_encoder.layers.16.self_attn.out_proj.weight', tensor([[ 1.0251e-02, -2.8010e-03,  1.3407e-02,  ...,  1.3056e-02,\n",
            "         -1.4501e-02,  3.8742e-02],\n",
            "        [ 7.2242e-03,  1.9960e-02,  3.5510e-03,  ...,  1.7321e-03,\n",
            "          3.1349e-03,  1.2476e-02],\n",
            "        [-2.3724e-02,  1.1098e-02,  1.3577e-02,  ...,  3.3675e-02,\n",
            "          1.0715e-02, -1.3748e-02],\n",
            "        ...,\n",
            "        [ 2.4676e-02, -6.8371e-03,  5.8006e-03,  ...,  2.8538e-03,\n",
            "          2.2170e-02, -5.9633e-04],\n",
            "        [ 4.2923e-05,  1.2052e-02, -2.0666e-02,  ..., -1.1656e-02,\n",
            "          1.5463e-02, -1.6269e-02],\n",
            "        [ 1.9048e-02, -6.8470e-03,  3.0709e-02,  ..., -1.1101e-02,\n",
            "          1.3404e-02, -1.9684e-02]])), ('transformer_encoder.layers.16.self_attn.out_proj.bias', tensor([-0.0184,  0.0314, -0.0152,  ...,  0.0061, -0.0199, -0.0122])), ('transformer_encoder.layers.16.linear1.weight', tensor([[ 0.0150,  0.0504,  0.0077,  ..., -0.0210,  0.0215,  0.0051],\n",
            "        [ 0.0218, -0.0043,  0.0214,  ...,  0.0189, -0.0052,  0.0111],\n",
            "        [ 0.0051,  0.0314, -0.0110,  ..., -0.0380,  0.0178,  0.0153],\n",
            "        ...,\n",
            "        [ 0.0061,  0.0272, -0.0253,  ..., -0.0094, -0.0040, -0.0104],\n",
            "        [ 0.0714, -0.0379,  0.0029,  ..., -0.0055, -0.0354,  0.0310],\n",
            "        [-0.0081, -0.0209,  0.0099,  ...,  0.0217,  0.0005, -0.0189]])), ('transformer_encoder.layers.16.linear1.bias', tensor([ 0.0084, -0.0072, -0.0268,  ..., -0.0066, -0.0208, -0.0118])), ('transformer_encoder.layers.16.linear2.weight', tensor([[-3.1970e-02,  3.2815e-02,  5.9773e-03,  ..., -3.8019e-02,\n",
            "          2.3991e-02,  9.3127e-03],\n",
            "        [-3.2184e-02, -3.4776e-02,  5.1621e-03,  ...,  1.5649e-02,\n",
            "         -1.2444e-02,  2.7191e-02],\n",
            "        [-1.0771e-02,  3.7057e-02, -1.6212e-02,  ..., -2.8129e-02,\n",
            "         -9.6836e-03,  8.0686e-05],\n",
            "        ...,\n",
            "        [ 1.5924e-02, -4.1752e-03, -2.7017e-02,  ..., -5.1090e-03,\n",
            "          2.3591e-05, -1.6428e-02],\n",
            "        [-6.4968e-03, -1.4521e-02, -1.5814e-02,  ..., -1.9514e-02,\n",
            "         -1.9949e-03,  1.0341e-02],\n",
            "        [ 2.8749e-02, -9.8806e-04, -1.3187e-02,  ..., -3.0087e-02,\n",
            "         -2.7057e-03, -1.1350e-03]])), ('transformer_encoder.layers.16.linear2.bias', tensor([ 0.0187, -0.0063, -0.0028,  ...,  0.0065, -0.0005,  0.0134])), ('transformer_encoder.layers.16.norm1.weight', tensor([0.9046, 0.9126, 0.9804,  ..., 1.0043, 0.9672, 0.9261])), ('transformer_encoder.layers.16.norm1.bias', tensor([-0.0175,  0.0324, -0.0173,  ...,  0.0082, -0.0230, -0.0107])), ('transformer_encoder.layers.16.norm2.weight', tensor([0.9042, 0.9270, 0.9862,  ..., 1.0136, 0.9619, 0.9407])), ('transformer_encoder.layers.16.norm2.bias', tensor([ 0.0193, -0.0028,  0.0111,  ..., -0.0045,  0.0113,  0.0168])), ('transformer_encoder.layers.17.self_attn.in_proj_weight', tensor([[-0.0099, -0.0381,  0.0179,  ..., -0.0032,  0.0153,  0.0557],\n",
            "        [ 0.0171, -0.0421, -0.0031,  ...,  0.0197, -0.0177, -0.0095],\n",
            "        [-0.0363,  0.0160,  0.0536,  ..., -0.0127,  0.0020,  0.0518],\n",
            "        ...,\n",
            "        [ 0.0162,  0.0172, -0.0277,  ..., -0.0033,  0.0248,  0.0435],\n",
            "        [ 0.0105, -0.0019, -0.0288,  ...,  0.0074, -0.0385,  0.0031],\n",
            "        [ 0.0109, -0.0117,  0.0129,  ...,  0.0292, -0.0106,  0.0064]])), ('transformer_encoder.layers.17.self_attn.in_proj_bias', tensor([-2.8952e-03, -1.8419e-03,  2.7103e-03,  ..., -1.7813e-03,\n",
            "         2.6362e-03,  9.2660e-05])), ('transformer_encoder.layers.17.self_attn.out_proj.weight', tensor([[ 9.2338e-03,  7.1761e-03,  1.6235e-02,  ..., -6.7765e-04,\n",
            "         -1.2952e-02,  4.8007e-02],\n",
            "        [ 1.2818e-02,  1.1925e-02,  6.6384e-03,  ..., -6.4127e-03,\n",
            "          1.1687e-03,  5.0194e-03],\n",
            "        [-3.6607e-02,  8.8802e-03,  8.0103e-03,  ...,  3.9564e-02,\n",
            "          4.0203e-02, -1.9648e-02],\n",
            "        ...,\n",
            "        [ 1.8867e-02, -2.1498e-05, -3.2926e-03,  ...,  5.3255e-03,\n",
            "          3.4594e-02,  3.8632e-04],\n",
            "        [ 4.2887e-03,  2.3015e-02, -2.3014e-02,  ..., -2.0202e-02,\n",
            "          1.4695e-02, -2.4557e-02],\n",
            "        [ 2.0745e-02, -2.0494e-02,  2.3129e-02,  ..., -3.8206e-03,\n",
            "          6.7988e-03, -2.5890e-03]])), ('transformer_encoder.layers.17.self_attn.out_proj.bias', tensor([-0.0068,  0.0284, -0.0112,  ...,  0.0103, -0.0211, -0.0094])), ('transformer_encoder.layers.17.linear1.weight', tensor([[ 0.0175,  0.0218, -0.0025,  ...,  0.0230,  0.0130, -0.0058],\n",
            "        [-0.0075, -0.0189,  0.0483,  ...,  0.0192,  0.0183,  0.0293],\n",
            "        [-0.0155, -0.0106, -0.0500,  ..., -0.0208,  0.0233,  0.0311],\n",
            "        ...,\n",
            "        [ 0.0107,  0.0303, -0.0135,  ...,  0.0135, -0.0098, -0.0045],\n",
            "        [ 0.0370, -0.0208,  0.0199,  ...,  0.0007, -0.0255,  0.0213],\n",
            "        [ 0.0062, -0.0033,  0.0236,  ...,  0.0275,  0.0015, -0.0175]])), ('transformer_encoder.layers.17.linear1.bias', tensor([ 0.0118,  0.0009, -0.0379,  ..., -0.0043, -0.0294, -0.0081])), ('transformer_encoder.layers.17.linear2.weight', tensor([[-0.0180,  0.0158,  0.0003,  ...,  0.0031,  0.0124,  0.0118],\n",
            "        [ 0.0139, -0.0144,  0.0328,  ...,  0.0206,  0.0229, -0.0102],\n",
            "        [-0.0147,  0.0233, -0.0207,  ..., -0.0024, -0.0060,  0.0152],\n",
            "        ...,\n",
            "        [ 0.0209,  0.0315,  0.0194,  ..., -0.0096,  0.0070, -0.0319],\n",
            "        [ 0.0119, -0.0017,  0.0338,  ..., -0.0116,  0.0009,  0.0486],\n",
            "        [ 0.0107, -0.0282, -0.0644,  ...,  0.0127,  0.0178,  0.0028]])), ('transformer_encoder.layers.17.linear2.bias', tensor([ 0.0087, -0.0122,  0.0011,  ...,  0.0127,  0.0030,  0.0111])), ('transformer_encoder.layers.17.norm1.weight', tensor([0.8890, 0.9320, 0.9850,  ..., 1.0068, 0.9717, 0.9266])), ('transformer_encoder.layers.17.norm1.bias', tensor([-0.0069,  0.0352, -0.0146,  ...,  0.0126, -0.0233, -0.0104])), ('transformer_encoder.layers.17.norm2.weight', tensor([0.9104, 0.9439, 0.9957,  ..., 1.0149, 0.9537, 0.9460])), ('transformer_encoder.layers.17.norm2.bias', tensor([ 0.0118, -0.0051,  0.0134,  ...,  0.0029,  0.0182,  0.0152])), ('transformer_encoder.layers.18.self_attn.in_proj_weight', tensor([[-0.0108, -0.0304,  0.0411,  ...,  0.0056,  0.0157,  0.0401],\n",
            "        [ 0.0214, -0.0435,  0.0154,  ...,  0.0251, -0.0251, -0.0017],\n",
            "        [-0.0380,  0.0114,  0.0337,  ..., -0.0256, -0.0023,  0.0581],\n",
            "        ...,\n",
            "        [ 0.0150,  0.0271, -0.0306,  ..., -0.0038,  0.0313,  0.0345],\n",
            "        [ 0.0281, -0.0055, -0.0194,  ...,  0.0089, -0.0257, -0.0010],\n",
            "        [-0.0071, -0.0057,  0.0097,  ...,  0.0128,  0.0034,  0.0024]])), ('transformer_encoder.layers.18.self_attn.in_proj_bias', tensor([ 0.0066,  0.0002,  0.0026,  ..., -0.0010,  0.0019, -0.0009])), ('transformer_encoder.layers.18.self_attn.out_proj.weight', tensor([[-0.0017, -0.0015,  0.0234,  ..., -0.0138, -0.0037,  0.0264],\n",
            "        [ 0.0256,  0.0150, -0.0060,  ..., -0.0066, -0.0091,  0.0248],\n",
            "        [-0.0338,  0.0085,  0.0085,  ...,  0.0335,  0.0217, -0.0286],\n",
            "        ...,\n",
            "        [ 0.0130, -0.0002, -0.0148,  ...,  0.0035,  0.0256, -0.0017],\n",
            "        [ 0.0024,  0.0236, -0.0186,  ..., -0.0205,  0.0160, -0.0205],\n",
            "        [ 0.0243, -0.0206,  0.0229,  ...,  0.0098,  0.0146, -0.0109]])), ('transformer_encoder.layers.18.self_attn.out_proj.bias', tensor([-0.0071,  0.0342, -0.0073,  ...,  0.0122, -0.0201, -0.0081])), ('transformer_encoder.layers.18.linear1.weight', tensor([[-0.0324,  0.0203, -0.0047,  ..., -0.0022,  0.0211, -0.0079],\n",
            "        [ 0.0075, -0.0121,  0.0648,  ...,  0.0214, -0.0133,  0.0065],\n",
            "        [ 0.0010, -0.0126, -0.0294,  ..., -0.0154,  0.0564,  0.0566],\n",
            "        ...,\n",
            "        [-0.0130,  0.0161, -0.0193,  ...,  0.0022,  0.0168,  0.0484],\n",
            "        [ 0.0268, -0.0202,  0.0053,  ..., -0.0102, -0.0295,  0.0251],\n",
            "        [-0.0266, -0.0480,  0.0146,  ...,  0.0031,  0.0008, -0.0180]])), ('transformer_encoder.layers.18.linear1.bias', tensor([ 0.0099, -0.0115, -0.0284,  ..., -0.0042, -0.0278, -0.0094])), ('transformer_encoder.layers.18.linear2.weight', tensor([[-0.0320,  0.0035,  0.0478,  ...,  0.0050, -0.0044,  0.0189],\n",
            "        [ 0.0222, -0.0338,  0.0404,  ...,  0.0353, -0.0075,  0.0219],\n",
            "        [-0.0370,  0.0075, -0.0131,  ...,  0.0104,  0.0015,  0.0155],\n",
            "        ...,\n",
            "        [ 0.0498, -0.0095, -0.0084,  ...,  0.0096,  0.0076, -0.0345],\n",
            "        [ 0.0131,  0.0331, -0.0120,  ..., -0.0062, -0.0046,  0.0610],\n",
            "        [ 0.0007, -0.0523, -0.0079,  ..., -0.0055,  0.0073,  0.0173]])), ('transformer_encoder.layers.18.linear2.bias', tensor([ 0.0069, -0.0093,  0.0031,  ...,  0.0086,  0.0076,  0.0099])), ('transformer_encoder.layers.18.norm1.weight', tensor([0.8913, 0.9573, 0.9923,  ..., 1.0103, 0.9729, 0.9229])), ('transformer_encoder.layers.18.norm1.bias', tensor([-0.0071,  0.0387, -0.0097,  ...,  0.0192, -0.0213, -0.0099])), ('transformer_encoder.layers.18.norm2.weight', tensor([0.9094, 0.9519, 1.0017,  ..., 1.0147, 0.9537, 0.9426])), ('transformer_encoder.layers.18.norm2.bias', tensor([ 0.0104, -0.0046,  0.0109,  ..., -0.0028,  0.0203,  0.0149])), ('transformer_encoder.layers.19.self_attn.in_proj_weight', tensor([[-0.0224, -0.0243,  0.0370,  ..., -0.0027,  0.0129,  0.0309],\n",
            "        [ 0.0209, -0.0392,  0.0309,  ...,  0.0153, -0.0254,  0.0090],\n",
            "        [-0.0462,  0.0166,  0.0256,  ..., -0.0204,  0.0069,  0.0516],\n",
            "        ...,\n",
            "        [ 0.0153,  0.0183, -0.0307,  ..., -0.0097,  0.0206,  0.0337],\n",
            "        [ 0.0299,  0.0020, -0.0073,  ..., -0.0029, -0.0311, -0.0046],\n",
            "        [-0.0070, -0.0022,  0.0113,  ...,  0.0114,  0.0055,  0.0033]])), ('transformer_encoder.layers.19.self_attn.in_proj_bias', tensor([ 0.0092,  0.0022, -0.0001,  ..., -0.0024,  0.0023,  0.0010])), ('transformer_encoder.layers.19.self_attn.out_proj.weight', tensor([[-0.0018, -0.0011,  0.0190,  ..., -0.0026, -0.0274,  0.0224],\n",
            "        [ 0.0182,  0.0204,  0.0026,  ..., -0.0019, -0.0058,  0.0094],\n",
            "        [-0.0426, -0.0029, -0.0016,  ...,  0.0143,  0.0115, -0.0193],\n",
            "        ...,\n",
            "        [ 0.0162,  0.0051, -0.0086,  ...,  0.0132,  0.0264, -0.0066],\n",
            "        [-0.0059,  0.0244, -0.0085,  ..., -0.0198,  0.0175, -0.0064],\n",
            "        [ 0.0078, -0.0207,  0.0236,  ..., -0.0084,  0.0132, -0.0157]])), ('transformer_encoder.layers.19.self_attn.out_proj.bias', tensor([ 0.0005,  0.0395, -0.0098,  ...,  0.0123, -0.0199, -0.0130])), ('transformer_encoder.layers.19.linear1.weight', tensor([[-0.0296,  0.0186, -0.0068,  ...,  0.0085,  0.0172,  0.0049],\n",
            "        [ 0.0131, -0.0459,  0.0448,  ...,  0.0156, -0.0343,  0.0103],\n",
            "        [ 0.0179,  0.0112, -0.0211,  ..., -0.0192,  0.0248,  0.0182],\n",
            "        ...,\n",
            "        [-0.0197,  0.0226, -0.0177,  ..., -0.0103,  0.0111,  0.0146],\n",
            "        [ 0.0584,  0.0125, -0.0167,  ..., -0.0134, -0.0346,  0.0318],\n",
            "        [ 0.0094, -0.0076,  0.0187,  ...,  0.0127, -0.0053, -0.0287]])), ('transformer_encoder.layers.19.linear1.bias', tensor([ 0.0086, -0.0104, -0.0348,  ..., -0.0050, -0.0264, -0.0052])), ('transformer_encoder.layers.19.linear2.weight', tensor([[-0.0161, -0.0221,  0.0168,  ..., -0.0069,  0.0042,  0.0262],\n",
            "        [ 0.0157, -0.0079,  0.0046,  ...,  0.0270,  0.0005, -0.0025],\n",
            "        [-0.0288,  0.0247, -0.0012,  ..., -0.0129,  0.0057, -0.0032],\n",
            "        ...,\n",
            "        [ 0.0120,  0.0258, -0.0012,  ...,  0.0043,  0.0398, -0.0222],\n",
            "        [ 0.0145, -0.0130,  0.0007,  ..., -0.0288, -0.0245,  0.0085],\n",
            "        [-0.0009, -0.0428, -0.0084,  ..., -0.0271,  0.0127,  0.0167]])), ('transformer_encoder.layers.19.linear2.bias', tensor([-0.0063, -0.0103,  0.0010,  ...,  0.0093,  0.0134,  0.0125])), ('transformer_encoder.layers.19.norm1.weight', tensor([0.8894, 0.9773, 0.9979,  ..., 1.0095, 0.9713, 0.9197])), ('transformer_encoder.layers.19.norm1.bias', tensor([ 0.0015,  0.0453, -0.0110,  ...,  0.0162, -0.0209, -0.0141])), ('transformer_encoder.layers.19.norm2.weight', tensor([0.9022, 0.9645, 1.0023,  ..., 1.0155, 0.9432, 0.9310])), ('transformer_encoder.layers.19.norm2.bias', tensor([-0.0024, -0.0070,  0.0150,  ...,  0.0002,  0.0278,  0.0172])), ('transformer_encoder.layers.20.self_attn.in_proj_weight', tensor([[-0.0127, -0.0339,  0.0297,  ...,  0.0046,  0.0134,  0.0406],\n",
            "        [ 0.0160, -0.0343,  0.0205,  ...,  0.0285, -0.0264,  0.0276],\n",
            "        [-0.0431,  0.0164,  0.0283,  ..., -0.0287, -0.0171,  0.0365],\n",
            "        ...,\n",
            "        [ 0.0256,  0.0280, -0.0292,  ..., -0.0028,  0.0214,  0.0397],\n",
            "        [ 0.0309, -0.0018, -0.0260,  ...,  0.0019, -0.0214, -0.0002],\n",
            "        [-0.0047, -0.0018,  0.0159,  ...,  0.0111,  0.0010,  0.0066]])), ('transformer_encoder.layers.20.self_attn.in_proj_bias', tensor([ 0.0011,  0.0017,  0.0054,  ..., -0.0002,  0.0016, -0.0005])), ('transformer_encoder.layers.20.self_attn.out_proj.weight', tensor([[-0.0048, -0.0001,  0.0134,  ..., -0.0090, -0.0106,  0.0302],\n",
            "        [ 0.0153,  0.0180,  0.0030,  ..., -0.0072,  0.0007,  0.0175],\n",
            "        [-0.0430, -0.0026,  0.0049,  ...,  0.0257,  0.0192, -0.0207],\n",
            "        ...,\n",
            "        [ 0.0264,  0.0098, -0.0066,  ..., -0.0027,  0.0245, -0.0066],\n",
            "        [-0.0181,  0.0292, -0.0199,  ..., -0.0112,  0.0262,  0.0058],\n",
            "        [ 0.0082, -0.0094,  0.0184,  ...,  0.0063, -0.0005, -0.0162]])), ('transformer_encoder.layers.20.self_attn.out_proj.bias', tensor([-0.0016,  0.0390, -0.0031,  ...,  0.0164, -0.0120, -0.0103])), ('transformer_encoder.layers.20.linear1.weight', tensor([[-0.0126,  0.0233,  0.0090,  ...,  0.0059,  0.0590, -0.0119],\n",
            "        [ 0.0208, -0.0288,  0.0749,  ...,  0.0381,  0.0111,  0.0020],\n",
            "        [ 0.0208,  0.0118, -0.0221,  ..., -0.0271,  0.0256,  0.0205],\n",
            "        ...,\n",
            "        [-0.0116,  0.0214, -0.0005,  ..., -0.0047,  0.0146, -0.0039],\n",
            "        [ 0.0580, -0.0486, -0.0038,  ..., -0.0040,  0.0101,  0.0596],\n",
            "        [-0.0180, -0.0082,  0.0567,  ...,  0.0455,  0.0134, -0.0399]])), ('transformer_encoder.layers.20.linear1.bias', tensor([ 0.0175, -0.0028, -0.0323,  ..., -0.0114, -0.0224, -0.0073])), ('transformer_encoder.layers.20.linear2.weight', tensor([[-0.0111, -0.0157,  0.0096,  ...,  0.0059,  0.0025,  0.0033],\n",
            "        [-0.0157,  0.0068, -0.0025,  ...,  0.0033, -0.0233,  0.0591],\n",
            "        [-0.0226,  0.0355,  0.0072,  ..., -0.0060, -0.0219,  0.0266],\n",
            "        ...,\n",
            "        [-0.0038,  0.0244, -0.0030,  ...,  0.0267,  0.0121,  0.0101],\n",
            "        [-0.0116,  0.0125, -0.0059,  ..., -0.0716,  0.0322,  0.0286],\n",
            "        [-0.0120, -0.0372, -0.0045,  ...,  0.0032, -0.0078, -0.0399]])), ('transformer_encoder.layers.20.linear2.bias', tensor([-0.0045, -0.0201, -0.0003,  ...,  0.0064,  0.0170,  0.0132])), ('transformer_encoder.layers.20.norm1.weight', tensor([0.8865, 0.9884, 1.0000,  ..., 1.0097, 0.9591, 0.9165])), ('transformer_encoder.layers.20.norm1.bias', tensor([-0.0042,  0.0448, -0.0059,  ...,  0.0214, -0.0117, -0.0125])), ('transformer_encoder.layers.20.norm2.weight', tensor([0.9203, 0.9616, 1.0082,  ..., 1.0137, 0.9438, 0.9406])), ('transformer_encoder.layers.20.norm2.bias', tensor([-0.0008, -0.0147,  0.0134,  ..., -0.0005,  0.0300,  0.0146])), ('transformer_encoder.layers.21.self_attn.in_proj_weight', tensor([[-0.0024, -0.0297,  0.0308,  ...,  0.0026,  0.0277,  0.0340],\n",
            "        [ 0.0301, -0.0288,  0.0269,  ...,  0.0247, -0.0142,  0.0201],\n",
            "        [-0.0335,  0.0234,  0.0256,  ..., -0.0223, -0.0199,  0.0304],\n",
            "        ...,\n",
            "        [ 0.0215,  0.0369, -0.0254,  ..., -0.0050,  0.0292,  0.0362],\n",
            "        [ 0.0325, -0.0028, -0.0298,  ...,  0.0065, -0.0264, -0.0049],\n",
            "        [-0.0219,  0.0033,  0.0115,  ...,  0.0101, -0.0003,  0.0071]])), ('transformer_encoder.layers.21.self_attn.in_proj_bias', tensor([0.0040, 0.0038, 0.0085,  ..., 0.0005, 0.0012, 0.0003])), ('transformer_encoder.layers.21.self_attn.out_proj.weight', tensor([[-0.0084, -0.0029,  0.0174,  ..., -0.0070, -0.0199,  0.0348],\n",
            "        [ 0.0047,  0.0172, -0.0083,  ...,  0.0068,  0.0101,  0.0165],\n",
            "        [-0.0346,  0.0077, -0.0112,  ...,  0.0196,  0.0049, -0.0147],\n",
            "        ...,\n",
            "        [ 0.0230,  0.0145, -0.0056,  ...,  0.0073,  0.0270, -0.0064],\n",
            "        [-0.0171,  0.0226, -0.0173,  ..., -0.0087,  0.0246, -0.0123],\n",
            "        [ 0.0151, -0.0131,  0.0074,  ..., -0.0016, -0.0037, -0.0071]])), ('transformer_encoder.layers.21.self_attn.out_proj.bias', tensor([-0.0074,  0.0297, -0.0023,  ...,  0.0157, -0.0039, -0.0117])), ('transformer_encoder.layers.21.linear1.weight', tensor([[-0.0318,  0.0261,  0.0290,  ...,  0.0075, -0.0104, -0.0225],\n",
            "        [-0.0015, -0.0333,  0.0251,  ...,  0.0161,  0.0066,  0.0148],\n",
            "        [ 0.0120,  0.0183, -0.0353,  ..., -0.0231,  0.0237,  0.0166],\n",
            "        ...,\n",
            "        [ 0.0235,  0.0135, -0.0228,  ...,  0.0147,  0.0402,  0.0418],\n",
            "        [ 0.0627,  0.0018,  0.0154,  ...,  0.0229, -0.0207,  0.0189],\n",
            "        [-0.0249,  0.0097,  0.0063,  ...,  0.0141, -0.0064, -0.0059]])), ('transformer_encoder.layers.21.linear1.bias', tensor([ 0.0042, -0.0092, -0.0319,  ..., -0.0048, -0.0176,  0.0032])), ('transformer_encoder.layers.21.linear2.weight', tensor([[-0.0109, -0.0021,  0.0068,  ..., -0.0250,  0.0655,  0.0237],\n",
            "        [ 0.0201,  0.0026,  0.0021,  ..., -0.0042,  0.0201, -0.0003],\n",
            "        [-0.0042,  0.0412,  0.0056,  ..., -0.0199, -0.0011,  0.0211],\n",
            "        ...,\n",
            "        [-0.0267, -0.0037, -0.0006,  ..., -0.0039, -0.0094, -0.0088],\n",
            "        [-0.0168,  0.0106,  0.0044,  ..., -0.0038,  0.0038, -0.0065],\n",
            "        [-0.0325, -0.0414,  0.0043,  ..., -0.0184, -0.0392,  0.0103]])), ('transformer_encoder.layers.21.linear2.bias', tensor([-0.0073, -0.0233, -0.0012,  ...,  0.0062,  0.0061,  0.0153])), ('transformer_encoder.layers.21.norm1.weight', tensor([0.9028, 0.9828, 1.0030,  ..., 1.0121, 0.9496, 0.9258])), ('transformer_encoder.layers.21.norm1.bias', tensor([-0.0090,  0.0374, -0.0047,  ...,  0.0231, -0.0033, -0.0131])), ('transformer_encoder.layers.21.norm2.weight', tensor([0.9272, 0.9602, 1.0085,  ..., 1.0137, 0.9464, 0.9400])), ('transformer_encoder.layers.21.norm2.bias', tensor([-0.0041, -0.0146,  0.0124,  ...,  0.0028,  0.0209,  0.0191])), ('transformer_encoder.layers.22.self_attn.in_proj_weight', tensor([[-6.5488e-04, -3.7238e-02,  2.1960e-02,  ...,  9.4786e-05,\n",
            "          1.5108e-02,  2.9301e-02],\n",
            "        [ 3.2249e-02, -3.7815e-02,  2.7705e-02,  ...,  2.3853e-02,\n",
            "         -2.3102e-02,  1.0734e-02],\n",
            "        [-4.4556e-02,  1.8712e-02,  2.7950e-02,  ..., -2.3412e-02,\n",
            "         -1.2409e-02,  3.1381e-02],\n",
            "        ...,\n",
            "        [ 2.3459e-02,  3.2481e-02, -2.6127e-02,  ...,  2.9690e-03,\n",
            "          2.8184e-02,  3.5773e-02],\n",
            "        [ 1.8985e-02, -4.3769e-03, -1.9195e-02,  ...,  6.0084e-03,\n",
            "         -2.6501e-02, -8.8080e-04],\n",
            "        [-9.1130e-03, -2.3334e-03,  1.4046e-02,  ...,  7.8686e-03,\n",
            "         -3.3431e-03,  3.9382e-03]])), ('transformer_encoder.layers.22.self_attn.in_proj_bias', tensor([0.0061, 0.0041, 0.0050,  ..., 0.0001, 0.0004, 0.0003])), ('transformer_encoder.layers.22.self_attn.out_proj.weight', tensor([[-0.0142,  0.0156,  0.0120,  ..., -0.0059, -0.0323,  0.0203],\n",
            "        [ 0.0170,  0.0136, -0.0012,  ..., -0.0186, -0.0038,  0.0093],\n",
            "        [-0.0365,  0.0010, -0.0089,  ...,  0.0307,  0.0183, -0.0246],\n",
            "        ...,\n",
            "        [ 0.0127,  0.0146, -0.0085,  ...,  0.0142,  0.0240, -0.0195],\n",
            "        [-0.0246,  0.0299, -0.0119,  ..., -0.0178,  0.0387, -0.0358],\n",
            "        [ 0.0152, -0.0177,  0.0242,  ..., -0.0018,  0.0105,  0.0107]])), ('transformer_encoder.layers.22.self_attn.out_proj.bias', tensor([-0.0141,  0.0297,  0.0001,  ...,  0.0189, -0.0077, -0.0015])), ('transformer_encoder.layers.22.linear1.weight', tensor([[-0.0144,  0.0087,  0.0199,  ...,  0.0147,  0.0206, -0.0252],\n",
            "        [ 0.0087, -0.0178,  0.0340,  ..., -0.0059,  0.0182,  0.0142],\n",
            "        [ 0.0683,  0.0092, -0.0224,  ..., -0.0202,  0.0244,  0.0263],\n",
            "        ...,\n",
            "        [-0.0113,  0.0119, -0.0327,  ...,  0.0059, -0.0182,  0.0004],\n",
            "        [ 0.0453,  0.0043,  0.0350,  ...,  0.0302, -0.0135,  0.0350],\n",
            "        [-0.0484, -0.0252,  0.0216,  ...,  0.0106,  0.0186, -0.0077]])), ('transformer_encoder.layers.22.linear1.bias', tensor([ 0.0128, -0.0120, -0.0359,  ..., -0.0036, -0.0174, -0.0046])), ('transformer_encoder.layers.22.linear2.weight', tensor([[-0.0166,  0.0148,  0.0054,  ..., -0.0016,  0.0529, -0.0080],\n",
            "        [-0.0034, -0.0049,  0.0106,  ...,  0.0276,  0.0380,  0.0133],\n",
            "        [-0.0015,  0.0183,  0.0076,  ..., -0.0112,  0.0040,  0.0253],\n",
            "        ...,\n",
            "        [ 0.0028,  0.0068,  0.0168,  ..., -0.0063, -0.0058,  0.0100],\n",
            "        [ 0.0062, -0.0091,  0.0055,  ..., -0.0221,  0.0144,  0.0583],\n",
            "        [-0.0002, -0.0284, -0.0002,  ..., -0.0029, -0.0206,  0.0120]])), ('transformer_encoder.layers.22.linear2.bias', tensor([-0.0136, -0.0224, -0.0039,  ...,  0.0107,  0.0048,  0.0123])), ('transformer_encoder.layers.22.norm1.weight', tensor([0.9155, 0.9786, 1.0036,  ..., 1.0179, 0.9500, 0.9206])), ('transformer_encoder.layers.22.norm1.bias', tensor([-0.0135,  0.0359, -0.0056,  ...,  0.0287, -0.0083, -0.0029])), ('transformer_encoder.layers.22.norm2.weight', tensor([0.9234, 0.9648, 1.0071,  ..., 1.0134, 0.9500, 0.9378])), ('transformer_encoder.layers.22.norm2.bias', tensor([-0.0085, -0.0137,  0.0121,  ...,  0.0025,  0.0192,  0.0158])), ('transformer_encoder.layers.23.self_attn.in_proj_weight', tensor([[ 0.0097, -0.0326,  0.0269,  ...,  0.0043,  0.0174,  0.0249],\n",
            "        [ 0.0427, -0.0413,  0.0225,  ...,  0.0243, -0.0193,  0.0049],\n",
            "        [-0.0424,  0.0210,  0.0261,  ..., -0.0241, -0.0103,  0.0308],\n",
            "        ...,\n",
            "        [ 0.0220,  0.0290, -0.0312,  ..., -0.0017,  0.0220,  0.0351],\n",
            "        [ 0.0250, -0.0007, -0.0143,  ...,  0.0034, -0.0301,  0.0022],\n",
            "        [-0.0174, -0.0045,  0.0148,  ...,  0.0118,  0.0087,  0.0091]])), ('transformer_encoder.layers.23.self_attn.in_proj_bias', tensor([0.0046, 0.0027, 0.0031,  ..., 0.0014, 0.0008, 0.0003])), ('transformer_encoder.layers.23.self_attn.out_proj.weight', tensor([[-0.0184,  0.0065,  0.0159,  ...,  0.0144, -0.0175,  0.0425],\n",
            "        [ 0.0079,  0.0119,  0.0022,  ..., -0.0146,  0.0096,  0.0141],\n",
            "        [-0.0365,  0.0095, -0.0123,  ...,  0.0302,  0.0182, -0.0127],\n",
            "        ...,\n",
            "        [ 0.0113,  0.0177, -0.0095,  ...,  0.0131,  0.0175, -0.0171],\n",
            "        [-0.0116,  0.0298, -0.0173,  ..., -0.0190,  0.0290, -0.0276],\n",
            "        [ 0.0179, -0.0071,  0.0240,  ..., -0.0102, -0.0014,  0.0101]])), ('transformer_encoder.layers.23.self_attn.out_proj.bias', tensor([-0.0138,  0.0247,  0.0020,  ...,  0.0179, -0.0018,  0.0017])), ('transformer_encoder.layers.23.linear1.weight', tensor([[-0.0021,  0.0111,  0.0168,  ...,  0.0128,  0.0221, -0.0384],\n",
            "        [ 0.0059, -0.0101, -0.0094,  ..., -0.0175, -0.0051,  0.0023],\n",
            "        [ 0.0535,  0.0305, -0.0022,  ..., -0.0087, -0.0050,  0.0295],\n",
            "        ...,\n",
            "        [ 0.0086, -0.0046, -0.0148,  ...,  0.0253,  0.0099,  0.0103],\n",
            "        [ 0.0285, -0.0412,  0.0239,  ...,  0.0121,  0.0103,  0.0253],\n",
            "        [-0.0314, -0.0040,  0.0236,  ...,  0.0175,  0.0226, -0.0329]])), ('transformer_encoder.layers.23.linear1.bias', tensor([ 0.0110, -0.0099, -0.0340,  ..., -0.0079, -0.0257, -0.0105])), ('transformer_encoder.layers.23.linear2.weight', tensor([[-0.0027, -0.0115,  0.0008,  ...,  0.0122,  0.0438,  0.0047],\n",
            "        [ 0.0333, -0.0111,  0.0190,  ...,  0.0424,  0.0364,  0.0245],\n",
            "        [-0.0057,  0.0220, -0.0084,  ..., -0.0015, -0.0031,  0.0119],\n",
            "        ...,\n",
            "        [ 0.0231, -0.0032,  0.0380,  ...,  0.0040, -0.0051, -0.0046],\n",
            "        [ 0.0414, -0.0260,  0.0216,  ..., -0.0309,  0.0469,  0.0062],\n",
            "        [-0.0009, -0.0145, -0.0029,  ..., -0.0126, -0.0159, -0.0052]])), ('transformer_encoder.layers.23.linear2.bias', tensor([-0.0035, -0.0181, -0.0040,  ...,  0.0049, -0.0043,  0.0105])), ('transformer_encoder.layers.23.norm1.weight', tensor([0.9214, 0.9764, 1.0030,  ..., 1.0183, 0.9480, 0.9194])), ('transformer_encoder.layers.23.norm1.bias', tensor([-0.0147,  0.0334, -0.0055,  ...,  0.0291, -0.0034,  0.0044])), ('transformer_encoder.layers.23.norm2.weight', tensor([0.9314, 0.9678, 1.0073,  ..., 1.0109, 0.9491, 0.9433])), ('transformer_encoder.layers.23.norm2.bias', tensor([ 0.0034, -0.0112,  0.0125,  ...,  0.0014,  0.0127,  0.0127])), ('transformer_encoder.layers.24.self_attn.in_proj_weight', tensor([[ 0.0061, -0.0404,  0.0229,  ...,  0.0064,  0.0226,  0.0267],\n",
            "        [ 0.0348, -0.0468,  0.0228,  ...,  0.0290, -0.0275,  0.0150],\n",
            "        [-0.0404,  0.0074,  0.0376,  ..., -0.0498, -0.0061,  0.0245],\n",
            "        ...,\n",
            "        [ 0.0119,  0.0331, -0.0311,  ..., -0.0002,  0.0336,  0.0297],\n",
            "        [ 0.0123, -0.0019, -0.0118,  ...,  0.0070, -0.0368, -0.0037],\n",
            "        [-0.0111, -0.0041,  0.0143,  ...,  0.0003,  0.0134, -0.0075]])), ('transformer_encoder.layers.24.self_attn.in_proj_bias', tensor([ 0.0046,  0.0042,  0.0015,  ...,  0.0028,  0.0006, -0.0001])), ('transformer_encoder.layers.24.self_attn.out_proj.weight', tensor([[-2.5049e-02, -6.2432e-03,  2.2512e-02,  ...,  6.7852e-03,\n",
            "         -2.9395e-02,  3.9882e-02],\n",
            "        [ 5.1954e-03,  1.5883e-02,  3.6275e-03,  ...,  1.3993e-02,\n",
            "          6.8737e-03,  1.4438e-02],\n",
            "        [-3.4362e-02,  1.1820e-02, -1.0331e-02,  ...,  4.9891e-02,\n",
            "          3.0791e-02, -2.0652e-02],\n",
            "        ...,\n",
            "        [ 2.4037e-02,  1.1930e-02, -4.8569e-03,  ...,  5.8542e-05,\n",
            "          3.0934e-02, -2.3073e-02],\n",
            "        [-7.1562e-03,  3.5588e-02, -1.3182e-02,  ..., -1.4987e-02,\n",
            "          2.3059e-02, -6.3720e-03],\n",
            "        [ 9.0304e-03, -1.6597e-02,  2.0521e-02,  ...,  4.9722e-03,\n",
            "         -1.1340e-03, -3.1627e-04]])), ('transformer_encoder.layers.24.self_attn.out_proj.bias', tensor([-0.0123,  0.0240, -0.0010,  ...,  0.0179, -0.0026, -0.0010])), ('transformer_encoder.layers.24.linear1.weight', tensor([[ 0.0120,  0.0073,  0.0452,  ..., -0.0098,  0.0228, -0.0183],\n",
            "        [ 0.0060, -0.0250,  0.0263,  ..., -0.0186, -0.0121,  0.0173],\n",
            "        [ 0.0293,  0.0112, -0.0303,  ..., -0.0182,  0.0205,  0.0266],\n",
            "        ...,\n",
            "        [ 0.0196, -0.0283, -0.0082,  ...,  0.0245,  0.0196,  0.0033],\n",
            "        [ 0.0509, -0.0184,  0.0080,  ...,  0.0103, -0.0147, -0.0104],\n",
            "        [-0.0724,  0.0035,  0.0301,  ...,  0.0082,  0.0111, -0.0520]])), ('transformer_encoder.layers.24.linear1.bias', tensor([ 0.0099, -0.0109, -0.0339,  ..., -0.0080, -0.0244, -0.0090])), ('transformer_encoder.layers.24.linear2.weight', tensor([[ 0.0176,  0.0094,  0.0228,  ...,  0.0273,  0.0230, -0.0217],\n",
            "        [ 0.0229,  0.0090,  0.0297,  ...,  0.0186,  0.0304,  0.0269],\n",
            "        [-0.0052,  0.0110, -0.0199,  ..., -0.0169,  0.0194,  0.0280],\n",
            "        ...,\n",
            "        [-0.0276,  0.0148,  0.0096,  ..., -0.0274,  0.0308, -0.0161],\n",
            "        [ 0.0079,  0.0097,  0.0039,  ..., -0.0203,  0.0576,  0.0322],\n",
            "        [ 0.0250, -0.0218, -0.0060,  ..., -0.0348, -0.0144, -0.0083]])), ('transformer_encoder.layers.24.linear2.bias', tensor([ 0.0045, -0.0307, -0.0057,  ..., -0.0015, -0.0028,  0.0108])), ('transformer_encoder.layers.24.norm1.weight', tensor([0.9262, 0.9739, 1.0046,  ..., 1.0178, 0.9387, 0.9237])), ('transformer_encoder.layers.24.norm1.bias', tensor([-0.0163,  0.0318, -0.0060,  ...,  0.0282, -0.0042,  0.0019])), ('transformer_encoder.layers.24.norm2.weight', tensor([0.9378, 0.9646, 1.0085,  ..., 1.0048, 0.9514, 0.9478])), ('transformer_encoder.layers.24.norm2.bias', tensor([ 0.0073, -0.0210,  0.0100,  ..., -0.0047,  0.0125,  0.0137])), ('transformer_encoder.layers.25.self_attn.in_proj_weight', tensor([[ 0.0250, -0.0393,  0.0431,  ..., -0.0002,  0.0251,  0.0369],\n",
            "        [ 0.0566, -0.0431,  0.0232,  ...,  0.0258, -0.0318,  0.0117],\n",
            "        [-0.0526,  0.0152,  0.0082,  ..., -0.0174,  0.0007,  0.0446],\n",
            "        ...,\n",
            "        [ 0.0072,  0.0255, -0.0324,  ..., -0.0063,  0.0422,  0.0292],\n",
            "        [ 0.0109,  0.0025, -0.0151,  ..., -0.0011, -0.0280, -0.0062],\n",
            "        [-0.0250, -0.0036,  0.0193,  ...,  0.0044,  0.0140, -0.0044]])), ('transformer_encoder.layers.25.self_attn.in_proj_bias', tensor([ 0.0140,  0.0025,  0.0012,  ...,  0.0018, -0.0007,  0.0005])), ('transformer_encoder.layers.25.self_attn.out_proj.weight', tensor([[-0.0178, -0.0137,  0.0244,  ...,  0.0065, -0.0129,  0.0222],\n",
            "        [ 0.0106,  0.0103,  0.0126,  ...,  0.0089,  0.0107,  0.0070],\n",
            "        [-0.0409,  0.0076, -0.0106,  ...,  0.0296,  0.0200, -0.0217],\n",
            "        ...,\n",
            "        [ 0.0119, -0.0006, -0.0138,  ...,  0.0046,  0.0174, -0.0136],\n",
            "        [-0.0156,  0.0338, -0.0130,  ..., -0.0103,  0.0109, -0.0144],\n",
            "        [ 0.0200, -0.0078,  0.0192,  ...,  0.0110, -0.0004,  0.0189]])), ('transformer_encoder.layers.25.self_attn.out_proj.bias', tensor([-0.0077,  0.0136, -0.0033,  ...,  0.0121, -0.0014,  0.0055])), ('transformer_encoder.layers.25.linear1.weight', tensor([[-0.0203,  0.0095,  0.0212,  ..., -0.0072, -0.0061, -0.0277],\n",
            "        [ 0.0065, -0.0057,  0.0026,  ..., -0.0304, -0.0378,  0.0056],\n",
            "        [ 0.0256,  0.0166, -0.0255,  ..., -0.0274,  0.0290,  0.0211],\n",
            "        ...,\n",
            "        [ 0.0300,  0.0189, -0.0400,  ...,  0.0114,  0.0169, -0.0022],\n",
            "        [ 0.0212, -0.0055,  0.0100,  ...,  0.0012, -0.0133,  0.0157],\n",
            "        [ 0.0112,  0.0096,  0.0205,  ...,  0.0155,  0.0113, -0.0285]])), ('transformer_encoder.layers.25.linear1.bias', tensor([ 0.0143, -0.0075, -0.0365,  ..., -0.0111, -0.0268, -0.0084])), ('transformer_encoder.layers.25.linear2.weight', tensor([[ 1.9732e-02, -2.2837e-02,  1.3800e-02,  ..., -2.9803e-03,\n",
            "          2.7498e-02,  9.4968e-03],\n",
            "        [-3.6981e-03, -8.2435e-03,  1.1760e-04,  ...,  4.6029e-03,\n",
            "          1.2510e-02,  4.9416e-02],\n",
            "        [-7.7939e-03,  1.3092e-02,  6.0411e-03,  ...,  7.3502e-03,\n",
            "          1.1455e-02, -4.0110e-03],\n",
            "        ...,\n",
            "        [ 2.5972e-03, -2.8920e-02, -9.6290e-04,  ..., -5.7341e-03,\n",
            "          1.9634e-05,  1.1635e-03],\n",
            "        [ 1.0254e-02, -4.4319e-03, -5.8128e-04,  ..., -7.8969e-03,\n",
            "         -3.4015e-03,  2.6201e-02],\n",
            "        [-6.3323e-03, -2.8185e-02, -6.3114e-03,  ..., -1.6335e-02,\n",
            "          4.6521e-03, -2.6161e-02]])), ('transformer_encoder.layers.25.linear2.bias', tensor([ 0.0080, -0.0287, -0.0056,  ..., -0.0053, -0.0096,  0.0086])), ('transformer_encoder.layers.25.norm1.weight', tensor([0.9279, 0.9651, 1.0064,  ..., 1.0132, 0.9341, 0.9338])), ('transformer_encoder.layers.25.norm1.bias', tensor([-0.0084,  0.0151, -0.0089,  ...,  0.0211,  0.0020,  0.0096])), ('transformer_encoder.layers.25.norm2.weight', tensor([0.9494, 0.9813, 1.0074,  ..., 1.0066, 0.9473, 0.9582])), ('transformer_encoder.layers.25.norm2.bias', tensor([ 0.0120, -0.0172,  0.0108,  ..., -0.0042,  0.0090,  0.0178])), ('transformer_encoder.layers.26.self_attn.in_proj_weight', tensor([[ 0.0158, -0.0241,  0.0405,  ..., -0.0054,  0.0202,  0.0367],\n",
            "        [ 0.0418, -0.0460,  0.0192,  ...,  0.0299, -0.0235,  0.0255],\n",
            "        [-0.0266,  0.0145, -0.0023,  ..., -0.0239, -0.0049,  0.0319],\n",
            "        ...,\n",
            "        [ 0.0062,  0.0229, -0.0274,  ...,  0.0005,  0.0255,  0.0196],\n",
            "        [-0.0026,  0.0070, -0.0161,  ...,  0.0070, -0.0303,  0.0038],\n",
            "        [-0.0227,  0.0035,  0.0238,  ..., -0.0005,  0.0137, -0.0178]])), ('transformer_encoder.layers.26.self_attn.in_proj_bias', tensor([ 0.0038,  0.0035,  0.0024,  ...,  0.0029, -0.0003, -0.0015])), ('transformer_encoder.layers.26.self_attn.out_proj.weight', tensor([[-0.0226, -0.0087,  0.0273,  ...,  0.0133, -0.0194,  0.0286],\n",
            "        [-0.0044,  0.0031,  0.0177,  ...,  0.0156,  0.0062,  0.0040],\n",
            "        [-0.0277,  0.0127, -0.0075,  ...,  0.0366,  0.0121, -0.0251],\n",
            "        ...,\n",
            "        [ 0.0008,  0.0123, -0.0015,  ...,  0.0028,  0.0166, -0.0110],\n",
            "        [-0.0064,  0.0175, -0.0194,  ..., -0.0156,  0.0233, -0.0225],\n",
            "        [ 0.0210, -0.0198,  0.0283,  ...,  0.0034,  0.0199,  0.0035]])), ('transformer_encoder.layers.26.self_attn.out_proj.bias', tensor([ 0.0014,  0.0111, -0.0007,  ...,  0.0116,  0.0023,  0.0153])), ('transformer_encoder.layers.26.linear1.weight', tensor([[-0.0064,  0.0031, -0.0048,  ..., -0.0310,  0.0342, -0.0382],\n",
            "        [ 0.0305, -0.0239, -0.0022,  ..., -0.0239,  0.0209,  0.0416],\n",
            "        [ 0.0612,  0.0157,  0.0157,  ..., -0.0169,  0.0187,  0.0332],\n",
            "        ...,\n",
            "        [-0.0076, -0.0030, -0.0413,  ...,  0.0469, -0.0655, -0.0094],\n",
            "        [ 0.0482, -0.0210, -0.0200,  ..., -0.0185, -0.0382,  0.0356],\n",
            "        [-0.0179, -0.0103,  0.0257,  ...,  0.0106,  0.0106, -0.0266]])), ('transformer_encoder.layers.26.linear1.bias', tensor([ 0.0100, -0.0038, -0.0388,  ..., -0.0069, -0.0299, -0.0141])), ('transformer_encoder.layers.26.linear2.weight', tensor([[-0.0019, -0.0089,  0.0503,  ..., -0.0065,  0.0557,  0.0084],\n",
            "        [ 0.0224, -0.0290, -0.0136,  ..., -0.0148,  0.0337,  0.0181],\n",
            "        [ 0.0062,  0.0034, -0.0057,  ..., -0.0003,  0.0011,  0.0116],\n",
            "        ...,\n",
            "        [ 0.0003, -0.0140, -0.0022,  ..., -0.0154,  0.0203, -0.0134],\n",
            "        [ 0.0310, -0.0244, -0.0004,  ..., -0.0464, -0.0014,  0.0336],\n",
            "        [ 0.0441,  0.0127, -0.0040,  ...,  0.0102, -0.0287, -0.0050]])), ('transformer_encoder.layers.26.linear2.bias', tensor([ 0.0041, -0.0184, -0.0044,  ..., -0.0015, -0.0138, -0.0048])), ('transformer_encoder.layers.26.norm1.weight', tensor([0.9382, 0.9747, 1.0040,  ..., 1.0140, 0.9261, 0.9404])), ('transformer_encoder.layers.26.norm1.bias', tensor([ 0.0007,  0.0139, -0.0070,  ...,  0.0212,  0.0058,  0.0235])), ('transformer_encoder.layers.26.norm2.weight', tensor([0.9536, 0.9881, 1.0089,  ..., 1.0071, 0.9554, 0.9689])), ('transformer_encoder.layers.26.norm2.bias', tensor([ 0.0096, -0.0099,  0.0088,  ..., -0.0015,  0.0055,  0.0096])), ('transformer_encoder.layers.27.self_attn.in_proj_weight', tensor([[ 0.0004, -0.0371,  0.0464,  ..., -0.0003,  0.0265,  0.0419],\n",
            "        [ 0.0355, -0.0567,  0.0290,  ...,  0.0202, -0.0215,  0.0301],\n",
            "        [-0.0210,  0.0139,  0.0135,  ..., -0.0422, -0.0233,  0.0376],\n",
            "        ...,\n",
            "        [-0.0038,  0.0216, -0.0283,  ..., -0.0040,  0.0295,  0.0259],\n",
            "        [-0.0017,  0.0099, -0.0235,  ..., -0.0033, -0.0378, -0.0050],\n",
            "        [-0.0222, -0.0035,  0.0212,  ...,  0.0024,  0.0087, -0.0157]])), ('transformer_encoder.layers.27.self_attn.in_proj_bias', tensor([ 4.9194e-03,  5.4363e-05, -6.7920e-03,  ...,  4.1575e-03,\n",
            "         3.6698e-04, -6.6412e-04])), ('transformer_encoder.layers.27.self_attn.out_proj.weight', tensor([[-0.0136, -0.0154,  0.0159,  ...,  0.0090, -0.0236,  0.0320],\n",
            "        [ 0.0156, -0.0047,  0.0260,  ...,  0.0100,  0.0157,  0.0098],\n",
            "        [-0.0201,  0.0109, -0.0140,  ...,  0.0356,  0.0152, -0.0315],\n",
            "        ...,\n",
            "        [ 0.0102,  0.0106, -0.0075,  ...,  0.0031,  0.0104, -0.0097],\n",
            "        [-0.0134,  0.0194, -0.0015,  ..., -0.0153,  0.0122, -0.0218],\n",
            "        [ 0.0184, -0.0152,  0.0243,  ..., -0.0100,  0.0224, -0.0054]])), ('transformer_encoder.layers.27.self_attn.out_proj.bias', tensor([-0.0010,  0.0143, -0.0011,  ...,  0.0155,  0.0054,  0.0196])), ('transformer_encoder.layers.27.linear1.weight', tensor([[-3.0867e-02,  2.7842e-02,  2.2353e-02,  ...,  1.1986e-02,\n",
            "         -9.6320e-03, -5.0226e-02],\n",
            "        [ 3.3781e-02, -5.2943e-03,  4.4068e-03,  ...,  6.2987e-03,\n",
            "         -1.5898e-02,  2.8045e-03],\n",
            "        [ 5.7973e-03,  2.0531e-02, -1.2936e-02,  ..., -1.6357e-02,\n",
            "          2.1565e-02,  1.4583e-02],\n",
            "        ...,\n",
            "        [ 1.4912e-02,  3.3880e-02, -2.7367e-02,  ...,  1.5532e-02,\n",
            "          4.6189e-02, -5.2854e-03],\n",
            "        [ 2.4053e-02, -3.6687e-02,  7.9459e-03,  ..., -1.1090e-02,\n",
            "          4.2628e-03,  3.2657e-02],\n",
            "        [-1.4278e-02, -2.4133e-03,  1.3153e-02,  ...,  8.7091e-03,\n",
            "         -8.1447e-05, -4.0152e-02]])), ('transformer_encoder.layers.27.linear1.bias', tensor([ 0.0170, -0.0074, -0.0386,  ..., -0.0048, -0.0258, -0.0120])), ('transformer_encoder.layers.27.linear2.weight', tensor([[-9.4637e-04,  1.3513e-02, -4.8344e-02,  ...,  1.5590e-09,\n",
            "          3.9926e-02,  1.7678e-02],\n",
            "        [-6.1451e-03, -3.8738e-02,  2.1805e-02,  ...,  4.4006e-04,\n",
            "          1.1864e-02,  1.5005e-02],\n",
            "        [-1.7983e-02, -1.4449e-02, -3.3807e-03,  ...,  2.6472e-02,\n",
            "         -3.8418e-03,  1.3736e-02],\n",
            "        ...,\n",
            "        [ 2.3044e-02, -2.4067e-03,  1.8309e-03,  ...,  2.2027e-02,\n",
            "          1.7537e-02, -1.1071e-02],\n",
            "        [ 5.4209e-04,  1.2255e-03,  1.3861e-03,  ..., -2.2690e-02,\n",
            "          2.0200e-02,  1.3584e-02],\n",
            "        [ 2.1127e-02, -1.3142e-02,  3.3666e-02,  ...,  1.2758e-03,\n",
            "          3.7578e-03, -1.0942e-03]])), ('transformer_encoder.layers.27.linear2.bias', tensor([ 0.0033, -0.0218, -0.0093,  ..., -0.0051, -0.0193, -0.0150])), ('transformer_encoder.layers.27.norm1.weight', tensor([0.9414, 0.9817, 1.0050,  ..., 1.0167, 0.9421, 0.9438])), ('transformer_encoder.layers.27.norm1.bias', tensor([-0.0006,  0.0162, -0.0092,  ...,  0.0232,  0.0091,  0.0265])), ('transformer_encoder.layers.27.norm2.weight', tensor([0.9639, 0.9939, 1.0063,  ..., 1.0065, 0.9563, 0.9733])), ('transformer_encoder.layers.27.norm2.bias', tensor([ 0.0098, -0.0036,  0.0081,  ..., -0.0030, -0.0038,  0.0069])), ('transformer_encoder.layers.28.self_attn.in_proj_weight', tensor([[ 0.0118, -0.0340,  0.0433,  ...,  0.0116,  0.0269,  0.0397],\n",
            "        [ 0.0286, -0.0568,  0.0153,  ...,  0.0363, -0.0428,  0.0209],\n",
            "        [-0.0288,  0.0014,  0.0120,  ..., -0.0200, -0.0439,  0.0284],\n",
            "        ...,\n",
            "        [-0.0060,  0.0240, -0.0402,  ..., -0.0122,  0.0275,  0.0086],\n",
            "        [-0.0077, -0.0029, -0.0101,  ...,  0.0004, -0.0347, -0.0132],\n",
            "        [-0.0166, -0.0155,  0.0249,  ...,  0.0021,  0.0082, -0.0113]])), ('transformer_encoder.layers.28.self_attn.in_proj_bias', tensor([ 0.0142, -0.0071, -0.0020,  ...,  0.0043, -0.0002, -0.0015])), ('transformer_encoder.layers.28.self_attn.out_proj.weight', tensor([[-0.0036, -0.0042,  0.0111,  ...,  0.0201, -0.0280,  0.0115],\n",
            "        [ 0.0016,  0.0222,  0.0296,  ...,  0.0034,  0.0032,  0.0057],\n",
            "        [-0.0274,  0.0108, -0.0108,  ...,  0.0365,  0.0170, -0.0222],\n",
            "        ...,\n",
            "        [-0.0023,  0.0101, -0.0054,  ..., -0.0016,  0.0118,  0.0008],\n",
            "        [ 0.0046,  0.0071, -0.0196,  ..., -0.0137,  0.0107, -0.0306],\n",
            "        [ 0.0103, -0.0048,  0.0308,  ..., -0.0092,  0.0126,  0.0029]])), ('transformer_encoder.layers.28.self_attn.out_proj.bias', tensor([ 0.0051,  0.0114, -0.0019,  ...,  0.0139,  0.0018,  0.0259])), ('transformer_encoder.layers.28.linear1.weight', tensor([[-0.0506,  0.0280,  0.0604,  ...,  0.0203,  0.0365, -0.0444],\n",
            "        [ 0.0324, -0.0351,  0.0282,  ..., -0.0014,  0.0477,  0.0287],\n",
            "        [ 0.0242,  0.0229, -0.0436,  ..., -0.0259,  0.0066, -0.0042],\n",
            "        ...,\n",
            "        [ 0.0083,  0.0358, -0.0245,  ...,  0.0230,  0.0047, -0.0119],\n",
            "        [ 0.0180, -0.0030,  0.0068,  ..., -0.0187, -0.0599,  0.0127],\n",
            "        [-0.0066,  0.0080,  0.0152,  ...,  0.0246, -0.0028, -0.0382]])), ('transformer_encoder.layers.28.linear1.bias', tensor([ 0.0174, -0.0072, -0.0379,  ..., -0.0074, -0.0267, -0.0059])), ('transformer_encoder.layers.28.linear2.weight', tensor([[ 0.0094,  0.0254, -0.0047,  ...,  0.0218,  0.0308,  0.0469],\n",
            "        [ 0.0184, -0.0130,  0.0268,  ...,  0.0210,  0.0470,  0.0551],\n",
            "        [ 0.0111,  0.0155,  0.0042,  ...,  0.0202,  0.0262,  0.0451],\n",
            "        ...,\n",
            "        [ 0.0050,  0.0181, -0.0304,  ...,  0.0135,  0.0075, -0.0010],\n",
            "        [-0.0050, -0.0113,  0.0338,  ..., -0.0037,  0.0081,  0.0066],\n",
            "        [ 0.0046,  0.0213, -0.0161,  ..., -0.0119, -0.0139, -0.0091]])), ('transformer_encoder.layers.28.linear2.bias', tensor([ 0.0006, -0.0198, -0.0100,  ..., -0.0079, -0.0131, -0.0263])), ('transformer_encoder.layers.28.norm1.weight', tensor([0.9489, 0.9878, 1.0007,  ..., 1.0173, 0.9419, 0.9580])), ('transformer_encoder.layers.28.norm1.bias', tensor([ 0.0047,  0.0204, -0.0104,  ...,  0.0220,  0.0012,  0.0323])), ('transformer_encoder.layers.28.norm2.weight', tensor([0.9657, 1.0010, 1.0021,  ..., 1.0037, 0.9647, 0.9801])), ('transformer_encoder.layers.28.norm2.bias', tensor([ 0.0065,  0.0044,  0.0076,  ..., -0.0073,  0.0020,  0.0003])), ('transformer_encoder.layers.29.self_attn.in_proj_weight', tensor([[-0.0057, -0.0145,  0.0286,  ...,  0.0169,  0.0294,  0.0284],\n",
            "        [ 0.0114, -0.0599,  0.0353,  ...,  0.0299, -0.0223,  0.0366],\n",
            "        [-0.0288,  0.0063, -0.0151,  ..., -0.0158, -0.0108,  0.0321],\n",
            "        ...,\n",
            "        [-0.0092,  0.0183, -0.0373,  ..., -0.0073,  0.0355,  0.0189],\n",
            "        [ 0.0027, -0.0012, -0.0201,  ...,  0.0077, -0.0290, -0.0073],\n",
            "        [-0.0161, -0.0133,  0.0185,  ..., -0.0019,  0.0124, -0.0146]])), ('transformer_encoder.layers.29.self_attn.in_proj_bias', tensor([ 0.0055, -0.0027, -0.0055,  ...,  0.0057, -0.0013, -0.0021])), ('transformer_encoder.layers.29.self_attn.out_proj.weight', tensor([[-0.0115, -0.0041,  0.0026,  ...,  0.0018, -0.0014,  0.0218],\n",
            "        [ 0.0077,  0.0120,  0.0254,  ...,  0.0090,  0.0169,  0.0199],\n",
            "        [-0.0316,  0.0162, -0.0180,  ...,  0.0232,  0.0044, -0.0259],\n",
            "        ...,\n",
            "        [-0.0019,  0.0118, -0.0043,  ...,  0.0041,  0.0130, -0.0126],\n",
            "        [-0.0089,  0.0121, -0.0320,  ..., -0.0155,  0.0207, -0.0144],\n",
            "        [ 0.0144, -0.0035,  0.0199,  ..., -0.0217,  0.0346, -0.0033]])), ('transformer_encoder.layers.29.self_attn.out_proj.bias', tensor([-0.0019,  0.0183, -0.0019,  ...,  0.0099,  0.0015,  0.0235])), ('transformer_encoder.layers.29.linear1.weight', tensor([[-0.0293,  0.0123,  0.0160,  ...,  0.0110,  0.0177, -0.0229],\n",
            "        [-0.0111, -0.0518,  0.0077,  ..., -0.0463,  0.0317,  0.0028],\n",
            "        [ 0.0067,  0.0271, -0.0147,  ..., -0.0476,  0.0386, -0.0123],\n",
            "        ...,\n",
            "        [-0.0001,  0.0039,  0.0072,  ...,  0.0171, -0.0142,  0.0004],\n",
            "        [ 0.0116, -0.0096, -0.0178,  ..., -0.0105, -0.0226,  0.0191],\n",
            "        [-0.0148,  0.0065,  0.0317,  ...,  0.0389, -0.0091, -0.0060]])), ('transformer_encoder.layers.29.linear1.bias', tensor([ 0.0092, -0.0102, -0.0347,  ..., -0.0086, -0.0279, -0.0089])), ('transformer_encoder.layers.29.linear2.weight', tensor([[-0.0180, -0.0062, -0.0320,  ...,  0.0082,  0.0271,  0.0485],\n",
            "        [-0.0070, -0.0062,  0.0086,  ...,  0.0339,  0.0489,  0.0057],\n",
            "        [-0.0030,  0.0039,  0.0105,  ..., -0.0010, -0.0084,  0.0131],\n",
            "        ...,\n",
            "        [-0.0076,  0.0107, -0.0103,  ..., -0.0125,  0.0015, -0.0125],\n",
            "        [ 0.0050,  0.0233, -0.0045,  ..., -0.0211,  0.0259,  0.0015],\n",
            "        [ 0.0050, -0.0032,  0.0061,  ..., -0.0160,  0.0128, -0.0009]])), ('transformer_encoder.layers.29.linear2.bias', tensor([-0.0059, -0.0245, -0.0127,  ..., -0.0105, -0.0049, -0.0334])), ('transformer_encoder.layers.29.norm1.weight', tensor([0.9527, 0.9980, 0.9955,  ..., 1.0149, 0.9506, 0.9678])), ('transformer_encoder.layers.29.norm1.bias', tensor([ 0.0015,  0.0267, -0.0098,  ...,  0.0174,  0.0042,  0.0253])), ('transformer_encoder.layers.29.norm2.weight', tensor([0.9727, 1.0083, 1.0000,  ..., 0.9998, 0.9687, 0.9823])), ('transformer_encoder.layers.29.norm2.bias', tensor([ 0.0045,  0.0064,  0.0090,  ..., -0.0117, -0.0008, -0.0051])), ('transformer_encoder.layers.30.self_attn.in_proj_weight', tensor([[ 0.0114, -0.0337,  0.0330,  ...,  0.0102,  0.0306,  0.0297],\n",
            "        [ 0.0156, -0.0352,  0.0299,  ...,  0.0368, -0.0188,  0.0261],\n",
            "        [-0.0433,  0.0051, -0.0123,  ..., -0.0214, -0.0324,  0.0240],\n",
            "        ...,\n",
            "        [ 0.0018,  0.0279, -0.0452,  ..., -0.0075,  0.0180,  0.0229],\n",
            "        [-0.0064, -0.0024, -0.0141,  ...,  0.0085, -0.0268, -0.0080],\n",
            "        [-0.0137, -0.0198,  0.0201,  ..., -0.0009,  0.0153, -0.0129]])), ('transformer_encoder.layers.30.self_attn.in_proj_bias', tensor([-0.0023,  0.0066,  0.0010,  ...,  0.0052, -0.0011, -0.0023])), ('transformer_encoder.layers.30.self_attn.out_proj.weight', tensor([[-0.0062, -0.0029,  0.0199,  ...,  0.0067, -0.0135,  0.0245],\n",
            "        [ 0.0176,  0.0176,  0.0066,  ..., -0.0052,  0.0015,  0.0071],\n",
            "        [-0.0220,  0.0076, -0.0097,  ...,  0.0313,  0.0071, -0.0333],\n",
            "        ...,\n",
            "        [ 0.0008,  0.0193, -0.0074,  ...,  0.0139,  0.0211, -0.0006],\n",
            "        [-0.0007, -0.0008, -0.0156,  ..., -0.0222,  0.0162, -0.0157],\n",
            "        [ 0.0088, -0.0059,  0.0203,  ..., -0.0184,  0.0248, -0.0026]])), ('transformer_encoder.layers.30.self_attn.out_proj.bias', tensor([ 0.0023,  0.0260, -0.0025,  ...,  0.0074, -0.0052,  0.0155])), ('transformer_encoder.layers.30.linear1.weight', tensor([[-0.0421,  0.0095,  0.0229,  ...,  0.0177, -0.0028, -0.0166],\n",
            "        [-0.0317, -0.0555,  0.0068,  ..., -0.0299,  0.0293,  0.0163],\n",
            "        [ 0.0334,  0.0238, -0.0171,  ..., -0.0378,  0.0362,  0.0075],\n",
            "        ...,\n",
            "        [-0.0065,  0.0292, -0.0236,  ...,  0.0108,  0.0035,  0.0074],\n",
            "        [ 0.0331, -0.0266,  0.0046,  ..., -0.0229, -0.0230,  0.0093],\n",
            "        [-0.0714, -0.0033,  0.0480,  ...,  0.0334,  0.0062, -0.0297]])), ('transformer_encoder.layers.30.linear1.bias', tensor([ 0.0117, -0.0088, -0.0338,  ..., -0.0090, -0.0310, -0.0076])), ('transformer_encoder.layers.30.linear2.weight', tensor([[ 0.0322,  0.0061, -0.0204,  ..., -0.0047,  0.0204,  0.0134],\n",
            "        [ 0.0123, -0.0155, -0.0064,  ...,  0.0349,  0.0392,  0.0321],\n",
            "        [-0.0220, -0.0185,  0.0269,  ...,  0.0055,  0.0135,  0.0258],\n",
            "        ...,\n",
            "        [-0.0046, -0.0144, -0.0365,  ..., -0.0067, -0.0002,  0.0061],\n",
            "        [ 0.0117,  0.0005,  0.0028,  ...,  0.0008,  0.0168,  0.0186],\n",
            "        [ 0.0297, -0.0093,  0.0210,  ...,  0.0233,  0.0105,  0.0052]])), ('transformer_encoder.layers.30.linear2.bias', tensor([-0.0164, -0.0341, -0.0113,  ..., -0.0081,  0.0033, -0.0319])), ('transformer_encoder.layers.30.norm1.weight', tensor([0.9600, 1.0105, 0.9950,  ..., 1.0121, 0.9593, 0.9733])), ('transformer_encoder.layers.30.norm1.bias', tensor([-0.0007,  0.0310, -0.0079,  ...,  0.0124, -0.0028,  0.0232])), ('transformer_encoder.layers.30.norm2.weight', tensor([0.9826, 1.0138, 0.9969,  ..., 1.0029, 0.9751, 0.9917])), ('transformer_encoder.layers.30.norm2.bias', tensor([-0.0020,  0.0059,  0.0070,  ..., -0.0141,  0.0015, -0.0019])), ('transformer_encoder.layers.31.self_attn.in_proj_weight', tensor([[ 0.0044, -0.0208,  0.0169,  ...,  0.0059,  0.0146,  0.0274],\n",
            "        [ 0.0138, -0.0482,  0.0298,  ...,  0.0332, -0.0102,  0.0198],\n",
            "        [-0.0446,  0.0063,  0.0105,  ..., -0.0282,  0.0158,  0.0463],\n",
            "        ...,\n",
            "        [ 0.0009,  0.0305, -0.0516,  ..., -0.0041,  0.0185,  0.0280],\n",
            "        [-0.0022, -0.0061, -0.0164,  ...,  0.0163, -0.0174, -0.0058],\n",
            "        [-0.0155, -0.0155,  0.0195,  ..., -0.0051,  0.0178, -0.0161]])), ('transformer_encoder.layers.31.self_attn.in_proj_bias', tensor([ 0.0115, -0.0148, -0.0059,  ...,  0.0045, -0.0006, -0.0028])), ('transformer_encoder.layers.31.self_attn.out_proj.weight', tensor([[-0.0234, -0.0015, -0.0034,  ...,  0.0053, -0.0088,  0.0207],\n",
            "        [ 0.0145,  0.0081,  0.0168,  ..., -0.0030,  0.0028,  0.0138],\n",
            "        [-0.0203, -0.0001, -0.0113,  ...,  0.0302,  0.0096, -0.0260],\n",
            "        ...,\n",
            "        [ 0.0004,  0.0135, -0.0022,  ...,  0.0169,  0.0262, -0.0084],\n",
            "        [-0.0002,  0.0090, -0.0248,  ..., -0.0198,  0.0038, -0.0207],\n",
            "        [ 0.0075, -0.0029,  0.0379,  ..., -0.0154,  0.0134, -0.0055]])), ('transformer_encoder.layers.31.self_attn.out_proj.bias', tensor([-0.0003,  0.0267, -0.0048,  ...,  0.0041, -0.0007,  0.0175])), ('transformer_encoder.layers.31.linear1.weight', tensor([[-0.0183,  0.0081,  0.0069,  ...,  0.0182,  0.0048, -0.0229],\n",
            "        [-0.0015, -0.0114,  0.0139,  ..., -0.0281,  0.0124,  0.0065],\n",
            "        [ 0.0369,  0.0116, -0.0283,  ..., -0.0475,  0.0210,  0.0130],\n",
            "        ...,\n",
            "        [-0.0040,  0.0242, -0.0009,  ...,  0.0170, -0.0149, -0.0020],\n",
            "        [ 0.0582, -0.0201,  0.0262,  ..., -0.0125, -0.0081,  0.0384],\n",
            "        [-0.0475, -0.0200,  0.0086,  ...,  0.0268,  0.0224, -0.0378]])), ('transformer_encoder.layers.31.linear1.bias', tensor([ 0.0161, -0.0131, -0.0373,  ..., -0.0118, -0.0276, -0.0089])), ('transformer_encoder.layers.31.linear2.weight', tensor([[-4.6476e-05, -1.2129e-02,  4.2244e-03,  ...,  1.9187e-03,\n",
            "          2.5099e-02,  3.7045e-02],\n",
            "        [ 6.9010e-03,  1.7458e-02,  1.6846e-02,  ...,  2.5046e-02,\n",
            "         -1.4494e-02, -4.9217e-03],\n",
            "        [-5.5921e-03, -7.5369e-03, -6.1655e-03,  ...,  1.2426e-02,\n",
            "          5.2253e-03,  2.4288e-03],\n",
            "        ...,\n",
            "        [-9.3335e-03,  1.5492e-02, -3.0673e-02,  ...,  1.5985e-03,\n",
            "          9.1155e-03,  4.4216e-04],\n",
            "        [ 1.5788e-02, -9.8119e-03, -2.9272e-02,  ...,  1.1127e-02,\n",
            "          5.0185e-02,  1.1977e-02],\n",
            "        [ 1.5762e-02,  1.6621e-02, -1.6816e-02,  ...,  5.5956e-03,\n",
            "          7.4215e-03,  1.0612e-02]])), ('transformer_encoder.layers.31.linear2.bias', tensor([-0.0208, -0.0381, -0.0196,  ..., -0.0063,  0.0132, -0.0422])), ('transformer_encoder.layers.31.norm1.weight', tensor([0.9685, 1.0247, 0.9934,  ..., 1.0141, 0.9667, 0.9887])), ('transformer_encoder.layers.31.norm1.bias', tensor([-0.0015,  0.0365, -0.0098,  ...,  0.0099, -0.0042,  0.0282])), ('transformer_encoder.layers.31.norm2.weight', tensor([0.9890, 1.0202, 1.0021,  ..., 1.0079, 0.9901, 0.9997])), ('transformer_encoder.layers.31.norm2.bias', tensor([-0.0035,  0.0043,  0.0109,  ..., -0.0199, -0.0015, -0.0028])), ('transformer_encoder.layers.32.self_attn.in_proj_weight', tensor([[ 1.7364e-02, -2.3079e-02,  1.9037e-02,  ..., -2.6358e-03,\n",
            "          7.2218e-03,  3.8258e-02],\n",
            "        [ 3.1300e-02, -4.7843e-02,  2.7608e-02,  ...,  4.0710e-02,\n",
            "         -4.0289e-02,  2.0854e-02],\n",
            "        [-5.6931e-02,  9.9880e-03,  1.2490e-02,  ..., -5.2865e-02,\n",
            "          1.4391e-02,  2.4228e-02],\n",
            "        ...,\n",
            "        [-8.1679e-04,  3.7201e-02, -3.7046e-02,  ..., -6.5455e-03,\n",
            "          1.7347e-02,  3.9389e-02],\n",
            "        [-2.1603e-03, -8.7021e-03, -2.7253e-02,  ...,  1.0697e-02,\n",
            "         -1.9831e-02,  5.5507e-05],\n",
            "        [-8.9453e-03, -9.8212e-03,  9.6359e-03,  ...,  5.5957e-03,\n",
            "          2.4752e-03, -1.2089e-02]])), ('transformer_encoder.layers.32.self_attn.in_proj_bias', tensor([-0.0004, -0.0199, -0.0054,  ...,  0.0017, -0.0008, -0.0002])), ('transformer_encoder.layers.32.self_attn.out_proj.weight', tensor([[-2.7234e-02, -9.2748e-03,  1.4822e-02,  ...,  8.2937e-03,\n",
            "         -2.4196e-03,  1.6898e-02],\n",
            "        [ 7.8737e-03,  1.4032e-02,  2.6702e-02,  ...,  1.4188e-02,\n",
            "          8.5883e-03,  2.5368e-02],\n",
            "        [-2.3277e-02,  3.0662e-03, -8.7867e-05,  ...,  2.0060e-02,\n",
            "          7.1855e-03, -2.1730e-02],\n",
            "        ...,\n",
            "        [ 1.1229e-02,  1.3810e-02, -3.2373e-03,  ...,  2.0119e-02,\n",
            "          1.1606e-02, -1.5305e-02],\n",
            "        [-7.6761e-03,  1.6127e-02, -3.5329e-02,  ..., -1.7555e-02,\n",
            "          3.9787e-03, -9.7274e-03],\n",
            "        [ 4.1208e-03, -6.9794e-03,  3.3506e-02,  ...,  7.8280e-03,\n",
            "          2.5605e-02,  5.6086e-03]])), ('transformer_encoder.layers.32.self_attn.out_proj.bias', tensor([-0.0005,  0.0254,  0.0017,  ...,  0.0058, -0.0040,  0.0179])), ('transformer_encoder.layers.32.linear1.weight', tensor([[-0.0360,  0.0276,  0.0060,  ...,  0.0071,  0.0233, -0.0328],\n",
            "        [ 0.0147, -0.0289,  0.0239,  ..., -0.0011,  0.0097,  0.0018],\n",
            "        [ 0.0262,  0.0113, -0.0113,  ..., -0.0505,  0.0210,  0.0091],\n",
            "        ...,\n",
            "        [ 0.0158,  0.0299,  0.0040,  ...,  0.0237,  0.0187, -0.0036],\n",
            "        [ 0.0416, -0.0262,  0.0158,  ...,  0.0037, -0.0023,  0.0136],\n",
            "        [-0.0108, -0.0161,  0.0117,  ...,  0.0247, -0.0041, -0.0374]])), ('transformer_encoder.layers.32.linear1.bias', tensor([ 0.0071, -0.0153, -0.0384,  ..., -0.0147, -0.0305, -0.0093])), ('transformer_encoder.layers.32.linear2.weight', tensor([[-0.0074, -0.0240, -0.0149,  ..., -0.0034,  0.0260, -0.0053],\n",
            "        [ 0.0194, -0.0046,  0.0047,  ..., -0.0012,  0.0333,  0.0154],\n",
            "        [ 0.0216,  0.0265,  0.0267,  ...,  0.0013,  0.0223,  0.0192],\n",
            "        ...,\n",
            "        [-0.0172, -0.0081, -0.0250,  ..., -0.0234,  0.0026,  0.0236],\n",
            "        [ 0.0193,  0.0270,  0.0008,  ..., -0.0171,  0.0062,  0.0035],\n",
            "        [ 0.0444,  0.0030,  0.0058,  ...,  0.0260, -0.0134,  0.0131]])), ('transformer_encoder.layers.32.linear2.bias', tensor([-0.0104, -0.0184, -0.0175,  ...,  0.0109, -0.0010, -0.0217])), ('transformer_encoder.layers.32.norm1.weight', tensor([0.9821, 1.0379, 0.9947,  ..., 1.0187, 0.9882, 1.0060])), ('transformer_encoder.layers.32.norm1.bias', tensor([-0.0051,  0.0428, -0.0022,  ...,  0.0036, -0.0101,  0.0309])), ('transformer_encoder.layers.32.norm2.weight', tensor([0.9927, 1.0077, 1.0052,  ..., 1.0166, 1.0039, 0.9791])), ('transformer_encoder.layers.32.norm2.bias', tensor([-0.0071, -0.0204,  0.0150,  ..., -0.0227,  0.0088, -0.0298])), ('decoder.0.0.weight', tensor([[-2.0038e-02,  2.5654e-02,  2.0129e-02,  ...,  3.0429e-02,\n",
            "         -8.8774e-03, -1.4654e-02],\n",
            "        [-4.3903e-05, -1.6769e-02, -9.6141e-03,  ..., -1.5721e-02,\n",
            "          2.0136e-02, -1.1666e-02],\n",
            "        [-1.3730e-02,  2.3098e-02, -1.1099e-02,  ...,  4.0468e-02,\n",
            "         -2.5593e-02,  5.2922e-02],\n",
            "        ...,\n",
            "        [-2.8808e-02, -5.0558e-03, -3.4145e-03,  ...,  9.1538e-03,\n",
            "         -1.9189e-02, -3.2736e-02],\n",
            "        [-1.4992e-02,  1.7029e-02,  2.9053e-02,  ..., -3.7816e-02,\n",
            "         -3.6349e-02,  1.7276e-02],\n",
            "        [-6.6262e-03,  2.7902e-03, -2.6776e-03,  ..., -1.7447e-02,\n",
            "         -2.6528e-02,  4.9182e-04]])), ('decoder.0.0.bias', tensor([ 0.0261,  0.0360, -0.0085,  ...,  0.0142, -0.0153,  0.0182])), ('decoder.0.1.weight', tensor([0.9973, 0.9796, 0.9631,  ..., 1.0297, 1.0001, 0.9667])), ('decoder.0.1.bias', tensor([-0.0258, -0.0096, -0.0228,  ..., -0.0324, -0.0282, -0.0055])), ('decoder.1.0.weight', tensor([[ 0.0095, -0.0214,  0.0033,  ..., -0.0029,  0.0129,  0.0114],\n",
            "        [ 0.0232, -0.0098, -0.0188,  ...,  0.0258, -0.0214,  0.0210],\n",
            "        [-0.0010,  0.0191, -0.0291,  ...,  0.0135,  0.0244,  0.0124],\n",
            "        ...,\n",
            "        [ 0.0066, -0.0082, -0.0198,  ...,  0.0241, -0.0014, -0.0314],\n",
            "        [ 0.0065,  0.0424,  0.0376,  ...,  0.0273,  0.0248,  0.0181],\n",
            "        [ 0.0393, -0.0104, -0.0377,  ...,  0.0250,  0.0043, -0.0044]])), ('decoder.1.0.bias', tensor([-0.0043, -0.0128,  0.0150,  ...,  0.0006,  0.0135,  0.0049])), ('decoder.1.1.weight', tensor([1.0037, 1.0211, 0.9852,  ..., 0.9706, 0.9708, 0.9385])), ('decoder.1.1.bias', tensor([-0.0791, -0.0783, -0.0698,  ..., -0.0810, -0.0628, -0.0449])), ('decoder.2.0.weight', tensor([[-0.0090,  0.0106,  0.0046,  ...,  0.0197, -0.0143, -0.0055],\n",
            "        [-0.0290, -0.0010, -0.0193,  ...,  0.0334,  0.0269, -0.0059],\n",
            "        [ 0.0178,  0.0451, -0.0001,  ..., -0.0068,  0.0319, -0.0091],\n",
            "        ...,\n",
            "        [-0.0067, -0.0050, -0.0044,  ..., -0.0479,  0.0134,  0.0066],\n",
            "        [-0.0397,  0.0143, -0.0166,  ..., -0.0286,  0.0160, -0.0067],\n",
            "        [-0.0028,  0.0273, -0.0268,  ...,  0.0003, -0.0236,  0.0397]])), ('decoder.2.0.bias', tensor([ 0.0140,  0.0364,  0.0105,  ..., -0.0243,  0.0252,  0.0069])), ('decoder.2.1.weight', tensor([0.9317, 0.9378, 1.0097,  ..., 1.0103, 0.9397, 0.9175])), ('decoder.2.1.bias', tensor([-0.1869, -0.2114, -0.1784,  ..., -0.1912, -0.1274, -0.1565])), ('decoder.3.weight', tensor([[-0.0244,  0.0167,  0.0238,  ...,  0.0061,  0.0100, -0.0398],\n",
            "        [-0.0217,  0.0313, -0.0231,  ...,  0.0225,  0.0055, -0.0051],\n",
            "        [-0.0108, -0.0013, -0.0184,  ..., -0.0180, -0.0040, -0.0426],\n",
            "        ...,\n",
            "        [-0.0057,  0.0277,  0.0003,  ..., -0.0296,  0.0051, -0.0212],\n",
            "        [ 0.0169,  0.0048,  0.0295,  ...,  0.0125, -0.0065,  0.0120],\n",
            "        [ 0.0294, -0.0042, -0.0312,  ..., -0.0348, -0.0216, -0.0081]])), ('decoder.3.bias', tensor([ 0.0076, -0.0322,  0.0220,  ...,  0.0237,  0.0014,  0.0296])), ('binary_decoder.0.0.weight', tensor([[ 0.0124, -0.0320, -0.0129,  ...,  0.0826,  0.0273, -0.0433],\n",
            "        [ 0.0105, -0.0206, -0.0039,  ..., -0.0534, -0.0284, -0.0477],\n",
            "        [-0.0230, -0.0170,  0.0063,  ..., -0.0222,  0.0323,  0.0418],\n",
            "        ...,\n",
            "        [-0.0108,  0.0257, -0.0145,  ..., -0.0274,  0.0134, -0.0105],\n",
            "        [ 0.0286,  0.0377,  0.0189,  ..., -0.0182, -0.0154,  0.0245],\n",
            "        [-0.0087, -0.0262, -0.0035,  ..., -0.0074,  0.0060,  0.0034]])), ('binary_decoder.0.0.bias', tensor([0.0226, 0.0360, 0.0267,  ..., 0.0002, 0.0143, 0.0124])), ('binary_decoder.0.1.weight', tensor([0.8399, 1.1321, 1.2598,  ..., 1.0272, 0.8824, 1.3780])), ('binary_decoder.0.1.bias', tensor([-0.9468, -0.5090, -0.3603,  ..., -0.6483, -0.7688, -0.4420])), ('binary_decoder.1.0.weight', tensor([[ 0.0166, -0.0254,  0.0507,  ..., -0.0072, -0.0144, -0.0252],\n",
            "        [ 0.0027, -0.0094,  0.0134,  ...,  0.0186, -0.0023,  0.0173],\n",
            "        [ 0.0479,  0.0440,  0.0062,  ...,  0.0080,  0.0310, -0.0446],\n",
            "        ...,\n",
            "        [ 0.0077, -0.0032, -0.0220,  ...,  0.0179,  0.0318, -0.0173],\n",
            "        [-0.0130,  0.0096, -0.0101,  ...,  0.0278,  0.0001, -0.0344],\n",
            "        [ 0.0252,  0.0175,  0.0125,  ...,  0.0094,  0.0182,  0.0014]])), ('binary_decoder.1.0.bias', tensor([ 6.7627e-02, -2.2849e-02, -1.2150e-04,  3.6635e-02, -2.7945e-03,\n",
            "         1.0508e-01, -3.5867e-02, -3.8205e-02,  2.8041e-02,  1.3932e-02,\n",
            "        -1.1655e-02,  2.5270e-02,  6.9934e-02,  5.4886e-02,  2.0316e-02,\n",
            "        -6.5882e-03,  5.7712e-02,  2.0832e-02,  4.9808e-02,  9.0213e-02,\n",
            "        -2.7730e-02,  3.5324e-02,  1.0120e-02,  2.1213e-02, -2.1070e-02,\n",
            "        -4.9097e-03, -1.6453e-02,  3.7329e-02, -1.9905e-02,  1.3805e-01,\n",
            "         2.5984e-02,  3.7746e-02,  6.7069e-02,  8.1790e-02, -1.0270e-02,\n",
            "        -2.4559e-02,  1.0005e-02, -1.3981e-02, -3.5379e-04,  2.3583e-03,\n",
            "         1.7495e-01, -8.5727e-02,  2.5243e-02,  1.7382e-02,  4.1480e-02,\n",
            "        -4.6269e-02,  6.8179e-02,  1.4776e-01,  3.6739e-02, -2.1984e-02,\n",
            "         2.6777e-02,  7.5200e-02,  1.0889e-02, -3.8415e-02, -3.7554e-02,\n",
            "        -5.0205e-03, -6.2557e-02,  5.5065e-02,  1.2307e-01, -4.2999e-02,\n",
            "        -2.8440e-03,  3.2760e-02, -2.8240e-04,  5.4039e-02,  3.5162e-02,\n",
            "        -1.9712e-02,  6.0554e-02, -2.7376e-02,  9.3224e-02,  1.0127e-01,\n",
            "        -4.4653e-02, -1.8535e-02,  3.3510e-02,  1.9930e-02, -1.9187e-02,\n",
            "        -2.1136e-02,  5.2417e-02, -3.0063e-02,  3.0855e-03,  3.1456e-02,\n",
            "        -2.0503e-02,  1.3929e-01,  7.2036e-02, -1.2156e-02,  2.1198e-02,\n",
            "        -4.2001e-02,  9.7883e-04,  7.1357e-03,  6.4497e-03,  7.6372e-03,\n",
            "         1.2370e-02,  5.4849e-03,  1.0146e-02,  6.1354e-02, -5.9139e-03,\n",
            "         3.6193e-02, -5.8062e-03,  2.0260e-02, -1.6427e-02,  5.4253e-02,\n",
            "         9.3938e-02, -1.8984e-02,  2.7438e-03,  8.2752e-04, -1.4478e-02,\n",
            "         2.2954e-02, -1.0246e-02,  1.3941e-01,  1.0153e-01, -5.9483e-02,\n",
            "         3.4090e-03,  3.1894e-02,  2.9550e-02,  1.4160e-03,  1.5419e-02,\n",
            "         2.9549e-02,  1.1482e-01, -3.1798e-02,  5.9207e-02, -2.3338e-02,\n",
            "         1.4304e-02, -1.6362e-02,  8.0810e-02,  3.3351e-02, -1.2523e-02,\n",
            "         5.5679e-02,  8.0816e-03,  1.4303e-01,  1.6669e-03,  3.8534e-03,\n",
            "        -1.6466e-02,  8.0639e-02, -7.7095e-03, -4.5677e-03,  8.0939e-02,\n",
            "        -1.1016e-02, -3.1207e-02,  3.3305e-02,  5.5364e-02,  7.9400e-03,\n",
            "         4.3714e-02,  3.4422e-02, -7.4196e-03, -2.6441e-02,  2.8694e-02,\n",
            "         9.5337e-02,  7.2337e-02,  4.1338e-02,  3.5401e-03,  6.1369e-02,\n",
            "        -1.9829e-02,  1.3173e-01, -2.2884e-02, -1.0001e-02,  7.3787e-02,\n",
            "        -7.6069e-03, -1.7468e-03,  6.0906e-04, -2.7236e-02,  9.8003e-03,\n",
            "        -4.9104e-02,  5.3183e-02,  1.5373e-03, -1.9413e-02, -1.7618e-02,\n",
            "         2.3970e-02, -5.5178e-02,  9.0560e-02,  1.5215e-01,  5.9992e-03,\n",
            "         6.4463e-02, -6.3524e-03,  1.2322e-01,  2.3628e-02, -7.8213e-03,\n",
            "         7.1541e-04,  2.9398e-02, -3.6782e-03,  2.5137e-02,  2.1240e-03,\n",
            "         1.2018e-01,  7.5296e-02,  4.4739e-02, -2.4701e-02,  4.2459e-02,\n",
            "         3.6561e-02,  3.1776e-02,  5.5482e-02, -3.7937e-02,  2.8965e-02,\n",
            "        -1.1494e-02, -2.3746e-03,  1.3248e-01,  1.1681e-01,  2.5502e-02,\n",
            "        -1.7155e-02, -2.6491e-03,  1.3847e-03, -5.6751e-02,  7.2737e-03,\n",
            "         6.7883e-02,  1.0394e-01,  4.5283e-02, -1.3411e-02, -2.9688e-02,\n",
            "         5.7510e-02,  1.9556e-02, -2.6876e-02, -4.3649e-02, -5.7122e-02,\n",
            "         1.2759e-01, -4.6110e-02, -2.5813e-02, -1.3909e-02,  4.1648e-02,\n",
            "         2.7356e-02,  1.4394e-03,  8.8917e-02,  4.8268e-03, -1.1646e-02,\n",
            "        -1.9400e-02,  1.3903e-02, -1.2735e-03, -6.7557e-02,  9.1191e-02,\n",
            "         5.1222e-02,  2.8358e-03,  7.8942e-02,  7.2970e-02,  1.0916e-02,\n",
            "         4.6123e-02,  4.9756e-03,  3.1336e-02,  3.6909e-02,  3.3596e-02,\n",
            "         1.0772e-02,  4.1929e-02,  3.0042e-02,  3.5311e-02, -1.7310e-02,\n",
            "         1.2728e-02, -1.3966e-02,  7.8137e-02, -1.4659e-02,  5.1150e-02,\n",
            "        -2.4328e-02,  1.1596e-01,  8.6547e-02, -7.7433e-03, -1.9332e-02,\n",
            "        -1.2394e-02,  3.6902e-02,  5.1605e-03,  8.6549e-02,  3.4094e-02,\n",
            "         3.0763e-02,  2.1390e-02, -1.0900e-02,  1.0586e-02,  3.1746e-02,\n",
            "        -2.1341e-02, -3.0436e-02,  2.0968e-02,  1.2737e-01, -6.4311e-03,\n",
            "        -3.2522e-02,  1.2706e-03,  5.5709e-03, -1.6568e-02, -3.2550e-03,\n",
            "         6.4981e-02,  2.3896e-02,  9.7369e-02,  1.2045e-02, -3.3620e-02,\n",
            "         4.4947e-02,  8.2873e-02, -7.6367e-04, -2.1723e-02, -1.9760e-02,\n",
            "        -4.3079e-03,  6.3359e-02,  3.3198e-02,  1.2788e-02,  2.9087e-02,\n",
            "        -1.0244e-02,  8.5036e-02, -5.6687e-02,  2.7263e-02, -5.1753e-02,\n",
            "        -1.8492e-02,  5.3674e-02, -8.4909e-03,  5.1807e-02,  7.1088e-03,\n",
            "         3.3454e-02, -3.3551e-02,  5.7634e-02,  7.6158e-02,  1.4642e-02,\n",
            "        -1.3136e-02,  1.0805e-01,  5.4748e-02,  5.3388e-02, -8.7486e-03,\n",
            "         4.3831e-02,  1.0071e-02,  2.5258e-02, -2.4269e-02, -2.4297e-02,\n",
            "        -9.0824e-03, -1.9511e-02, -4.5608e-03, -2.8110e-03, -2.8366e-02,\n",
            "        -3.5713e-02, -1.3409e-02,  1.3769e-01,  1.1789e-02,  9.5114e-02,\n",
            "         1.2100e-01,  4.0182e-03, -2.2441e-02, -2.0365e-02, -2.7735e-02,\n",
            "        -2.9049e-02,  1.2508e-03,  4.1195e-02, -5.3551e-03,  3.8964e-03,\n",
            "         5.3291e-02,  5.5457e-03, -4.8242e-02,  3.9110e-02, -3.7992e-03,\n",
            "        -1.0423e-02,  3.1258e-02, -2.4422e-02,  9.0485e-03,  1.2415e-02,\n",
            "         1.1633e-01,  3.6105e-02, -7.0997e-03,  3.5687e-02, -1.9635e-02,\n",
            "        -2.7335e-02,  7.0860e-02, -3.2597e-03, -2.6543e-02,  3.8218e-02,\n",
            "         1.4281e-02,  3.1295e-02,  9.9629e-02,  9.6856e-02,  1.3054e-01,\n",
            "        -2.4111e-02,  3.0579e-02,  1.0721e-02,  5.6763e-02,  6.2900e-03,\n",
            "         4.3007e-03,  7.9394e-02, -2.9344e-02,  1.4183e-02, -4.0242e-03,\n",
            "        -4.3333e-03,  8.4822e-03,  8.2747e-03,  4.5406e-02, -2.9130e-02,\n",
            "         3.1416e-02, -4.5393e-02,  1.9440e-02,  3.7787e-02, -1.0470e-02,\n",
            "         1.2434e-03,  2.4775e-02, -2.4348e-02,  4.2330e-03, -2.4181e-02,\n",
            "        -3.3149e-02, -2.7648e-02, -6.7857e-03, -1.1383e-02, -3.0010e-02,\n",
            "         2.8651e-04,  1.5195e-02,  1.1337e-01,  1.0671e-02,  6.0644e-02,\n",
            "         1.5779e-01,  9.7150e-02, -3.9352e-02,  8.3885e-02, -4.4864e-02,\n",
            "        -6.2242e-02,  7.4887e-02,  1.5421e-02, -1.3725e-03,  7.0995e-03,\n",
            "         3.0224e-02,  7.8631e-02, -1.0950e-02,  3.4112e-02,  2.9181e-03,\n",
            "        -3.9225e-02, -1.1832e-02,  5.5111e-02, -7.4314e-03, -3.0470e-02,\n",
            "         1.5020e-01, -3.5683e-02,  2.4367e-02,  6.1723e-02, -1.8714e-02,\n",
            "         3.5404e-03, -2.2037e-02,  3.4270e-02,  1.1416e-02, -4.8141e-02,\n",
            "         4.4860e-02, -1.3405e-03,  3.4263e-03,  1.3853e-02, -3.1337e-02,\n",
            "         1.9247e-02,  2.1051e-03,  5.7202e-02,  5.6018e-02,  7.7638e-02,\n",
            "        -3.1072e-03,  1.0301e-01,  2.0010e-02,  4.0739e-02,  1.0354e-01,\n",
            "         2.0797e-02,  1.6686e-02, -1.5351e-02, -2.0251e-03, -2.4529e-02,\n",
            "         6.0288e-02,  9.7139e-02,  2.8395e-02,  3.0868e-03, -8.5958e-03,\n",
            "         1.9567e-01,  4.0023e-02, -2.2171e-02,  3.2182e-03, -1.9420e-02,\n",
            "         1.2744e-02, -1.6535e-02, -4.1479e-02,  9.1224e-02,  4.8291e-02,\n",
            "         9.6052e-02,  5.6626e-02,  9.5233e-03,  1.0165e-02,  1.0991e-01,\n",
            "         7.3595e-02,  2.3263e-02,  4.7188e-03,  5.9375e-02,  8.2943e-02,\n",
            "         3.1643e-02, -1.6426e-02,  9.9766e-02,  8.7654e-02,  1.2309e-02,\n",
            "         6.8534e-03,  1.4496e-01, -1.9508e-02,  1.6739e-02,  3.9497e-02,\n",
            "        -3.9441e-02, -3.6894e-02,  6.4848e-02, -6.8034e-03,  7.5713e-02,\n",
            "        -1.1273e-02, -2.3158e-03,  2.8335e-02,  3.4301e-02,  5.4812e-02,\n",
            "        -2.5705e-02,  9.2106e-02,  8.2179e-02, -3.9914e-02,  1.4186e-02,\n",
            "         4.5582e-02,  8.5890e-03,  1.2007e-02,  1.6657e-01, -1.0374e-02,\n",
            "        -2.4647e-02,  4.3673e-02, -3.6050e-02,  3.6838e-02, -1.0439e-02,\n",
            "         5.3637e-02,  6.7097e-03, -1.3651e-02,  3.4204e-02, -2.5492e-02,\n",
            "         4.1399e-02,  4.2011e-02,  2.1788e-02,  4.3599e-02,  3.8034e-02,\n",
            "         7.5714e-03,  8.3272e-03])), ('binary_decoder.1.1.weight', tensor([0.8017, 1.3282, 1.2183, 0.9259, 1.2140, 0.8883, 1.3156, 1.3131, 0.8740,\n",
            "        0.9691, 1.3034, 0.9819, 0.9076, 0.9682, 1.1075, 1.2401, 0.7915, 1.0335,\n",
            "        0.8227, 0.9653, 1.3642, 0.8788, 1.2236, 1.0550, 1.0923, 1.3573, 1.3376,\n",
            "        1.0355, 1.1647, 0.8001, 1.1723, 1.0168, 0.8520, 0.7613, 1.3819, 1.3411,\n",
            "        1.2856, 1.3545, 0.9553, 1.1756, 0.7171, 1.2383, 1.1680, 1.2664, 0.7927,\n",
            "        1.2158, 1.0638, 0.7877, 1.1653, 1.3734, 0.6579, 0.6515, 1.0357, 1.3484,\n",
            "        1.2681, 1.2873, 0.8572, 0.8221, 0.7538, 1.3344, 1.1397, 0.9059, 1.3428,\n",
            "        0.9813, 0.8543, 1.3214, 1.0554, 1.3604, 0.6372, 0.9427, 1.3754, 1.3335,\n",
            "        1.1400, 1.3082, 1.3944, 1.1105, 0.8579, 1.4442, 1.3211, 0.9328, 1.3237,\n",
            "        0.7534, 0.9029, 1.2772, 1.2898, 1.3572, 1.0655, 1.3460, 1.1755, 1.0504,\n",
            "        1.2902, 1.2366, 1.2473, 1.0833, 1.2593, 0.9847, 1.3437, 1.1551, 1.3392,\n",
            "        0.7535, 0.9123, 1.3676, 1.2919, 1.1566, 1.2815, 1.0799, 1.3079, 0.7909,\n",
            "        0.9141, 1.4151, 1.2565, 1.0949, 0.8836, 1.3327, 1.1283, 1.0492, 0.6905,\n",
            "        1.2990, 0.8735, 1.3508, 1.0475, 1.2774, 0.7925, 1.0711, 1.0976, 0.8436,\n",
            "        1.2856, 0.7951, 1.3223, 1.3657, 1.3209, 0.7655, 1.1834, 1.3048, 0.9159,\n",
            "        1.0348, 1.3907, 0.9827, 1.1998, 1.0670, 1.1763, 1.1636, 1.2639, 1.3503,\n",
            "        1.1921, 0.8179, 0.8830, 0.7830, 1.2867, 0.8223, 1.4147, 0.6798, 1.3819,\n",
            "        1.2253, 0.9839, 1.3183, 1.2721, 0.8846, 1.2419, 1.2776, 1.3838, 0.9715,\n",
            "        1.1425, 1.3588, 1.3025, 0.9953, 1.4000, 0.8318, 0.7670, 1.3510, 0.9548,\n",
            "        1.0262, 0.6883, 1.0827, 1.2532, 1.2627, 1.1103, 1.2517, 1.0434, 1.2134,\n",
            "        0.7516, 0.7997, 0.9644, 1.3718, 1.1863, 1.1642, 1.0804, 0.8524, 0.9805,\n",
            "        1.0647, 1.3612, 1.0246, 0.6619, 0.8023, 1.0779, 1.2784, 1.3299, 1.2928,\n",
            "        1.3795, 1.2756, 1.0186, 0.6611, 0.7377, 1.3039, 1.1495, 1.1187, 0.6886,\n",
            "        1.3240, 1.3697, 1.3306, 0.8098, 1.3821, 1.4022, 1.3003, 1.1835, 1.2987,\n",
            "        1.2726, 0.8514, 1.2969, 1.2811, 1.0720, 1.3074, 0.8086, 1.4051, 0.7385,\n",
            "        0.8692, 1.3947, 1.1159, 0.9893, 1.2762, 1.0745, 1.1566, 1.1424, 1.2106,\n",
            "        1.0173, 1.2494, 0.9115, 1.2083, 1.1252, 1.2491, 1.3348, 1.2455, 0.7869,\n",
            "        0.9153, 0.9028, 1.3355, 0.7843, 0.8690, 1.3512, 1.3460, 1.3316, 1.1920,\n",
            "        1.0827, 0.7136, 0.9919, 1.0819, 1.1420, 1.2860, 1.2419, 0.9449, 1.3572,\n",
            "        1.3582, 1.0872, 0.8245, 1.2068, 1.3377, 1.1081, 1.3125, 1.2049, 1.2590,\n",
            "        0.8521, 1.2461, 0.6909, 1.2884, 1.3747, 1.1438, 0.8148, 1.2203, 1.2919,\n",
            "        1.3882, 1.2890, 0.7368, 0.9518, 1.0499, 1.1034, 1.2801, 0.7540, 1.3713,\n",
            "        1.1465, 1.3699, 1.3751, 0.8756, 1.1016, 1.0014, 1.3139, 0.9432, 1.3177,\n",
            "        1.0826, 0.9000, 1.1376, 1.2085, 0.8676, 0.9523, 0.8318, 1.3690, 1.1076,\n",
            "        1.1450, 1.1718, 1.1075, 1.3152, 1.1729, 0.7157, 1.2487, 1.2744, 1.3098,\n",
            "        1.3442, 1.1622, 0.7559, 1.2610, 0.6922, 0.8317, 1.1682, 1.3724, 1.2715,\n",
            "        1.2839, 1.3503, 1.1549, 1.0811, 1.3289, 1.3140, 0.7824, 1.1654, 1.4005,\n",
            "        1.1393, 1.1599, 1.3043, 1.1497, 1.2460, 1.0184, 0.8092, 0.7379, 0.9823,\n",
            "        1.1627, 1.0649, 1.3110, 1.2779, 0.9569, 1.3094, 1.3056, 0.8479, 0.8936,\n",
            "        1.2871, 0.7141, 0.6515, 0.7598, 1.3332, 1.2056, 1.1100, 1.0388, 1.1074,\n",
            "        1.1668, 0.9492, 1.4034, 1.2047, 1.1556, 1.2583, 1.2130, 1.2902, 0.7737,\n",
            "        1.3686, 1.3187, 1.3768, 1.0712, 1.0764, 1.3459, 1.2925, 1.2616, 1.3691,\n",
            "        1.2499, 1.3654, 1.3215, 1.3586, 1.2543, 1.3635, 1.3906, 1.3494, 1.2604,\n",
            "        0.9145, 1.2596, 0.7717, 0.7307, 0.7988, 1.3941, 0.8690, 1.4044, 1.3266,\n",
            "        0.9658, 1.3386, 1.2298, 1.0936, 1.1234, 0.6681, 1.2844, 1.2213, 1.1709,\n",
            "        1.3739, 1.2446, 1.0608, 1.2488, 1.3196, 0.7176, 1.3705, 1.1424, 1.0136,\n",
            "        1.3849, 1.2884, 1.3470, 1.0586, 1.2062, 1.3171, 1.2191, 1.3096, 1.3427,\n",
            "        0.8892, 1.3450, 1.1679, 1.2442, 1.0547, 0.9643, 0.8175, 1.0540, 0.7333,\n",
            "        1.2863, 1.1684, 0.8511, 1.1044, 1.2886, 1.2081, 1.2370, 1.3124, 0.7714,\n",
            "        0.7405, 1.2047, 1.2794, 1.2980, 0.6719, 1.1639, 0.9600, 1.0863, 1.3499,\n",
            "        1.2985, 1.1794, 1.3778, 0.7010, 1.0661, 0.7846, 1.0543, 1.1793, 1.2588,\n",
            "        0.7636, 0.7370, 1.2128, 0.9450, 0.9758, 0.6859, 1.2144, 1.2933, 0.9299,\n",
            "        0.6841, 0.7629, 1.2107, 0.8031, 1.3236, 1.1546, 1.0352, 1.3279, 1.2882,\n",
            "        1.0034, 1.3425, 0.7026, 1.2873, 1.1816, 1.2744, 1.0429, 0.8747, 1.3580,\n",
            "        0.7242, 0.8536, 1.4054, 1.3013, 1.0176, 1.1051, 1.1308, 0.6768, 1.3610,\n",
            "        1.2858, 1.0771, 1.3171, 1.1031, 1.3434, 0.8749, 1.3359, 1.3142, 1.0150,\n",
            "        1.2815, 1.2348, 1.2323, 1.3407, 0.9865, 0.9791, 1.2692, 1.0748])), ('binary_decoder.1.1.bias', tensor([-0.6140, -0.3920, -0.4685, -0.4829, -0.4102, -0.7498, -0.2672, -0.2695,\n",
            "        -0.5246, -0.5152, -0.3488, -0.5440, -0.3696, -0.5176, -0.4119, -0.4066,\n",
            "        -0.7443, -0.4192, -0.6037, -0.5207, -0.3394, -0.5248, -0.3550, -0.5659,\n",
            "        -0.3294, -0.2870, -0.2937, -0.3806, -0.2927, -0.6341, -0.4483, -0.4287,\n",
            "        -0.5369, -0.6627, -0.3112, -0.3055, -0.3641, -0.4002, -0.4661, -0.4022,\n",
            "        -0.6730, -0.2569, -0.4015, -0.4765, -0.5654, -0.3585, -0.3691, -0.5674,\n",
            "        -0.4500, -0.3032, -0.6221, -0.6740, -0.3847, -0.2956, -0.2852, -0.3099,\n",
            "        -0.4705, -0.4638, -0.4782, -0.2853, -0.3795, -0.4574, -0.3544, -0.4709,\n",
            "        -0.5620, -0.3210, -0.3442, -0.2824, -0.6804, -0.5717, -0.2988, -0.3053,\n",
            "        -0.4220, -0.4034, -0.3218, -0.3086, -0.5264, -0.4634, -0.3898, -0.4814,\n",
            "        -0.3478, -0.6171, -0.4373, -0.2686, -0.3061, -0.3140, -0.4785, -0.3949,\n",
            "        -0.4504, -0.3103, -0.3473, -0.3304, -0.3860, -0.4104, -0.2665, -0.5259,\n",
            "        -0.3661, -0.3767, -0.3616, -0.6853, -0.6489, -0.3949, -0.4190, -0.3126,\n",
            "        -0.3604, -0.4913, -0.2936, -0.7400, -0.5800, -0.3048, -0.4276, -0.4160,\n",
            "        -0.4903, -0.2980, -0.5119, -0.4756, -0.6304, -0.3316, -0.5388, -0.2863,\n",
            "        -0.4564, -0.2847, -0.6346, -0.3493, -0.4573, -0.5494, -0.3478, -0.6075,\n",
            "        -0.2859, -0.2992, -0.3469, -0.5174, -0.3607, -0.3411, -0.6554, -0.4288,\n",
            "        -0.3116, -0.4328, -0.4566, -0.5783, -0.4514, -0.4585, -0.3282, -0.2920,\n",
            "        -0.4487, -0.5441, -0.4587, -0.4010, -0.3831, -0.4089, -0.3056, -0.6853,\n",
            "        -0.3018, -0.3198, -0.5256, -0.3644, -0.3894, -0.4274, -0.2815, -0.3663,\n",
            "        -0.3280, -0.6087, -0.4125, -0.2905, -0.3361, -0.5513, -0.3117, -0.7113,\n",
            "        -0.5243, -0.3039, -0.5859, -0.3567, -0.5902, -0.4276, -0.3506, -0.3311,\n",
            "        -0.4354, -0.3396, -0.3572, -0.3739, -0.6021, -0.5362, -0.4885, -0.2851,\n",
            "        -0.4224, -0.4881, -0.5338, -0.5891, -0.3471, -0.4543, -0.2809, -0.3782,\n",
            "        -0.5553, -0.5391, -0.3332, -0.4026, -0.3829, -0.3078, -0.3334, -0.3714,\n",
            "        -0.6336, -0.6216, -0.5514, -0.2746, -0.5516, -0.4887, -0.6680, -0.3169,\n",
            "        -0.3965, -0.3038, -0.7058, -0.2900, -0.3205, -0.4083, -0.3807, -0.4099,\n",
            "        -0.3813, -0.4745, -0.3193, -0.2913, -0.3142, -0.4146, -0.5449, -0.3290,\n",
            "        -0.4707, -0.3956, -0.4576, -0.5196, -0.3927, -0.3052, -0.4063, -0.3403,\n",
            "        -0.4472, -0.4192, -0.4047, -0.3328, -0.5625, -0.3904, -0.5067, -0.3231,\n",
            "        -0.3543, -0.2369, -0.4880, -0.5169, -0.6311, -0.3555, -0.5906, -0.4332,\n",
            "        -0.3097, -0.2938, -0.3276, -0.3804, -0.5170, -0.6468, -0.5637, -0.5188,\n",
            "        -0.3569, -0.3672, -0.3385, -0.4895, -0.2913, -0.3209, -0.3657, -0.5522,\n",
            "        -0.3261, -0.3325, -0.4102, -0.3949, -0.2587, -0.3991, -0.7072, -0.3716,\n",
            "        -0.7640, -0.3611, -0.3130, -0.3320, -0.5656, -0.3960, -0.3355, -0.2993,\n",
            "        -0.3403, -0.7463, -0.6180, -0.4412, -0.3918, -0.2910, -0.6507, -0.3288,\n",
            "        -0.3434, -0.3000, -0.3125, -0.5764, -0.3432, -0.3802, -0.4202, -0.4664,\n",
            "        -0.2938, -0.4968, -0.3976, -0.3877, -0.3024, -0.6290, -0.4295, -0.3715,\n",
            "        -0.2885, -0.3327, -0.3663, -0.4418, -0.2598, -0.3232, -0.3809, -0.5729,\n",
            "        -0.3603, -0.3018, -0.3626, -0.3137, -0.4029, -0.6556, -0.3972, -0.5883,\n",
            "        -0.5671, -0.3311, -0.3067, -0.2996, -0.3383, -0.2956, -0.4539, -0.4714,\n",
            "        -0.3013, -0.2850, -0.7298, -0.4301, -0.3167, -0.3582, -0.3379, -0.2766,\n",
            "        -0.4410, -0.3040, -0.4470, -0.5785, -0.6652, -0.5095, -0.2735, -0.5243,\n",
            "        -0.3415, -0.2680, -0.5369, -0.3834, -0.3373, -0.6233, -0.4740, -0.3967,\n",
            "        -0.7160, -0.6376, -0.4396, -0.3246, -0.4381, -0.4786, -0.5017, -0.4638,\n",
            "        -0.3291, -0.4206, -0.3279, -0.4060, -0.3102, -0.3198, -0.3571, -0.3354,\n",
            "        -0.5808, -0.2811, -0.3926, -0.3245, -0.4828, -0.3727, -0.3961, -0.3500,\n",
            "        -0.3917, -0.4261, -0.3601, -0.3221, -0.3118, -0.3116, -0.2738, -0.4532,\n",
            "        -0.3070, -0.2856, -0.3258, -0.5206, -0.2923, -0.7717, -0.6079, -0.5839,\n",
            "        -0.3331, -0.5979, -0.3066, -0.3245, -0.5267, -0.3855, -0.2900, -0.4054,\n",
            "        -0.4385, -0.6645, -0.3778, -0.4492, -0.3761, -0.3259, -0.2739, -0.4892,\n",
            "        -0.2648, -0.3729, -0.7032, -0.2872, -0.3960, -0.4927, -0.3066, -0.3149,\n",
            "        -0.3047, -0.5358, -0.4522, -0.3566, -0.3961, -0.3335, -0.4000, -0.5341,\n",
            "        -0.2790, -0.3023, -0.3748, -0.5148, -0.3941, -0.5161, -0.4046, -0.4511,\n",
            "        -0.3792, -0.3944, -0.4272, -0.4814, -0.3670, -0.2997, -0.4662, -0.2892,\n",
            "        -0.5074, -0.6418, -0.4284, -0.3390, -0.3203, -0.5592, -0.4381, -0.4061,\n",
            "        -0.3835, -0.3014, -0.3419, -0.3897, -0.3349, -0.7416, -0.4358, -0.6646,\n",
            "        -0.3885, -0.2998, -0.3203, -0.5390, -0.6956, -0.3595, -0.3966, -0.5657,\n",
            "        -0.5640, -0.3502, -0.3525, -0.5537, -0.5207, -0.5926, -0.3481, -0.5921,\n",
            "        -0.3399, -0.3245, -0.4523, -0.2870, -0.3292, -0.5154, -0.2876, -0.5480,\n",
            "        -0.2985, -0.4487, -0.3756, -0.4797, -0.5651, -0.3255, -0.5885, -0.5504,\n",
            "        -0.3064, -0.3093, -0.4923, -0.4655, -0.5431, -0.6041, -0.3157, -0.3704,\n",
            "        -0.4348, -0.2900, -0.3605, -0.3118, -0.5652, -0.3584, -0.3842, -0.5445,\n",
            "        -0.2535, -0.4176, -0.4179, -0.3643, -0.5898, -0.5027, -0.3793, -0.3453])), ('binary_decoder.2.0.weight', tensor([[ 0.0154,  0.0040, -0.0164,  ..., -0.0325, -0.0069, -0.0167],\n",
            "        [-0.0044, -0.0213,  0.0096,  ...,  0.0146,  0.0323, -0.0015],\n",
            "        [-0.0342,  0.0061,  0.0081,  ...,  0.0091, -0.0143,  0.0004],\n",
            "        ...,\n",
            "        [-0.0547, -0.0003, -0.0013,  ..., -0.0074, -0.0032, -0.0458],\n",
            "        [ 0.0245,  0.0152,  0.0298,  ..., -0.0212, -0.0032,  0.0043],\n",
            "        [-0.0177,  0.0074, -0.0286,  ..., -0.0216,  0.0258, -0.0140]])), ('binary_decoder.2.0.bias', tensor([-0.1166, -0.0819, -0.0050, -0.0171,  0.0417, -0.0484, -0.0852,  0.0570,\n",
            "         0.0733, -0.0733,  0.0843,  0.1714,  0.1638, -0.0614,  0.0236, -0.0500,\n",
            "        -0.0565, -0.0186, -0.0998,  0.0483,  0.1957,  0.0254, -0.0210, -0.0411,\n",
            "        -0.0114,  0.0444, -0.0745,  0.0157,  0.0517, -0.0668, -0.0173,  0.0433,\n",
            "        -0.0316,  0.0248,  0.0665,  0.0396, -0.0201,  0.0601, -0.0371, -0.1127,\n",
            "         0.0374, -0.1082, -0.0428,  0.0444, -0.0884,  0.0179, -0.0410, -0.1079,\n",
            "        -0.0435,  0.2383,  0.2963,  0.0499,  0.2147, -0.0398,  0.3106, -0.1246,\n",
            "         0.2675, -0.0473, -0.0884,  0.2598,  0.0139,  0.2470, -0.0264, -0.0110,\n",
            "         0.1931,  0.2736, -0.0240,  0.1686,  0.0628,  0.0528, -0.0310, -0.0264,\n",
            "         0.0132, -0.0008, -0.0454,  0.1865, -0.0768,  0.0347,  0.2219,  0.0035,\n",
            "         0.0633,  0.0043,  0.0262, -0.1626, -0.0893,  0.2262,  0.2685, -0.0810,\n",
            "        -0.0293, -0.0794,  0.0018,  0.0573, -0.0192, -0.0501,  0.0022, -0.0219,\n",
            "        -0.0682, -0.1066,  0.0458,  0.2620,  0.2839,  0.2280, -0.0200, -0.1093,\n",
            "        -0.0064, -0.0482, -0.0014,  0.0437, -0.0346,  0.2470, -0.0582, -0.0058,\n",
            "         0.0101, -0.0350, -0.1049, -0.0012, -0.0102,  0.0594,  0.2515, -0.0039,\n",
            "        -0.2385, -0.0626,  0.0618,  0.0089, -0.0176, -0.0917, -0.0075, -0.0750])), ('binary_decoder.2.1.weight', tensor([1.6685, 1.3789, 1.3196, 1.2936, 1.3342, 1.4655, 1.4320, 1.1472, 1.0698,\n",
            "        1.4567, 1.2097, 0.8791, 0.9276, 1.2961, 1.1045, 1.4769, 1.1472, 1.2012,\n",
            "        1.3347, 1.3190, 0.9431, 1.3270, 1.1965, 1.3206, 1.0872, 1.3217, 1.5326,\n",
            "        1.2840, 1.3808, 1.4745, 1.2766, 1.2502, 1.1211, 1.0844, 1.1080, 1.3710,\n",
            "        1.2344, 1.0477, 1.1737, 1.6424, 1.1999, 1.3916, 1.3778, 1.2764, 1.4198,\n",
            "        1.0839, 1.4247, 1.3618, 1.2152, 0.9490, 0.9648, 1.3885, 0.9838, 1.3790,\n",
            "        0.9670, 1.4004, 0.9869, 1.4479, 1.5908, 0.9628, 1.2880, 0.9472, 1.1743,\n",
            "        1.0969, 0.9541, 0.9022, 1.2503, 1.0009, 1.0414, 1.1122, 1.2609, 1.4442,\n",
            "        1.2542, 1.2376, 1.4197, 0.9858, 1.4705, 1.3543, 0.9540, 1.2397, 1.2267,\n",
            "        1.2926, 1.4360, 1.6602, 1.4529, 0.9891, 0.9466, 1.1764, 1.3157, 1.4380,\n",
            "        1.4186, 1.1197, 1.2909, 1.1920, 1.2484, 1.1278, 1.4843, 1.6595, 1.2508,\n",
            "        0.9796, 0.9495, 0.9795, 1.2680, 1.6237, 1.3456, 1.2751, 1.3185, 1.3797,\n",
            "        1.3624, 0.9821, 1.2582, 1.2994, 1.3301, 1.4925, 1.2331, 1.4353, 1.4728,\n",
            "        1.3133, 0.9019, 1.1366, 1.8859, 1.2591, 1.0579, 1.2474, 1.3297, 1.4883,\n",
            "        1.3352, 1.3998])), ('binary_decoder.2.1.bias', tensor([-0.0748, -0.0222, -0.0399, -0.0153, -0.1493, -0.2632, -0.0644, -0.1174,\n",
            "        -0.1072, -0.0604, -0.2249, -0.3057, -0.2418, -0.0223, -0.1385, -0.1200,\n",
            "        -0.0532, -0.0666, -0.0184, -0.1087, -0.0422, -0.0315, -0.0733, -0.1200,\n",
            "        -0.0902, -0.1683, -0.0561, -0.1001, -0.2409, -0.0137, -0.0389, -0.1140,\n",
            "        -0.0683, -0.1072, -0.1969, -0.1163, -0.0936, -0.1655, -0.0537, -0.1055,\n",
            "        -0.0434, -0.0328, -0.0478, -0.1798, -0.0519, -0.1595, -0.0481, -0.0565,\n",
            "        -0.0259, -0.2324, -0.0890, -0.1788,  0.0441, -0.0382, -0.0172, -0.0161,\n",
            "        -0.0055, -0.1010, -0.0978, -0.0716, -0.1808, -0.1475, -0.0707, -0.1366,\n",
            "        -0.1897, -0.2274, -0.0360, -0.2672, -0.1732, -0.1003, -0.0649, -0.3095,\n",
            "        -0.1878, -0.0374, -0.0430, -0.1667, -0.0660, -0.2204, -0.1700, -0.1172,\n",
            "        -0.1146, -0.1017, -0.1070, -0.0800, -0.1057, -0.0184, -0.2195, -0.0341,\n",
            "        -0.0443, -0.1079, -0.2890, -0.0897, -0.0821, -0.0467, -0.0642, -0.0826,\n",
            "        -0.0848, -0.4056, -0.1385,  0.0090, -0.0646, -0.0356, -0.0513, -0.1632,\n",
            "        -0.1762, -0.0467, -0.0825, -0.1863, -0.0207, -0.0162, -0.0974, -0.0542,\n",
            "        -0.1448, -0.1482, -0.0362, -0.0567, -0.3325, -0.1558, -0.1713, -0.0603,\n",
            "        -0.1862, -0.0299, -0.1337, -0.0987, -0.0442, -0.0416, -0.0547, -0.0264])), ('binary_decoder.3.weight', tensor([[ 1.2834e-01,  1.4888e-01,  1.4440e-01,  1.6662e-01, -1.1409e-01,\n",
            "         -1.4760e-01,  1.2425e-01, -1.1175e-01, -1.2622e-01,  1.2714e-01,\n",
            "         -9.6048e-02, -2.2083e-04, -3.8470e-02,  1.6773e-01, -1.2496e-01,\n",
            "          1.3173e-01,  1.8474e-01, -1.5140e-01,  1.7298e-01, -1.2102e-01,\n",
            "          7.3056e-04, -1.5459e-01, -1.3763e-01,  1.3435e-01, -1.6340e-01,\n",
            "         -1.2815e-01,  1.4804e-01,  1.1812e-01, -1.2128e-01, -1.6354e-01,\n",
            "          1.3974e-01, -1.1051e-01,  1.6433e-01,  1.2930e-01, -1.0555e-01,\n",
            "         -1.0698e-01, -1.2092e-01, -9.1380e-02,  1.6388e-01,  1.4818e-01,\n",
            "         -1.8096e-01,  1.3766e-01,  1.3187e-01,  1.1437e-01,  1.2719e-01,\n",
            "          1.0754e-01, -1.6257e-01,  1.3968e-01,  1.8464e-01,  5.6648e-03,\n",
            "          2.2533e-03, -1.1783e-01,  1.8331e-03,  1.3523e-01,  4.3527e-04,\n",
            "          1.5765e-01,  1.2701e-03,  1.7409e-01,  1.2579e-01,  3.0641e-03,\n",
            "         -1.1602e-01, -3.2037e-03,  1.3978e-01,  1.0706e-01,  1.6088e-02,\n",
            "         -4.6320e-03,  1.6017e-01,  3.4978e-02, -8.7203e-02,  1.1698e-01,\n",
            "          1.2853e-01, -1.4870e-01,  1.1058e-01, -1.8250e-01,  1.3037e-01,\n",
            "          5.7813e-02,  1.3009e-01, -1.1612e-01, -4.4886e-03, -1.1334e-01,\n",
            "         -1.1228e-01, -1.1545e-01, -1.3154e-01,  1.2883e-01,  1.5775e-01,\n",
            "          1.4818e-03, -5.4942e-03,  2.0222e-01,  1.6632e-01,  1.3943e-01,\n",
            "         -1.4315e-01, -1.3015e-01, -1.2841e-01,  1.7018e-01,  1.5028e-01,\n",
            "         -1.6654e-01,  1.1603e-01, -2.7170e-01, -1.1455e-01,  9.2469e-04,\n",
            "          5.2275e-04,  8.2615e-04,  1.4736e-01,  1.4253e-01, -1.2020e-01,\n",
            "         -1.4484e-01, -1.3172e-01, -1.1635e-01,  1.3714e-01,  7.6136e-04,\n",
            "          1.0743e-01,  1.5299e-01, -1.1080e-01, -1.6349e-01,  1.7825e-01,\n",
            "         -1.8268e-01, -1.6429e-01,  1.1742e-01, -2.1268e-03, -1.8283e-01,\n",
            "          2.1899e-01,  1.6380e-01, -1.0537e-01, -1.4908e-01,  1.4575e-01,\n",
            "          1.2678e-01, -1.3359e-01,  1.4040e-01]])), ('binary_decoder.3.bias', tensor([0.0843])), ('gene_embedding_layer.0.weight', tensor([[-0.0114,  0.0591,  0.0060,  ...,  0.0408,  0.0045,  0.0366],\n",
            "        [-0.0202,  0.0543, -0.0189,  ...,  0.0089,  0.0045, -0.0008],\n",
            "        [-0.0193, -0.0134,  0.0254,  ...,  0.0091, -0.0102,  0.0228],\n",
            "        ...,\n",
            "        [-0.0146,  0.0074,  0.0028,  ..., -0.0108,  0.0115,  0.0074],\n",
            "        [ 0.0064,  0.0036,  0.0099,  ..., -0.0032, -0.0167, -0.0088],\n",
            "        [-0.0016, -0.0061,  0.0156,  ...,  0.0386, -0.0207, -0.0164]])), ('gene_embedding_layer.0.bias', tensor([-0.0073, -0.0033, -0.0101,  ..., -0.0058, -0.0110,  0.0059])), ('gene_embedding_layer.2.weight', tensor([0.9784, 1.0278, 1.0090,  ..., 1.0066, 0.9926, 0.9831])), ('gene_embedding_layer.2.bias', tensor([0.0277, 0.0155, 0.0219,  ..., 0.0196, 0.0260, 0.0194])), ('pe_embedding.weight', tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [-1.1892,  1.3932,  2.1059,  ...,  1.4319,  1.6443, -0.6632],\n",
            "        [-1.6191, -0.3972, -0.4643,  ..., -0.3136, -2.2376,  0.2842],\n",
            "        ...,\n",
            "        [-0.6270, -2.2880, -0.2370,  ...,  0.0053, -0.0179, -0.8923],\n",
            "        [ 0.7524, -1.0703, -0.3290,  ...,  0.3269, -0.3019, -0.9970],\n",
            "        [ 1.3600, -0.9809, -1.6618,  ...,  0.7187, -0.7521, -1.9612]]))])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "model = torch.load(\"/content/drive/MyDrive/Colab Notebooks/esm cell state/33l_8ep_1024t_1280.torch\", map_location=\"cpu\")\n",
        "print(model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u90d8fbPyCR_"
      },
      "outputs": [],
      "source": [
        "import scanpy as sc\n",
        "adata = sc.read_h5ad(\"/content/drive/MyDrive/Colab Notebooks/esm cell state/avg_counts/RMC-6236_block9051.h5ad\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "btTPmKLcy9h8",
        "outputId": "84baf337-d900-42b4-db0c-974d993e38f0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AnnData object with n_obs × n_vars = 1 × 62710\n",
              "    obs: 'drug', 'block_id'"
            ]
          },
          "execution_count": 75,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "adata"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBTb5FUEACS-"
      },
      "source": [
        "## uce embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5FdVUogZB1kT"
      },
      "outputs": [],
      "source": [
        "rm -rf /content/UCE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OVbAgN0tB_cW",
        "outputId": "7dff480d-800f-43c9-fad5-3519c0c0beca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "%cd /content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NVxhkQgFADbU",
        "outputId": "ac6739dd-fa81-417e-9fb2-18626175eebd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'UCE'...\n",
            "remote: Enumerating objects: 227, done.\u001b[K\n",
            "remote: Counting objects: 100% (87/87), done.\u001b[K\n",
            "remote: Compressing objects: 100% (43/43), done.\u001b[K\n",
            "remote: Total 227 (delta 62), reused 52 (delta 44), pack-reused 140 (from 1)\u001b[K\n",
            "Receiving objects: 100% (227/227), 328.67 KiB | 14.29 MiB/s, done.\n",
            "Resolving deltas: 100% (119/119), done.\n",
            "/content/UCE\n"
          ]
        }
      ],
      "source": [
        "# First, clone the repository\n",
        "!git clone https://github.com/snap-stanford/UCE.git\n",
        "%cd UCE\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VU9U05dMGWZX",
        "outputId": "f7bd28e1-59c0-487c-cd9c-ff1af568219a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index(['TSPAN6', 'TNMD', 'DPM1', 'SCYL3', 'C1orf112', 'FGR', 'CFH', 'FUCA2',\n",
            "       'GCLC', 'NFYA'],\n",
            "      dtype='object', name='gene_name')\n"
          ]
        }
      ],
      "source": [
        "import scanpy as sc\n",
        "\n",
        "adata = sc.read_h5ad(\"/content/drive/MyDrive/Colab Notebooks/esm cell state/avg_counts_control/control.h5ad\")\n",
        "print(adata.var.index[:10])  # Print first 10 gene names\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oj6rTqbfIGvO",
        "outputId": "f394ab9d-2462-4f34-f499-44624f498d09"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting mygene\n",
            "  Downloading mygene-3.2.2-py2.py3-none-any.whl.metadata (10 kB)\n",
            "Collecting biothings-client>=0.2.6 (from mygene)\n",
            "  Downloading biothings_client-0.4.1-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: httpx>=0.22.0 in /usr/local/lib/python3.11/dist-packages (from biothings-client>=0.2.6->mygene) (0.28.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.22.0->biothings-client>=0.2.6->mygene) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.22.0->biothings-client>=0.2.6->mygene) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.22.0->biothings-client>=0.2.6->mygene) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.22.0->biothings-client>=0.2.6->mygene) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.22.0->biothings-client>=0.2.6->mygene) (0.14.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.22.0->biothings-client>=0.2.6->mygene) (1.3.1)\n",
            "Downloading mygene-3.2.2-py2.py3-none-any.whl (5.4 kB)\n",
            "Downloading biothings_client-0.4.1-py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.7/46.7 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: biothings-client, mygene\n",
            "Successfully installed biothings-client-0.4.1 mygene-3.2.2\n"
          ]
        }
      ],
      "source": [
        "!pip install mygene"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RR5LFojJK68j",
        "outputId": "b2b5cf82-d41a-4b5a-8edf-1aa41e6ac550"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:biothings.client:Input sequence provided is already in string format. No operation performed\n",
            "WARNING:biothings.client:Input sequence provided is already in string format. No operation performed\n",
            "INFO:biothings.client:querying 1-1000 ...\n",
            "INFO:biothings.client:querying 1001-2000 ...\n",
            "INFO:biothings.client:querying 2001-3000 ...\n",
            "INFO:biothings.client:querying 3001-4000 ...\n",
            "INFO:biothings.client:querying 4001-5000 ...\n",
            "INFO:biothings.client:querying 5001-6000 ...\n",
            "INFO:biothings.client:querying 6001-7000 ...\n",
            "INFO:biothings.client:querying 7001-8000 ...\n",
            "INFO:biothings.client:querying 8001-9000 ...\n",
            "INFO:biothings.client:querying 9001-10000 ...\n",
            "INFO:biothings.client:querying 10001-11000 ...\n",
            "INFO:biothings.client:querying 11001-12000 ...\n",
            "INFO:biothings.client:querying 12001-13000 ...\n",
            "INFO:biothings.client:querying 13001-14000 ...\n",
            "INFO:biothings.client:querying 14001-15000 ...\n",
            "INFO:biothings.client:querying 15001-16000 ...\n",
            "INFO:biothings.client:querying 16001-17000 ...\n",
            "INFO:biothings.client:querying 17001-18000 ...\n",
            "INFO:biothings.client:querying 18001-19000 ...\n",
            "INFO:biothings.client:querying 19001-20000 ...\n",
            "INFO:biothings.client:querying 20001-20930 ...\n",
            "INFO:biothings.client:Finished.\n",
            "WARNING:biothings.client:27 input query terms found dup hits:\t[('ENSG00000215156', 2), ('ENSG00000226506', 2), ('ENSG00000228044', 2), ('ENSG00000228566', 2), ('E\n",
            "WARNING:biothings.client:1071 input query terms found no hit:\t['ENSG00000131484', 'ENSG00000132832', 'ENSG00000188525', 'ENSG00000198106', 'ENSG00000203520', 'ENS\n",
            "INFO:biothings.client:Pass \"returnall=True\" to return complete lists of duplicate or missing query terms.\n"
          ]
        }
      ],
      "source": [
        "import scanpy as sc\n",
        "import pandas as pd\n",
        "import mygene\n",
        "\n",
        "adata = sc.read_h5ad(\"/content/drive/MyDrive/Colab Notebooks/esm cell state/avg_counts_control/control.h5ad\")\n",
        "\n",
        "mg = mygene.MyGeneInfo()\n",
        "ensg_ids = [gene for gene in adata.var_names if gene.startswith(\"ENSG\")]\n",
        "query_result = mg.querymany(ensg_ids, scopes=\"ensembl.gene\", fields=\"symbol\", species=\"human\")\n",
        "\n",
        "ensg_to_symbol = {entry[\"query\"]: entry.get(\"symbol\", entry[\"query\"]) for entry in query_result}\n",
        "adata.var[\"gene_symbols\"] = adata.var_names.map(lambda x: ensg_to_symbol.get(x, x))\n",
        "\n",
        "# adata.write_h5ad(\"/content/drive/MyDrive/Colab Notebooks/esm cell state/avg_counts_control/control_updated.h5ad\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C0G-she6MvKH"
      },
      "outputs": [],
      "source": [
        "adata.write_h5ad(\"/content/drive/MyDrive/Colab Notebooks/esm cell state/avg_counts_control/control_updated.h5ad\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mCYAdAqnMwva"
      },
      "outputs": [],
      "source": [
        "control_adata = sc.read_h5ad(\"/content/drive/MyDrive/Colab Notebooks/esm cell state/avg_counts_control/control_updated.h5ad\")\n",
        "\n",
        "ensg_mask = control_adata.var[\"gene_symbols\"].str.startswith(\"ENSG\")\n",
        "ensg_indexes = control_adata.var.index[ensg_mask]\n",
        "\n",
        "control_adata = control_adata[:, ~ensg_mask]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ADLutpi6NNfS"
      },
      "outputs": [],
      "source": [
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FK_TVRSZK7Hu",
        "outputId": "597d1d5f-bfdc-44fa-c538-9564563241d3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-36-457643d69bfe>:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  human_embeddings = torch.load(human_embeddings_path)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Example gene keys in embedding file: ['A1BG', 'A1CF', 'A2M', 'A2ML1', 'A3GALT2', 'A4GALT', 'A4GNT', 'AAAS', 'AACS', 'AADAC']\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "human_embeddings_path = \"model_files/protein_embeddings/Homo_sapiens.GRCh38.gene_symbol_to_embedding_ESM2.pt\"\n",
        "human_embeddings = torch.load(human_embeddings_path)\n",
        "\n",
        "# Print a sample of stored gene keys\n",
        "print(\"Example gene keys in embedding file:\", list(human_embeddings.keys())[:10])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ElJQnDYNPcHb",
        "outputId": "ad22c458-922d-470e-8e34-6b0066c4f02e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/anndata/_core/anndata.py:845: UserWarning: \n",
            "AnnData expects .var.index to contain strings, but got values like:\n",
            "    ['TSPAN6', 'TNMD', 'DPM1', 'SCYL3', 'C1orf112']\n",
            "\n",
            "    Inferred to be: categorical\n",
            "\n",
            "  names = self._prep_dim_index(names, \"var\")\n"
          ]
        }
      ],
      "source": [
        "control_adata.var_names = control_adata.var[\"gene_symbols\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vm2iMMS2PcoQ",
        "outputId": "fb33b3db-ff97-4619-93ac-cd0aef0987a8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "CategoricalIndex(['TSPAN6', 'TNMD', 'DPM1', 'SCYL3', 'C1orf112', 'FGR', 'CFH',\n",
              "                  'FUCA2', 'GCLC', 'NFYA',\n",
              "                  ...\n",
              "                  'SMG1P7-1', 'SMG1P5-1', 'ANKRD20A11P-1', 'COL6A4P1-1',\n",
              "                  'PRSS30P-1', 'LNX2BP', 'POLGARF', 'LY6S', 'TMEM276-ZFTRAF1',\n",
              "                  'TMEM276'],\n",
              "                 categories=['5S_rRNA', '5S_rRNA-1', '5S_rRNA-2', '5S_rRNA-3', ..., 'hsa-mir-423', 'hsa-mir-1253', 'hsa-mir-8069-1', 'snoZ196'], ordered=False, dtype='category', name='gene_symbols', length=45584)"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "control_adata.var_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_eN0PoDmPcsn",
        "outputId": "9c564b8a-97e3-479b-e977-347744955bbd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of overlapping genes: 19648\n"
          ]
        }
      ],
      "source": [
        "# Convert to lowercase to match embedding format\n",
        "adata_gene_names = set(adata.var_names.str.lower())\n",
        "embedding_gene_names = set(map(str.lower, human_embeddings.keys()))\n",
        "\n",
        "# Find overlap\n",
        "overlapping_genes = adata_gene_names.intersection(embedding_gene_names)\n",
        "print(f\"Number of overlapping genes: {len(overlapping_genes)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22uTWSLkQPF7",
        "outputId": "4240cf61-ce4e-407f-df94-2a42709bc5e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total genes in adata: 45584\n",
            "Number of overlapping genes: 19648\n"
          ]
        }
      ],
      "source": [
        "print(f\"Total genes in adata: {len(control_adata.var_names)}\")\n",
        "print(f\"Number of overlapping genes: {len(overlapping_genes)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hG22vNluPcvA"
      },
      "outputs": [],
      "source": [
        "control_adata.write_h5ad(\"/content/filtered_control.h5ad\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p-Pdc0hFVJmS",
        "outputId": "b5eaf80d-859c-4acb-ac6d-2470606100f0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "CategoricalIndex(['TSPAN6', 'TNMD', 'DPM1', 'SCYL3', 'C1orf112', 'FGR', 'CFH',\n",
              "                  'FUCA2', 'GCLC', 'NFYA',\n",
              "                  ...\n",
              "                  'SMG1P7-1', 'SMG1P5-1', 'ANKRD20A11P-1', 'COL6A4P1-1',\n",
              "                  'PRSS30P-1', 'LNX2BP', 'POLGARF', 'LY6S', 'TMEM276-ZFTRAF1',\n",
              "                  'TMEM276'],\n",
              "                 categories=['5S_rRNA', '5S_rRNA-1', '5S_rRNA-2', '5S_rRNA-3', ..., 'hsa-mir-423', 'hsa-mir-1253', 'hsa-mir-8069-1', 'snoZ196'], ordered=False, dtype='category', name='gene_symbols', length=45584)"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "control_adata.var_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qr9HqM6lIiVX",
        "outputId": "14ec22ef-2ea2-41d9-92da-b58c349747ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ANNDATA PROCESSOR DONE...\n",
            "Proccessing filtered_control\n",
            "species human\n",
            "adata shape before load gene embedding adata function (0, 0)\n",
            "genes with embeddings 19790\n",
            "Total genes with embeddings: 19790\n",
            "First few genes in adata.var_names: []\n",
            "Overlap between adata genes and embedding genes: 0\n",
            "genes to use 0\n",
            "Overlap between adata genes and embedding genes: 0\n",
            "Genes actually kept in adata: 0\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/UCE/eval_single_anndata.py\", line 158, in <module>\n",
            "    main(args, accelerator)\n",
            "  File \"/content/UCE/eval_single_anndata.py\", line 84, in main\n",
            "    processor.preprocess_anndata()\n",
            "  File \"/content/UCE/evaluate.py\", line 94, in preprocess_anndata\n",
            "    process_raw_anndata(self.row,\n",
            "  File \"/content/UCE/data_proc/data_utils.py\", line 214, in process_raw_anndata\n",
            "    dataset, adata = anndata_to_sc_dataset(ad, species=species, labels=labels, covar_col=covar_col, hv_genes=None)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/UCE/data_proc/data_utils.py\", line 124, in anndata_to_sc_dataset\n",
            "    adata, protein_embeddings = load_gene_embeddings_adata(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/UCE/data_proc/gene_embeddings.py\", line 87, in load_gene_embeddings_adata\n",
            "    species_to_gene_embeddings = {\n",
            "                                 ^\n",
            "  File \"/content/UCE/data_proc/gene_embeddings.py\", line 88, in <dictcomp>\n",
            "    species_name: torch.stack([\n",
            "                  ^^^^^^^^^^^^^\n",
            "RuntimeError: stack expects a non-empty TensorList\n"
          ]
        }
      ],
      "source": [
        "# Run evaluation on control dataset\n",
        "!python eval_single_anndata.py --adata_path \"/content/filtered_control.h5ad\" --dir \"./control_output\" --model_loc \"minwoosun/uce-100m\" --species \"human\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "931FeVAcIjyF"
      },
      "outputs": [],
      "source": [
        "# Process first 5 drug files\n",
        "for drug_file in drug_files[:5]:\n",
        "    drug_path = os.path.join(drug_dir, f\"filtered_{drug_file}\")\n",
        "    drug_name = drug_file.split('_')[0]\n",
        "    output_dir = f\"./drug_output/{drug_name}\"\n",
        "    os.system(f'python eval_single_anndata.py --adata_path \"{drug_path}\" --dir \"{output_dir}\" --model_loc \"minwoosun/uce-100m\"')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLnLUoZdZujS"
      },
      "source": [
        "## scgpt embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aoJmi8CRhpx4",
        "outputId": "0c705841-d866-4afc-f875-8f1890748879"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "plate3_10percent.h5ad  plate3_filt_Vevo_Tahoe100M_WServicesFrom_ParseGigalab.h5ad\n"
          ]
        }
      ],
      "source": [
        "!ls \"/content/drive/MyDrive/Colab Notebooks/esm cell state/h5ad\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W9qyPSaGjFJt"
      },
      "outputs": [],
      "source": [
        "import scanpy as sc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CrcfJ8E2il4_",
        "outputId": "8887c80a-9586-4a98-f941-e1f165da9908"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AnnData object with n_obs × n_vars = 470494 × 62710\n",
              "    obs: 'sample', 'gene_count', 'tscp_count', 'mread_count', 'drugname_drugconc', 'drug', 'cell_line', 'sublibrary', 'BARCODE', 'pcnt_mito', 'S_score', 'G2M_score', 'phase', 'pass_filter', 'cell_name', 'plate'"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "adata = sc.read_h5ad(\"/content/drive/MyDrive/Colab Notebooks/esm cell state/h5ad/plate3_10percent.h5ad\")\n",
        "adata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z4liEDf9Zw8X",
        "outputId": "a8adcebb-8b36-4beb-8825-a200f533bf6d"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-145ad79c-47ce-4d55-8be8-fa14351b1add\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sample</th>\n",
              "      <th>gene_count</th>\n",
              "      <th>tscp_count</th>\n",
              "      <th>mread_count</th>\n",
              "      <th>drugname_drugconc</th>\n",
              "      <th>drug</th>\n",
              "      <th>cell_line</th>\n",
              "      <th>sublibrary</th>\n",
              "      <th>BARCODE</th>\n",
              "      <th>pcnt_mito</th>\n",
              "      <th>S_score</th>\n",
              "      <th>G2M_score</th>\n",
              "      <th>phase</th>\n",
              "      <th>pass_filter</th>\n",
              "      <th>cell_name</th>\n",
              "      <th>plate</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>BARCODE_SUB_LIB_ID</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>01_094_002-lib_1072</th>\n",
              "      <td>smp_1687</td>\n",
              "      <td>774</td>\n",
              "      <td>924</td>\n",
              "      <td>1137</td>\n",
              "      <td>[('Infigratinib', 5.0, 'uM')]</td>\n",
              "      <td>Infigratinib</td>\n",
              "      <td>CVCL_1547</td>\n",
              "      <td>lib_1072</td>\n",
              "      <td>01_094_002</td>\n",
              "      <td>0.008658</td>\n",
              "      <td>-0.023810</td>\n",
              "      <td>0.077473</td>\n",
              "      <td>G2M</td>\n",
              "      <td>full</td>\n",
              "      <td>NCI-H23</td>\n",
              "      <td>plate3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>01_154_050-lib_1033</th>\n",
              "      <td>smp_1687</td>\n",
              "      <td>1864</td>\n",
              "      <td>2492</td>\n",
              "      <td>2972</td>\n",
              "      <td>[('Infigratinib', 5.0, 'uM')]</td>\n",
              "      <td>Infigratinib</td>\n",
              "      <td>CVCL_0293</td>\n",
              "      <td>lib_1033</td>\n",
              "      <td>01_154_050</td>\n",
              "      <td>0.026886</td>\n",
              "      <td>0.054286</td>\n",
              "      <td>-0.103297</td>\n",
              "      <td>S</td>\n",
              "      <td>full</td>\n",
              "      <td>HEC-1-A</td>\n",
              "      <td>plate3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>01_059_130-lib_1074</th>\n",
              "      <td>smp_1687</td>\n",
              "      <td>516</td>\n",
              "      <td>684</td>\n",
              "      <td>827</td>\n",
              "      <td>[('Infigratinib', 5.0, 'uM')]</td>\n",
              "      <td>Infigratinib</td>\n",
              "      <td>CVCL_1478</td>\n",
              "      <td>lib_1074</td>\n",
              "      <td>01_059_130</td>\n",
              "      <td>0.002924</td>\n",
              "      <td>-0.027778</td>\n",
              "      <td>-0.023810</td>\n",
              "      <td>G1</td>\n",
              "      <td>minimal</td>\n",
              "      <td>NCI-H1573</td>\n",
              "      <td>plate3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>01_030_092-lib_1027</th>\n",
              "      <td>smp_1687</td>\n",
              "      <td>2016</td>\n",
              "      <td>3044</td>\n",
              "      <td>3645</td>\n",
              "      <td>[('Infigratinib', 5.0, 'uM')]</td>\n",
              "      <td>Infigratinib</td>\n",
              "      <td>CVCL_1517</td>\n",
              "      <td>lib_1027</td>\n",
              "      <td>01_030_092</td>\n",
              "      <td>0.017740</td>\n",
              "      <td>-0.107143</td>\n",
              "      <td>-0.158654</td>\n",
              "      <td>G1</td>\n",
              "      <td>full</td>\n",
              "      <td>NCI-H2030</td>\n",
              "      <td>plate3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>01_027_080-lib_1096</th>\n",
              "      <td>smp_1687</td>\n",
              "      <td>1677</td>\n",
              "      <td>2372</td>\n",
              "      <td>2830</td>\n",
              "      <td>[('Infigratinib', 5.0, 'uM')]</td>\n",
              "      <td>Infigratinib</td>\n",
              "      <td>CVCL_0131</td>\n",
              "      <td>lib_1096</td>\n",
              "      <td>01_027_080</td>\n",
              "      <td>0.026560</td>\n",
              "      <td>-0.040125</td>\n",
              "      <td>0.102747</td>\n",
              "      <td>G2M</td>\n",
              "      <td>full</td>\n",
              "      <td>A-172</td>\n",
              "      <td>plate3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96_163_049-lib_1077</th>\n",
              "      <td>smp_1782</td>\n",
              "      <td>580</td>\n",
              "      <td>697</td>\n",
              "      <td>851</td>\n",
              "      <td>[('DMSO_TF', 0.0, 'uM')]</td>\n",
              "      <td>DMSO_TF</td>\n",
              "      <td>CVCL_0480</td>\n",
              "      <td>lib_1077</td>\n",
              "      <td>96_163_049</td>\n",
              "      <td>0.060258</td>\n",
              "      <td>-0.000381</td>\n",
              "      <td>0.038830</td>\n",
              "      <td>G2M</td>\n",
              "      <td>minimal</td>\n",
              "      <td>PANC-1</td>\n",
              "      <td>plate3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95_124_186-lib_1089</th>\n",
              "      <td>smp_1781</td>\n",
              "      <td>1279</td>\n",
              "      <td>1823</td>\n",
              "      <td>2119</td>\n",
              "      <td>[('DMSO_TF', 0.0, 'uM')]</td>\n",
              "      <td>DMSO_TF</td>\n",
              "      <td>CVCL_0397</td>\n",
              "      <td>lib_1089</td>\n",
              "      <td>95_124_186</td>\n",
              "      <td>0.042238</td>\n",
              "      <td>-0.032571</td>\n",
              "      <td>-0.157143</td>\n",
              "      <td>G1</td>\n",
              "      <td>full</td>\n",
              "      <td>LS 180</td>\n",
              "      <td>plate3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95_098_128-lib_1064</th>\n",
              "      <td>smp_1781</td>\n",
              "      <td>486</td>\n",
              "      <td>577</td>\n",
              "      <td>672</td>\n",
              "      <td>[('DMSO_TF', 0.0, 'uM')]</td>\n",
              "      <td>DMSO_TF</td>\n",
              "      <td>CVCL_0399</td>\n",
              "      <td>lib_1064</td>\n",
              "      <td>95_098_128</td>\n",
              "      <td>0.032929</td>\n",
              "      <td>-0.043825</td>\n",
              "      <td>-0.004579</td>\n",
              "      <td>G1</td>\n",
              "      <td>minimal</td>\n",
              "      <td>LoVo</td>\n",
              "      <td>plate3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95_171_127-lib_1028</th>\n",
              "      <td>smp_1781</td>\n",
              "      <td>833</td>\n",
              "      <td>1027</td>\n",
              "      <td>1218</td>\n",
              "      <td>[('DMSO_TF', 0.0, 'uM')]</td>\n",
              "      <td>DMSO_TF</td>\n",
              "      <td>CVCL_1125</td>\n",
              "      <td>lib_1028</td>\n",
              "      <td>95_171_127</td>\n",
              "      <td>0.023369</td>\n",
              "      <td>-0.092000</td>\n",
              "      <td>-0.042857</td>\n",
              "      <td>G1</td>\n",
              "      <td>full</td>\n",
              "      <td>CHP-212</td>\n",
              "      <td>plate3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95_026_010-lib_1069</th>\n",
              "      <td>smp_1781</td>\n",
              "      <td>1155</td>\n",
              "      <td>1651</td>\n",
              "      <td>1974</td>\n",
              "      <td>[('DMSO_TF', 0.0, 'uM')]</td>\n",
              "      <td>DMSO_TF</td>\n",
              "      <td>CVCL_1381</td>\n",
              "      <td>lib_1069</td>\n",
              "      <td>95_026_010</td>\n",
              "      <td>0.069049</td>\n",
              "      <td>-0.024762</td>\n",
              "      <td>0.236198</td>\n",
              "      <td>G2M</td>\n",
              "      <td>full</td>\n",
              "      <td>LOX-IMVI</td>\n",
              "      <td>plate3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>470494 rows × 16 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-145ad79c-47ce-4d55-8be8-fa14351b1add')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-145ad79c-47ce-4d55-8be8-fa14351b1add button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-145ad79c-47ce-4d55-8be8-fa14351b1add');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-dd8f5d76-dd32-4ce4-9013-9e41411fdb84\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-dd8f5d76-dd32-4ce4-9013-9e41411fdb84')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-dd8f5d76-dd32-4ce4-9013-9e41411fdb84 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                       sample  gene_count  tscp_count  mread_count  \\\n",
              "BARCODE_SUB_LIB_ID                                                   \n",
              "01_094_002-lib_1072  smp_1687         774         924         1137   \n",
              "01_154_050-lib_1033  smp_1687        1864        2492         2972   \n",
              "01_059_130-lib_1074  smp_1687         516         684          827   \n",
              "01_030_092-lib_1027  smp_1687        2016        3044         3645   \n",
              "01_027_080-lib_1096  smp_1687        1677        2372         2830   \n",
              "...                       ...         ...         ...          ...   \n",
              "96_163_049-lib_1077  smp_1782         580         697          851   \n",
              "95_124_186-lib_1089  smp_1781        1279        1823         2119   \n",
              "95_098_128-lib_1064  smp_1781         486         577          672   \n",
              "95_171_127-lib_1028  smp_1781         833        1027         1218   \n",
              "95_026_010-lib_1069  smp_1781        1155        1651         1974   \n",
              "\n",
              "                                 drugname_drugconc          drug  cell_line  \\\n",
              "BARCODE_SUB_LIB_ID                                                            \n",
              "01_094_002-lib_1072  [('Infigratinib', 5.0, 'uM')]  Infigratinib  CVCL_1547   \n",
              "01_154_050-lib_1033  [('Infigratinib', 5.0, 'uM')]  Infigratinib  CVCL_0293   \n",
              "01_059_130-lib_1074  [('Infigratinib', 5.0, 'uM')]  Infigratinib  CVCL_1478   \n",
              "01_030_092-lib_1027  [('Infigratinib', 5.0, 'uM')]  Infigratinib  CVCL_1517   \n",
              "01_027_080-lib_1096  [('Infigratinib', 5.0, 'uM')]  Infigratinib  CVCL_0131   \n",
              "...                                            ...           ...        ...   \n",
              "96_163_049-lib_1077       [('DMSO_TF', 0.0, 'uM')]       DMSO_TF  CVCL_0480   \n",
              "95_124_186-lib_1089       [('DMSO_TF', 0.0, 'uM')]       DMSO_TF  CVCL_0397   \n",
              "95_098_128-lib_1064       [('DMSO_TF', 0.0, 'uM')]       DMSO_TF  CVCL_0399   \n",
              "95_171_127-lib_1028       [('DMSO_TF', 0.0, 'uM')]       DMSO_TF  CVCL_1125   \n",
              "95_026_010-lib_1069       [('DMSO_TF', 0.0, 'uM')]       DMSO_TF  CVCL_1381   \n",
              "\n",
              "                    sublibrary     BARCODE  pcnt_mito   S_score  G2M_score  \\\n",
              "BARCODE_SUB_LIB_ID                                                           \n",
              "01_094_002-lib_1072   lib_1072  01_094_002   0.008658 -0.023810   0.077473   \n",
              "01_154_050-lib_1033   lib_1033  01_154_050   0.026886  0.054286  -0.103297   \n",
              "01_059_130-lib_1074   lib_1074  01_059_130   0.002924 -0.027778  -0.023810   \n",
              "01_030_092-lib_1027   lib_1027  01_030_092   0.017740 -0.107143  -0.158654   \n",
              "01_027_080-lib_1096   lib_1096  01_027_080   0.026560 -0.040125   0.102747   \n",
              "...                        ...         ...        ...       ...        ...   \n",
              "96_163_049-lib_1077   lib_1077  96_163_049   0.060258 -0.000381   0.038830   \n",
              "95_124_186-lib_1089   lib_1089  95_124_186   0.042238 -0.032571  -0.157143   \n",
              "95_098_128-lib_1064   lib_1064  95_098_128   0.032929 -0.043825  -0.004579   \n",
              "95_171_127-lib_1028   lib_1028  95_171_127   0.023369 -0.092000  -0.042857   \n",
              "95_026_010-lib_1069   lib_1069  95_026_010   0.069049 -0.024762   0.236198   \n",
              "\n",
              "                    phase pass_filter  cell_name   plate  \n",
              "BARCODE_SUB_LIB_ID                                        \n",
              "01_094_002-lib_1072   G2M        full    NCI-H23  plate3  \n",
              "01_154_050-lib_1033     S        full    HEC-1-A  plate3  \n",
              "01_059_130-lib_1074    G1     minimal  NCI-H1573  plate3  \n",
              "01_030_092-lib_1027    G1        full  NCI-H2030  plate3  \n",
              "01_027_080-lib_1096   G2M        full      A-172  plate3  \n",
              "...                   ...         ...        ...     ...  \n",
              "96_163_049-lib_1077   G2M     minimal     PANC-1  plate3  \n",
              "95_124_186-lib_1089    G1        full     LS 180  plate3  \n",
              "95_098_128-lib_1064    G1     minimal       LoVo  plate3  \n",
              "95_171_127-lib_1028    G1        full    CHP-212  plate3  \n",
              "95_026_010-lib_1069   G2M        full   LOX-IMVI  plate3  \n",
              "\n",
              "[470494 rows x 16 columns]"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "adata.obs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CxtmAIXHmZVi"
      },
      "outputs": [],
      "source": [
        "adata.obs['batch_id'] = adata.obs['plate']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mq12B9lHnH0R",
        "outputId": "216f0d7e-7c25-4bde-bb77-0ac401a610ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'scGPT-spatial'...\n",
            "remote: Enumerating objects: 78, done.\u001b[K\n",
            "remote: Counting objects: 100% (78/78), done.\u001b[K\n",
            "remote: Compressing objects: 100% (68/68), done.\u001b[K\n",
            "remote: Total 78 (delta 29), reused 40 (delta 6), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (78/78), 1.70 MiB | 40.43 MiB/s, done.\n",
            "Resolving deltas: 100% (29/29), done.\n",
            "/content/scGPT-spatial\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/bowang-lab/scGPT-spatial\n",
        "%cd scGPT-spatial/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mkczzNdgnOhs",
        "outputId": "b95c61ef-91ec-482d-e485-95788ec09792"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running on Google Colab\n",
            "Installing dependencies...\n",
            "Collecting scgpt\n",
            "  Downloading scgpt-0.1.5-py3-none-any.whl.metadata (7.9 kB)\n",
            "Collecting datasets<3.0.0,>=2.3.0 (from scgpt)\n",
            "  Downloading datasets-2.21.0-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting flash-attn<2.0.0,>=1.0.1 (from scgpt)\n",
            "  Downloading flash_attn-1.0.9.tar.gz (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m50.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting leidenalg<0.9.0,>=0.8.10 (from scgpt)\n",
            "  Downloading leidenalg-0.8.10.tar.gz (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m48.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "INFO: pip is looking at multiple versions of scgpt to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting scgpt\n",
            "  Downloading scgpt-0.1.3-py3-none-any.whl.metadata (6.4 kB)\n",
            "\u001b[31mERROR: Cannot install scgpt==0.1.3 and scgpt==0.1.5 because these package versions have conflicting dependencies.\u001b[0m\u001b[31m\n",
            "\u001b[0m\n",
            "The conflict is caused by:\n",
            "    scgpt 0.1.5 depends on llvmlite<0.39.0 and >=0.38.0\n",
            "    scgpt 0.1.3 depends on llvmlite<0.39.0 and >=0.38.0\n",
            "\n",
            "To fix this you could try to:\n",
            "1. loosen the range of package versions you've specified\n",
            "2. remove package versions to allow pip to attempt to solve the dependency conflict\n",
            "\n",
            "\u001b[31mERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.19.8)\n",
            "Collecting louvain\n",
            "  Downloading louvain-0.8.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.1.8)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.25.6)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pydantic<3,>=2.6 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.10.6)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.22.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb) (75.1.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.12.2)\n",
            "Collecting igraph<0.12,>=0.10.0 (from louvain)\n",
            "  Downloading igraph-0.11.8-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Collecting texttable>=1.6.2 (from igraph<0.12,>=0.10.0->louvain)\n",
            "  Downloading texttable-1.7.0-py2.py3-none-any.whl.metadata (9.8 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2025.1.31)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
            "Downloading louvain-0.8.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m59.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading igraph-0.11.8-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m96.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading texttable-1.7.0-py2.py3-none-any.whl (10 kB)\n",
            "Installing collected packages: texttable, igraph, louvain\n",
            "Successfully installed igraph-0.11.8 louvain-0.8.2 texttable-1.7.0\n",
            "Downloading data and model ckpt...\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1z_0vWYMhRuRiD1EyhuFtY9ReIR0msWaL\n",
            "From (redirected): https://drive.google.com/uc?id=1z_0vWYMhRuRiD1EyhuFtY9ReIR0msWaL&confirm=t&uuid=55dcd029-6028-4225-a2b2-29e1562fc98e\n",
            "To: /data/Kim2020_Lung.h5ad\n",
            "100% 581M/581M [00:06<00:00, 88.9MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1eD9LbxNJ35YUde3VtdVcjkwm-f4iyJ6x\n",
            "To: /data/covid_subsampled.h5ad\n",
            "100% 25.0M/25.0M [00:00<00:00, 66.7MB/s]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Retrieving folder contents\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing file 1Vp51xYAOEu5_Wd7lilnatvMJ7tr2bsbl .DS_Store\n",
            "Processing file 1bV1SHKVZgkcL-RmmuN51_IIUJTSJbXOi c_data.h5ad\n",
            "Processing file 1casFhq4InuBNhJLMnGebzkRXM2UTTeQG filtered_ms_adata.h5ad\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Retrieving folder contents completed\n",
            "Building directory structure\n",
            "Building directory structure completed\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1Vp51xYAOEu5_Wd7lilnatvMJ7tr2bsbl\n",
            "To: /data/ms/.DS_Store\n",
            "100%|██████████| 6.15k/6.15k [00:00<00:00, 9.96MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1bV1SHKVZgkcL-RmmuN51_IIUJTSJbXOi\n",
            "To: /data/ms/c_data.h5ad\n",
            "100%|██████████| 20.6M/20.6M [00:00<00:00, 48.2MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1casFhq4InuBNhJLMnGebzkRXM2UTTeQG\n",
            "To: /data/ms/filtered_ms_adata.h5ad\n",
            "100%|██████████| 47.1M/47.1M [00:01<00:00, 42.4MB/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading model ckpt...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Download completed\n",
            "Retrieving folder contents\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing file 1hh2zGKyWAx3DyovD30GStZ3QlzmSqdk1 args.json\n",
            "Processing file 14AebJfGOUF047Eg40hk57HCtrb0fyDTm best_model.pt\n",
            "Processing file 1H3E_MJ-Dl36AQV6jLbna2EdvgPaqvqcC vocab.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Retrieving folder contents completed\n",
            "Building directory structure\n",
            "Building directory structure completed\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1hh2zGKyWAx3DyovD30GStZ3QlzmSqdk1\n",
            "To: /save/scGPT_human/args.json\n",
            "100%|██████████| 1.30k/1.30k [00:00<00:00, 4.96MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=14AebJfGOUF047Eg40hk57HCtrb0fyDTm\n",
            "From (redirected): https://drive.google.com/uc?id=14AebJfGOUF047Eg40hk57HCtrb0fyDTm&confirm=t&uuid=19afc246-9d6f-4434-8e88-a18cc3d255fe\n",
            "To: /save/scGPT_human/best_model.pt\n",
            "100%|██████████| 205M/205M [00:03<00:00, 59.7MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1H3E_MJ-Dl36AQV6jLbna2EdvgPaqvqcC\n",
            "To: /save/scGPT_human/vocab.json\n",
            "100%|██████████| 1.32M/1.32M [00:00<00:00, 182MB/s]\n",
            "Download completed\n"
          ]
        }
      ],
      "source": [
        "# Specifically for Google Colab, install dependencies and download data\n",
        "\n",
        "import os\n",
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    print(\"Running on Google Colab\")\n",
        "    print(\"Installing dependencies...\")\n",
        "    !pip install -U scgpt\n",
        "    # the optional dependency of flash-attion is skipped on colab\n",
        "    !pip install wandb louvain\n",
        "\n",
        "    # # NOTE: May need to restart runtime after the installation\n",
        "\n",
        "    print(\"Downloading data and model ckpt...\")\n",
        "    !pip install -q -U gdown\n",
        "    import gdown\n",
        "\n",
        "    data_dir = \"../../data\"\n",
        "    if not os.path.exists(data_dir):\n",
        "        os.mkdir(data_dir)\n",
        "    if not os.path.exists(os.path.join(data_dir, \"Kim2020_Lung.h5ad\")):\n",
        "        !gdown https://drive.google.com/uc?id=1z_0vWYMhRuRiD1EyhuFtY9ReIR0msWaL -O $data_dir/\n",
        "    if not os.path.exists(os.path.join(data_dir, \"covid_subsampled.h5ad\")):\n",
        "        !gdown https://drive.google.com/uc?id=1eD9LbxNJ35YUde3VtdVcjkwm-f4iyJ6x -O $data_dir/\n",
        "    if not os.path.exists(os.path.join(data_dir, \"ms\")):\n",
        "        gdown.download_folder(\n",
        "            \"https://drive.google.com/drive/folders/1Qd42YNabzyr2pWt9xoY4cVMTAxsNBt4v\",\n",
        "            output=os.path.join(data_dir, \"ms\"),\n",
        "        )\n",
        "\n",
        "    print(\"Downloading model ckpt...\")\n",
        "    model_dir = \"../../save/scGPT_human\"\n",
        "    if not os.path.exists(model_dir):\n",
        "        !mkdir -p $model_dir\n",
        "        gdown.download_folder(\n",
        "            \"https://drive.google.com/drive/folders/1oWh_-ZRdhtoGQ2Fw24HP41FgLoomVo-y\",\n",
        "            output=model_dir,\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TghxLeNwdv1y",
        "outputId": "9b4244cc-72a0-4211-fcd5-b5ee921d0d4f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48crMWOtc210",
        "outputId": "115d8aa3-02a8-4b11-d9af-182062f794d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting flash-attn<1.0.5\n",
            "  Downloading flash_attn-1.0.4.tar.gz (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n",
            "Collecting pandas==1.3.5\n",
            "  Using cached pandas-1.3.5.tar.gz (4.7 MB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting scanpy==1.9.1\n",
            "  Using cached scanpy-1.9.1-py3-none-any.whl.metadata (5.5 kB)\n",
            "Collecting scikit-misc==0.1.4\n",
            "  Using cached scikit-misc-0.1.4.tar.gz (852 kB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n",
            "/bin/bash: line 1: 0.39.0: No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!pip install --no-cache-dir \"flash-attn<1.0.5\"\n",
        "!pip install pandas==1.3.5 scanpy==1.9.1 scikit-misc==0.1.4\n",
        "!pip install llvmlite<0.39.0 numba<0.56.0 leidenalg<0.9.0 scvi-tools<0.17.0 datasets<3.0.0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MdjI-06kc48A"
      },
      "outputs": [],
      "source": [
        "!pip install scgpt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JdHIC8CPbFiz",
        "outputId": "f603978b-20ac-4d1f-aa33-087f4f8b5fc9"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'scgpt'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-4aacdd824def>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mscgpt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"scGPT successfully installed!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'scgpt'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "import scgpt\n",
        "print(\"scGPT successfully installed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_phDH-KJZyQ0",
        "outputId": "001ca4db-acaa-4361-f490-02bb9f2e3b62"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/scgpt/model/model.py:19: UserWarning: flash_attn is not installed\n",
            "  warnings.warn(\"flash_attn is not installed\")\n"
          ]
        },
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'torchtext'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-373f5a0d7794>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"../\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mscgpt\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mscg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0manndata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scgpt/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscbank\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtasks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdata_collator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataCollator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdata_sampler\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSubsetsBatchSampler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scgpt/tokenizer/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mgene_tokenizer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scgpt/tokenizer/gene_tokenizer.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorchtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtorch_vocab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVocab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenization_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPreTrainedTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchtext'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import warnings\n",
        "\n",
        "import scanpy as sc\n",
        "import scib\n",
        "import numpy as np\n",
        "import sys\n",
        "\n",
        "sys.path.insert(0, \"../\")\n",
        "\n",
        "import scgpt as scg\n",
        "import matplotlib.pyplot as plt\n",
        "import anndata\n",
        "\n",
        "plt.style.context('default')\n",
        "warnings.simplefilter(\"ignore\", ResourceWarning)\n",
        "\n",
        "model_dir = Path(\"../../save/scGPT_human\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M8NteAp-qlOt"
      },
      "outputs": [],
      "source": [
        "from flash_attn.modules.mha import FlashMHA\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "noEpmFckqErF",
        "outputId": "a9402f67-33d4-4957-def5-bf2d324b8714"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Name: flash_attn\n",
            "Version: 2.7.4.post1\n",
            "Summary: Flash Attention: Fast and Memory-Efficient Exact Attention\n",
            "Home-page: https://github.com/Dao-AILab/flash-attention\n",
            "Author: Tri Dao\n",
            "Author-email: tri@tridao.me\n",
            "License: \n",
            "Location: /usr/local/lib/python3.11/dist-packages\n",
            "Requires: einops, torch\n",
            "Required-by: \n"
          ]
        }
      ],
      "source": [
        "!pip show flash-attn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hRG-tiS6pCPS",
        "outputId": "c36b166f-3765-490d-da77-4cd1c6fef507"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'flash_attn.flash_attention'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-48a71778a041>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mflash_attn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflash_attention\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFlashMHA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'flash_attn.flash_attention'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "from flash_attn.flash_attention import FlashMHA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DWSV1DtRmdIj",
        "outputId": "9f541913-f0c0-4585-e2ed-eaa740113255"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'flash_attn.flash_attention'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-0b1fc2ad3f4e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscgpt_spatial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcell_emb\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0membed_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/drive/MyDrive/Colab Notebooks/esm cell state/scGPT_spatial_v1\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mgene_col\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"feature_name\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcell_type_col\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"celltype\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/scGPT-spatial/scgpt_spatial/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlogger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetLogger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"scGPT-spatial\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtasks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdata_collator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataCollator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdata_sampler\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSubsetsBatchSampler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/scGPT-spatial/scgpt_spatial/model/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m from .model import (\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mTransformerModel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mFlashTransformerEncoderLayer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mGeneEncoder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mAdversarialDiscriminator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/scGPT-spatial/scgpt_spatial/model/model.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTransformerEncoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTransformerEncoderLayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBernoulli\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mflash_attn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflash_attention\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFlashMHA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrange\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'flash_attn.flash_attention'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "from scgpt_spatial.tasks.cell_emb import embed_data\n",
        "\n",
        "model_dir = \"/content/drive/MyDrive/Colab Notebooks/esm cell state/scGPT_spatial_v1\"\n",
        "gene_col = \"feature_name\"\n",
        "cell_type_col = \"celltype\"\n",
        "batch_id_col = \"batch_id\"\n",
        "\n",
        "ref_embed_adata = embed_data(\n",
        "    adata,\n",
        "    model_dir,\n",
        "    gene_col=gene_col,\n",
        "    obs_to_save=cell_type_col,\n",
        "    batch_size=64,\n",
        "    return_new_adata=True,\n",
        ")\n",
        "\n",
        "ref_embed_adata.obsm[\"X\"] = ref_embed_adata.X.copy()\n",
        "ref_embed_adata.obs[\"batch_id\"] = adata.obs[\"batch_id\"]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2u2SeXzUrfY"
      },
      "source": [
        "## scAce embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QLtEARAHU0ZI"
      },
      "outputs": [],
      "source": [
        "import scanpy as sc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "krl6YUmSUtVp"
      },
      "outputs": [],
      "source": [
        "adata = sc.read_h5ad(\"/content/drive/MyDrive/Colab Notebooks/esm cell state/h5ad/plate3_10percent.h5ad\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oSzqcKCjO23b",
        "outputId": "18d839a4-69a6-49a7-d02c-c466e06d933b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-3-ade24781ed1f>:5: FutureWarning: Use sc.pp.normalize_total instead\n",
            "  sc.pp.normalize_per_cell(adata)\n",
            "/usr/local/lib/python3.11/dist-packages/scanpy/preprocessing/_simple.py:588: FutureWarning: Use sc.pp.normalize_total instead\n",
            "  normalize_per_cell(\n"
          ]
        }
      ],
      "source": [
        "sc.pp.filter_genes(adata, min_cells=3)\n",
        "sc.pp.filter_cells(adata, min_genes=200)\n",
        "adata.raw = adata.copy()\n",
        "\n",
        "sc.pp.normalize_per_cell(adata)\n",
        "adata.obs['scale_factor'] = adata.obs.n_counts / adata.obs.n_counts.mean()\n",
        "\n",
        "sc.pp.log1p(adata)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ChCr5JejWyJY",
        "outputId": "0f046a9d-bf97-41a2-9120-1a76c02a39d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error: Command '['/content/scace_env/bin/python3', '-m', 'ensurepip', '--upgrade', '--default-pip']' returned non-zero exit status 1.\n"
          ]
        }
      ],
      "source": [
        "# !python -m venv /content/scace_env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_8IHiAeVP8c",
        "outputId": "54bd308a-e146-4ff3-ce91-9f141bff64d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting scikit-learn\n",
            "  Downloading scikit_learn-1.6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
            "Collecting numpy>=1.19.5 (from scikit-learn)\n",
            "  Downloading numpy-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy>=1.6.0 (from scikit-learn)\n",
            "  Downloading scipy-1.15.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting joblib>=1.2.0 (from scikit-learn)\n",
            "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
            "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
            "Downloading scikit_learn-1.6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m183.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.8/301.8 kB\u001b[0m \u001b[31m303.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m171.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.15.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.6/37.6 MB\u001b[0m \u001b[31m235.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
            "Installing collected packages: threadpoolctl, numpy, joblib, scipy, scikit-learn\n",
            "  Attempting uninstall: threadpoolctl\n",
            "    Found existing installation: threadpoolctl 3.5.0\n",
            "    Uninstalling threadpoolctl-3.5.0:\n",
            "      Successfully uninstalled threadpoolctl-3.5.0\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "  Attempting uninstall: joblib\n",
            "    Found existing installation: joblib 1.4.2\n",
            "    Uninstalling joblib-1.4.2:\n",
            "      Successfully uninstalled joblib-1.4.2\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.14.1\n",
            "    Uninstalling scipy-1.14.1:\n",
            "      Successfully uninstalled scipy-1.14.1\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.5.2\n",
            "    Uninstalling scikit-learn-1.5.2:\n",
            "      Successfully uninstalled scikit-learn-1.5.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "scanpy 1.11.0 requires scikit-learn<1.6.0,>=1.1, but you have scikit-learn 1.6.1 which is incompatible.\n",
            "pytensor 2.27.1 requires numpy<2,>=1.17.0, but you have numpy 2.2.3 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.3 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.2.3 which is incompatible.\n",
            "thinc 8.2.5 requires numpy<2.0.0,>=1.19.0; python_version >= \"3.9\", but you have numpy 2.2.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed joblib-1.4.2 numpy-2.2.3 scikit-learn-1.6.1 scipy-1.15.2 threadpoolctl-3.6.0\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "f957e3e0f6534cbd93eb52b32894d1c5",
              "pip_warning": {
                "packages": [
                  "joblib"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting scace\n",
            "  Downloading scace-0.1.2-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting numpy (from scace)\n",
            "  Downloading numpy-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scanpy (from scace)\n",
            "  Downloading scanpy-1.11.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Collecting torch (from scace)\n",
            "  Downloading torch-2.6.0-cp311-cp311-manylinux1_x86_64.whl.metadata (28 kB)\n",
            "Collecting pandas (from scace)\n",
            "  Downloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tqdm (from scace)\n",
            "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m251.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy (from scace)\n",
            "  Downloading scipy-1.15.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m237.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sklearn (from scace)\n",
            "  Downloading sklearn-0.0.post12.tar.gz (2.6 kB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n"
          ]
        }
      ],
      "source": [
        "!pip install --no-cache-dir --upgrade --force-reinstall scikit-learn\n",
        "!pip install --no-cache-dir --upgrade --force-reinstall scace\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N40LaW2HO1GI"
      },
      "outputs": [],
      "source": [
        "import h5py\n",
        "import numpy as np\n",
        "import scanpy as sc\n",
        "import torch\n",
        "import random\n",
        "from scace import run_scace\n",
        "\n",
        "seed = 666\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "adata = run_scace(adata, return_all=True)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "qfQbO5eHdGTg",
        "zZPsvyAScZDa",
        "ebQTGkVlIMRS",
        "AWmzm43Oh9KN",
        "Q7AaVafohlm-",
        "0ioBg-DMM9ML",
        "-lK7RrqkIIha",
        "4oKTuiyJsOaq",
        "Jt5UeGYvyC0n"
      ],
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}